{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.9168 - accuracy: 0.5112 - val_loss: 0.8640 - val_accuracy: 0.5195\n",
      "Epoch 2/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8446 - accuracy: 0.5112 - val_loss: 0.8050 - val_accuracy: 0.5195\n",
      "Epoch 3/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7920 - accuracy: 0.5112 - val_loss: 0.7598 - val_accuracy: 0.5195\n",
      "Epoch 4/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7517 - accuracy: 0.5112 - val_loss: 0.7297 - val_accuracy: 0.5195\n",
      "Epoch 5/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7251 - accuracy: 0.5112 - val_loss: 0.7099 - val_accuracy: 0.5195\n",
      "Epoch 6/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7082 - accuracy: 0.5112 - val_loss: 0.6979 - val_accuracy: 0.5195\n",
      "Epoch 7/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6975 - accuracy: 0.5112 - val_loss: 0.6901 - val_accuracy: 0.5195\n",
      "Epoch 8/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6912 - accuracy: 0.5112 - val_loss: 0.6853 - val_accuracy: 0.5195\n",
      "Epoch 9/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.5112 - val_loss: 0.6830 - val_accuracy: 0.5195\n",
      "Epoch 10/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6845 - accuracy: 0.5112 - val_loss: 0.6808 - val_accuracy: 0.5195\n",
      "Epoch 11/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6822 - accuracy: 0.5112 - val_loss: 0.6787 - val_accuracy: 0.5195\n",
      "Epoch 12/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6799 - accuracy: 0.5112 - val_loss: 0.6767 - val_accuracy: 0.5195\n",
      "Epoch 13/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6776 - accuracy: 0.5154 - val_loss: 0.6743 - val_accuracy: 0.5617\n",
      "Epoch 14/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6754 - accuracy: 0.6215 - val_loss: 0.6719 - val_accuracy: 0.7143\n",
      "Epoch 15/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6728 - accuracy: 0.6313 - val_loss: 0.6686 - val_accuracy: 0.6656\n",
      "Epoch 16/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6694 - accuracy: 0.7039 - val_loss: 0.6655 - val_accuracy: 0.7630\n",
      "Epoch 17/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6657 - accuracy: 0.7277 - val_loss: 0.6614 - val_accuracy: 0.7630\n",
      "Epoch 18/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6619 - accuracy: 0.7165 - val_loss: 0.6569 - val_accuracy: 0.7565\n",
      "Epoch 19/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.7291 - val_loss: 0.6520 - val_accuracy: 0.7727\n",
      "Epoch 20/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6524 - accuracy: 0.7402 - val_loss: 0.6464 - val_accuracy: 0.7857\n",
      "Epoch 21/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6468 - accuracy: 0.7542 - val_loss: 0.6400 - val_accuracy: 0.7987\n",
      "Epoch 22/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6403 - accuracy: 0.7696 - val_loss: 0.6329 - val_accuracy: 0.8084\n",
      "Epoch 23/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6333 - accuracy: 0.7919 - val_loss: 0.6252 - val_accuracy: 0.8279\n",
      "Epoch 24/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6253 - accuracy: 0.7975 - val_loss: 0.6166 - val_accuracy: 0.8214\n",
      "Epoch 25/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6167 - accuracy: 0.7975 - val_loss: 0.6066 - val_accuracy: 0.8214\n",
      "Epoch 26/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6071 - accuracy: 0.7975 - val_loss: 0.5962 - val_accuracy: 0.8214\n",
      "Epoch 27/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5970 - accuracy: 0.8003 - val_loss: 0.5851 - val_accuracy: 0.8377\n",
      "Epoch 28/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5865 - accuracy: 0.7989 - val_loss: 0.5734 - val_accuracy: 0.8377\n",
      "Epoch 29/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5750 - accuracy: 0.8045 - val_loss: 0.5619 - val_accuracy: 0.8442\n",
      "Epoch 30/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5635 - accuracy: 0.8212 - val_loss: 0.5496 - val_accuracy: 0.8474\n",
      "Epoch 31/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5520 - accuracy: 0.8212 - val_loss: 0.5366 - val_accuracy: 0.8474\n",
      "Epoch 32/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5402 - accuracy: 0.8142 - val_loss: 0.5246 - val_accuracy: 0.8442\n",
      "Epoch 33/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5294 - accuracy: 0.8254 - val_loss: 0.5135 - val_accuracy: 0.8701\n",
      "Epoch 34/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5179 - accuracy: 0.8142 - val_loss: 0.5006 - val_accuracy: 0.8506\n",
      "Epoch 35/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5073 - accuracy: 0.8254 - val_loss: 0.4893 - val_accuracy: 0.8539\n",
      "Epoch 36/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4972 - accuracy: 0.8310 - val_loss: 0.4796 - val_accuracy: 0.8442\n",
      "Epoch 37/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4877 - accuracy: 0.8296 - val_loss: 0.4696 - val_accuracy: 0.8442\n",
      "Epoch 38/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4788 - accuracy: 0.8296 - val_loss: 0.4591 - val_accuracy: 0.8571\n",
      "Epoch 39/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4706 - accuracy: 0.8282 - val_loss: 0.4515 - val_accuracy: 0.8442\n",
      "Epoch 40/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.8310 - val_loss: 0.4436 - val_accuracy: 0.8474\n",
      "Epoch 41/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4559 - accuracy: 0.8338 - val_loss: 0.4362 - val_accuracy: 0.8474\n",
      "Epoch 42/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.8310 - val_loss: 0.4301 - val_accuracy: 0.8571\n",
      "Epoch 43/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4438 - accuracy: 0.8254 - val_loss: 0.4238 - val_accuracy: 0.8474\n",
      "Epoch 44/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.8310 - val_loss: 0.4176 - val_accuracy: 0.8474\n",
      "Epoch 45/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4331 - accuracy: 0.8254 - val_loss: 0.4136 - val_accuracy: 0.8474\n",
      "Epoch 46/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.8268 - val_loss: 0.4087 - val_accuracy: 0.8474\n",
      "Epoch 47/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4240 - accuracy: 0.8338 - val_loss: 0.4035 - val_accuracy: 0.8442\n",
      "Epoch 48/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4206 - accuracy: 0.8268 - val_loss: 0.3983 - val_accuracy: 0.8442\n",
      "Epoch 49/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4176 - accuracy: 0.8268 - val_loss: 0.3965 - val_accuracy: 0.8377\n",
      "Epoch 50/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4133 - accuracy: 0.8338 - val_loss: 0.3921 - val_accuracy: 0.8442\n",
      "Epoch 51/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4111 - accuracy: 0.8268 - val_loss: 0.3886 - val_accuracy: 0.8442\n",
      "Epoch 52/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4080 - accuracy: 0.8310 - val_loss: 0.3883 - val_accuracy: 0.8442\n",
      "Epoch 53/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4051 - accuracy: 0.8310 - val_loss: 0.3843 - val_accuracy: 0.8377\n",
      "Epoch 54/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4032 - accuracy: 0.8296 - val_loss: 0.3809 - val_accuracy: 0.8442\n",
      "Epoch 55/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.8268 - val_loss: 0.3805 - val_accuracy: 0.8377\n",
      "Epoch 56/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3989 - accuracy: 0.8268 - val_loss: 0.3772 - val_accuracy: 0.8344\n",
      "Epoch 57/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3981 - accuracy: 0.8282 - val_loss: 0.3789 - val_accuracy: 0.8474\n",
      "Epoch 58/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3945 - accuracy: 0.8296 - val_loss: 0.3723 - val_accuracy: 0.8539\n",
      "Epoch 59/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3937 - accuracy: 0.8310 - val_loss: 0.3730 - val_accuracy: 0.8344\n",
      "Epoch 60/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3920 - accuracy: 0.8296 - val_loss: 0.3726 - val_accuracy: 0.8442\n",
      "Epoch 61/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3914 - accuracy: 0.8324 - val_loss: 0.3690 - val_accuracy: 0.8539\n",
      "Epoch 62/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3890 - accuracy: 0.8296 - val_loss: 0.3690 - val_accuracy: 0.8409\n",
      "Epoch 63/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8366 - val_loss: 0.3688 - val_accuracy: 0.8442\n",
      "Epoch 64/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3869 - accuracy: 0.8394 - val_loss: 0.3667 - val_accuracy: 0.8539\n",
      "Epoch 65/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3857 - accuracy: 0.8422 - val_loss: 0.3660 - val_accuracy: 0.8539\n",
      "Epoch 66/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3849 - accuracy: 0.8422 - val_loss: 0.3674 - val_accuracy: 0.8442\n",
      "Epoch 67/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.8310 - val_loss: 0.3656 - val_accuracy: 0.8474\n",
      "Epoch 68/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8408 - val_loss: 0.3637 - val_accuracy: 0.8474\n",
      "Epoch 69/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3819 - accuracy: 0.8450 - val_loss: 0.3631 - val_accuracy: 0.8474\n",
      "Epoch 70/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3812 - accuracy: 0.8478 - val_loss: 0.3626 - val_accuracy: 0.8474\n",
      "Epoch 71/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3808 - accuracy: 0.8394 - val_loss: 0.3627 - val_accuracy: 0.8377\n",
      "Epoch 72/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3800 - accuracy: 0.8380 - val_loss: 0.3618 - val_accuracy: 0.8377\n",
      "Epoch 73/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3793 - accuracy: 0.8408 - val_loss: 0.3606 - val_accuracy: 0.8506\n",
      "Epoch 74/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8478 - val_loss: 0.3599 - val_accuracy: 0.8506\n",
      "Epoch 75/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3783 - accuracy: 0.8408 - val_loss: 0.3593 - val_accuracy: 0.8506\n",
      "Epoch 76/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3778 - accuracy: 0.8422 - val_loss: 0.3579 - val_accuracy: 0.8571\n",
      "Epoch 77/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3773 - accuracy: 0.8464 - val_loss: 0.3580 - val_accuracy: 0.8506\n",
      "Epoch 78/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3766 - accuracy: 0.8408 - val_loss: 0.3589 - val_accuracy: 0.8409\n",
      "Epoch 79/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3767 - accuracy: 0.8436 - val_loss: 0.3593 - val_accuracy: 0.8409\n",
      "Epoch 80/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3758 - accuracy: 0.8436 - val_loss: 0.3551 - val_accuracy: 0.8571\n",
      "Epoch 81/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3753 - accuracy: 0.8478 - val_loss: 0.3564 - val_accuracy: 0.8506\n",
      "Epoch 82/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8464 - val_loss: 0.3582 - val_accuracy: 0.8442\n",
      "Epoch 83/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3744 - accuracy: 0.8492 - val_loss: 0.3576 - val_accuracy: 0.8442\n",
      "Epoch 84/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3740 - accuracy: 0.8436 - val_loss: 0.3546 - val_accuracy: 0.8506\n",
      "Epoch 85/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3738 - accuracy: 0.8408 - val_loss: 0.3551 - val_accuracy: 0.8506\n",
      "Epoch 86/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8450 - val_loss: 0.3546 - val_accuracy: 0.8506\n",
      "Epoch 87/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3733 - accuracy: 0.8492 - val_loss: 0.3551 - val_accuracy: 0.8539\n",
      "Epoch 88/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3731 - accuracy: 0.8422 - val_loss: 0.3529 - val_accuracy: 0.8506\n",
      "Epoch 89/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8492 - val_loss: 0.3557 - val_accuracy: 0.8539\n",
      "Epoch 90/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3720 - accuracy: 0.8492 - val_loss: 0.3538 - val_accuracy: 0.8539\n",
      "Epoch 91/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3721 - accuracy: 0.8492 - val_loss: 0.3518 - val_accuracy: 0.8571\n",
      "Epoch 92/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3721 - accuracy: 0.8478 - val_loss: 0.3558 - val_accuracy: 0.8539\n",
      "Epoch 93/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3716 - accuracy: 0.8492 - val_loss: 0.3524 - val_accuracy: 0.8539\n",
      "Epoch 94/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3710 - accuracy: 0.8492 - val_loss: 0.3541 - val_accuracy: 0.8539\n",
      "Epoch 95/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8492 - val_loss: 0.3531 - val_accuracy: 0.8539\n",
      "Epoch 96/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3707 - accuracy: 0.8492 - val_loss: 0.3510 - val_accuracy: 0.8539\n",
      "Epoch 97/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3701 - accuracy: 0.8506 - val_loss: 0.3534 - val_accuracy: 0.8539\n",
      "Epoch 98/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8492 - val_loss: 0.3544 - val_accuracy: 0.8539\n",
      "Epoch 99/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3696 - accuracy: 0.8492 - val_loss: 0.3518 - val_accuracy: 0.8539\n",
      "Epoch 100/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8492 - val_loss: 0.3514 - val_accuracy: 0.8539\n",
      "Epoch 101/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8492 - val_loss: 0.3514 - val_accuracy: 0.8539\n",
      "Epoch 102/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8492 - val_loss: 0.3513 - val_accuracy: 0.8539\n",
      "Epoch 103/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8492 - val_loss: 0.3506 - val_accuracy: 0.8539\n",
      "Epoch 104/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8492 - val_loss: 0.3502 - val_accuracy: 0.8539\n",
      "Epoch 105/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8492 - val_loss: 0.3521 - val_accuracy: 0.8539\n",
      "Epoch 106/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8492 - val_loss: 0.3496 - val_accuracy: 0.8539\n",
      "Epoch 107/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8506 - val_loss: 0.3489 - val_accuracy: 0.8539\n",
      "Epoch 108/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8492 - val_loss: 0.3513 - val_accuracy: 0.8539\n",
      "Epoch 109/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8506 - val_loss: 0.3500 - val_accuracy: 0.8539\n",
      "Epoch 110/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8492 - val_loss: 0.3504 - val_accuracy: 0.8539\n",
      "Epoch 111/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8492 - val_loss: 0.3474 - val_accuracy: 0.8604\n",
      "Epoch 112/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8492 - val_loss: 0.3498 - val_accuracy: 0.8539\n",
      "Epoch 113/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8492 - val_loss: 0.3485 - val_accuracy: 0.8539\n",
      "Epoch 114/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8506 - val_loss: 0.3479 - val_accuracy: 0.8539\n",
      "Epoch 115/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3662 - accuracy: 0.8492 - val_loss: 0.3493 - val_accuracy: 0.8539\n",
      "Epoch 116/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3659 - accuracy: 0.8492 - val_loss: 0.3483 - val_accuracy: 0.8539\n",
      "Epoch 117/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3658 - accuracy: 0.8492 - val_loss: 0.3475 - val_accuracy: 0.8539\n",
      "Epoch 118/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3658 - accuracy: 0.8492 - val_loss: 0.3478 - val_accuracy: 0.8539\n",
      "Epoch 119/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3656 - accuracy: 0.8492 - val_loss: 0.3496 - val_accuracy: 0.8539\n",
      "Epoch 120/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3655 - accuracy: 0.8506 - val_loss: 0.3493 - val_accuracy: 0.8539\n",
      "Epoch 121/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8492 - val_loss: 0.3474 - val_accuracy: 0.8539\n",
      "Epoch 122/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8492 - val_loss: 0.3494 - val_accuracy: 0.8539\n",
      "Epoch 123/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3651 - accuracy: 0.8492 - val_loss: 0.3472 - val_accuracy: 0.8539\n",
      "Epoch 124/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3648 - accuracy: 0.8492 - val_loss: 0.3473 - val_accuracy: 0.8539\n",
      "Epoch 125/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3653 - accuracy: 0.8520 - val_loss: 0.3478 - val_accuracy: 0.8539\n",
      "Epoch 126/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3645 - accuracy: 0.8492 - val_loss: 0.3468 - val_accuracy: 0.8539\n",
      "Epoch 127/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8492 - val_loss: 0.3471 - val_accuracy: 0.8539\n",
      "Epoch 128/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3642 - accuracy: 0.8492 - val_loss: 0.3471 - val_accuracy: 0.8539\n",
      "Epoch 129/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3642 - accuracy: 0.8492 - val_loss: 0.3468 - val_accuracy: 0.8539\n",
      "Epoch 130/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3641 - accuracy: 0.8492 - val_loss: 0.3476 - val_accuracy: 0.8539\n",
      "Epoch 131/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3640 - accuracy: 0.8492 - val_loss: 0.3462 - val_accuracy: 0.8539\n",
      "Epoch 132/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3641 - accuracy: 0.8520 - val_loss: 0.3482 - val_accuracy: 0.8539\n",
      "Epoch 133/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3631 - accuracy: 0.8492 - val_loss: 0.3451 - val_accuracy: 0.8506\n",
      "Epoch 134/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3637 - accuracy: 0.8464 - val_loss: 0.3440 - val_accuracy: 0.8571\n",
      "Epoch 135/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3635 - accuracy: 0.8492 - val_loss: 0.3477 - val_accuracy: 0.8539\n",
      "Epoch 136/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8492 - val_loss: 0.3451 - val_accuracy: 0.8506\n",
      "Epoch 137/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3630 - accuracy: 0.8464 - val_loss: 0.3442 - val_accuracy: 0.8571\n",
      "Epoch 138/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8450 - val_loss: 0.3453 - val_accuracy: 0.8506\n",
      "Epoch 139/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3627 - accuracy: 0.8464 - val_loss: 0.3447 - val_accuracy: 0.8506\n",
      "Epoch 140/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8450 - val_loss: 0.3454 - val_accuracy: 0.8506\n",
      "Epoch 141/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3623 - accuracy: 0.8450 - val_loss: 0.3447 - val_accuracy: 0.8506\n",
      "Epoch 142/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8450 - val_loss: 0.3449 - val_accuracy: 0.8506\n",
      "Epoch 143/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3629 - accuracy: 0.8547 - val_loss: 0.3469 - val_accuracy: 0.8506\n",
      "Epoch 144/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3636 - accuracy: 0.8464 - val_loss: 0.3418 - val_accuracy: 0.8636\n",
      "Epoch 145/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.8464 - val_loss: 0.3452 - val_accuracy: 0.8506\n",
      "Epoch 146/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8450 - val_loss: 0.3449 - val_accuracy: 0.8506\n",
      "Epoch 147/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.8464 - val_loss: 0.3442 - val_accuracy: 0.8506\n",
      "Epoch 148/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8464 - val_loss: 0.3436 - val_accuracy: 0.8571\n",
      "Epoch 149/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3613 - accuracy: 0.8464 - val_loss: 0.3434 - val_accuracy: 0.8571\n",
      "Epoch 150/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3613 - accuracy: 0.8464 - val_loss: 0.3437 - val_accuracy: 0.8506\n",
      "Epoch 151/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8478 - val_loss: 0.3462 - val_accuracy: 0.8506\n",
      "Epoch 152/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8450 - val_loss: 0.3441 - val_accuracy: 0.8506\n",
      "Epoch 153/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8464 - val_loss: 0.3421 - val_accuracy: 0.8636\n",
      "Epoch 154/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3607 - accuracy: 0.8464 - val_loss: 0.3439 - val_accuracy: 0.8506\n",
      "Epoch 155/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3606 - accuracy: 0.8450 - val_loss: 0.3432 - val_accuracy: 0.8571\n",
      "Epoch 156/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3606 - accuracy: 0.8464 - val_loss: 0.3438 - val_accuracy: 0.8506\n",
      "Epoch 157/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3611 - accuracy: 0.8464 - val_loss: 0.3450 - val_accuracy: 0.8506\n",
      "Epoch 158/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3603 - accuracy: 0.8478 - val_loss: 0.3407 - val_accuracy: 0.8636\n",
      "Epoch 159/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8450 - val_loss: 0.3431 - val_accuracy: 0.8571\n",
      "Epoch 160/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8464 - val_loss: 0.3429 - val_accuracy: 0.8571\n",
      "Epoch 161/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3598 - accuracy: 0.8464 - val_loss: 0.3415 - val_accuracy: 0.8636\n",
      "Epoch 162/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3599 - accuracy: 0.8478 - val_loss: 0.3411 - val_accuracy: 0.8636\n",
      "Epoch 163/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3597 - accuracy: 0.8478 - val_loss: 0.3418 - val_accuracy: 0.8571\n",
      "Epoch 164/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3598 - accuracy: 0.8450 - val_loss: 0.3439 - val_accuracy: 0.8506\n",
      "Epoch 165/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8464 - val_loss: 0.3416 - val_accuracy: 0.8571\n",
      "Epoch 166/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.8464 - val_loss: 0.3422 - val_accuracy: 0.8571\n",
      "Epoch 167/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3596 - accuracy: 0.8478 - val_loss: 0.3410 - val_accuracy: 0.8636\n",
      "Epoch 168/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3591 - accuracy: 0.8464 - val_loss: 0.3429 - val_accuracy: 0.8506\n",
      "Epoch 169/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3591 - accuracy: 0.8464 - val_loss: 0.3422 - val_accuracy: 0.8571\n",
      "Epoch 170/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3594 - accuracy: 0.8478 - val_loss: 0.3415 - val_accuracy: 0.8571\n",
      "Epoch 171/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.8464 - val_loss: 0.3431 - val_accuracy: 0.8506\n",
      "Epoch 172/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3592 - accuracy: 0.8478 - val_loss: 0.3412 - val_accuracy: 0.8571\n",
      "Epoch 173/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3588 - accuracy: 0.8478 - val_loss: 0.3408 - val_accuracy: 0.8636\n",
      "Epoch 174/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3590 - accuracy: 0.8478 - val_loss: 0.3407 - val_accuracy: 0.8571\n",
      "Epoch 175/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3586 - accuracy: 0.8464 - val_loss: 0.3414 - val_accuracy: 0.8571\n",
      "Epoch 176/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3586 - accuracy: 0.8478 - val_loss: 0.3395 - val_accuracy: 0.8636\n",
      "Epoch 177/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8464 - val_loss: 0.3414 - val_accuracy: 0.8571\n",
      "Epoch 178/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3584 - accuracy: 0.8464 - val_loss: 0.3413 - val_accuracy: 0.8571\n",
      "Epoch 179/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8478 - val_loss: 0.3416 - val_accuracy: 0.8571\n",
      "Epoch 180/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3585 - accuracy: 0.8450 - val_loss: 0.3413 - val_accuracy: 0.8571\n",
      "Epoch 181/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3578 - accuracy: 0.8478 - val_loss: 0.3385 - val_accuracy: 0.8636\n",
      "Epoch 182/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3578 - accuracy: 0.8478 - val_loss: 0.3396 - val_accuracy: 0.8636\n",
      "Epoch 183/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3578 - accuracy: 0.8464 - val_loss: 0.3412 - val_accuracy: 0.8571\n",
      "Epoch 184/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3577 - accuracy: 0.8478 - val_loss: 0.3396 - val_accuracy: 0.8636\n",
      "Epoch 185/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3576 - accuracy: 0.8464 - val_loss: 0.3410 - val_accuracy: 0.8571\n",
      "Epoch 186/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3576 - accuracy: 0.8478 - val_loss: 0.3385 - val_accuracy: 0.8636\n",
      "Epoch 187/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8478 - val_loss: 0.3389 - val_accuracy: 0.8636\n",
      "Epoch 188/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8464 - val_loss: 0.3409 - val_accuracy: 0.8571\n",
      "Epoch 189/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8478 - val_loss: 0.3376 - val_accuracy: 0.8636\n",
      "Epoch 190/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8478 - val_loss: 0.3393 - val_accuracy: 0.8636\n",
      "Epoch 191/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8478 - val_loss: 0.3387 - val_accuracy: 0.8636\n",
      "Epoch 192/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3570 - accuracy: 0.8478 - val_loss: 0.3403 - val_accuracy: 0.8636\n",
      "Epoch 193/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8478 - val_loss: 0.3397 - val_accuracy: 0.8636\n",
      "Epoch 194/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8478 - val_loss: 0.3387 - val_accuracy: 0.8636\n",
      "Epoch 195/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8478 - val_loss: 0.3376 - val_accuracy: 0.8636\n",
      "Epoch 196/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.8478 - val_loss: 0.3413 - val_accuracy: 0.8474\n",
      "Epoch 197/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8464 - val_loss: 0.3381 - val_accuracy: 0.8636\n",
      "Epoch 198/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3567 - accuracy: 0.8478 - val_loss: 0.3390 - val_accuracy: 0.8636\n",
      "Epoch 199/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3564 - accuracy: 0.8450 - val_loss: 0.3400 - val_accuracy: 0.8636\n",
      "Epoch 200/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3568 - accuracy: 0.8478 - val_loss: 0.3372 - val_accuracy: 0.8636\n",
      "Epoch 201/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3560 - accuracy: 0.8478 - val_loss: 0.3383 - val_accuracy: 0.8636\n",
      "Epoch 202/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3565 - accuracy: 0.8534 - val_loss: 0.3382 - val_accuracy: 0.8636\n",
      "Epoch 203/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.8478 - val_loss: 0.3361 - val_accuracy: 0.8636\n",
      "Epoch 204/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.8478 - val_loss: 0.3381 - val_accuracy: 0.8636\n",
      "Epoch 205/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.8478 - val_loss: 0.3364 - val_accuracy: 0.8636\n",
      "Epoch 206/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3558 - accuracy: 0.8478 - val_loss: 0.3369 - val_accuracy: 0.8636\n",
      "Epoch 207/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3567 - accuracy: 0.8492 - val_loss: 0.3365 - val_accuracy: 0.8636\n",
      "Epoch 208/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3567 - accuracy: 0.8506 - val_loss: 0.3395 - val_accuracy: 0.8636\n",
      "Epoch 209/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3565 - accuracy: 0.8506 - val_loss: 0.3354 - val_accuracy: 0.8636\n",
      "Epoch 210/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8478 - val_loss: 0.3382 - val_accuracy: 0.8636\n",
      "Epoch 211/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.8492 - val_loss: 0.3389 - val_accuracy: 0.8636\n",
      "Epoch 212/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8492 - val_loss: 0.3367 - val_accuracy: 0.8636\n",
      "Epoch 213/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8478 - val_loss: 0.3365 - val_accuracy: 0.8636\n",
      "Epoch 214/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.8478 - val_loss: 0.3375 - val_accuracy: 0.8636\n",
      "Epoch 215/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8478 - val_loss: 0.3364 - val_accuracy: 0.8636\n",
      "Epoch 216/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8464 - val_loss: 0.3380 - val_accuracy: 0.8636\n",
      "Epoch 217/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8478 - val_loss: 0.3350 - val_accuracy: 0.8636\n",
      "Epoch 218/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8478 - val_loss: 0.3363 - val_accuracy: 0.8636\n",
      "Epoch 219/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3546 - accuracy: 0.8478 - val_loss: 0.3369 - val_accuracy: 0.8636\n",
      "Epoch 220/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3547 - accuracy: 0.8478 - val_loss: 0.3355 - val_accuracy: 0.8636\n",
      "Epoch 221/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8506 - val_loss: 0.3367 - val_accuracy: 0.8636\n",
      "Epoch 222/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3545 - accuracy: 0.8478 - val_loss: 0.3353 - val_accuracy: 0.8636\n",
      "Epoch 223/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8506 - val_loss: 0.3350 - val_accuracy: 0.8636\n",
      "Epoch 224/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8492 - val_loss: 0.3360 - val_accuracy: 0.8636\n",
      "Epoch 225/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3542 - accuracy: 0.8478 - val_loss: 0.3349 - val_accuracy: 0.8636\n",
      "Epoch 226/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3541 - accuracy: 0.8478 - val_loss: 0.3346 - val_accuracy: 0.8636\n",
      "Epoch 227/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3540 - accuracy: 0.8478 - val_loss: 0.3357 - val_accuracy: 0.8636\n",
      "Epoch 228/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8506 - val_loss: 0.3345 - val_accuracy: 0.8636\n",
      "Epoch 229/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3540 - accuracy: 0.8478 - val_loss: 0.3352 - val_accuracy: 0.8636\n",
      "Epoch 230/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8506 - val_loss: 0.3359 - val_accuracy: 0.8636\n",
      "Epoch 231/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3544 - accuracy: 0.8520 - val_loss: 0.3358 - val_accuracy: 0.8636\n",
      "Epoch 232/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3540 - accuracy: 0.8506 - val_loss: 0.3342 - val_accuracy: 0.8636\n",
      "Epoch 233/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3535 - accuracy: 0.8478 - val_loss: 0.3346 - val_accuracy: 0.8636\n",
      "Epoch 234/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3537 - accuracy: 0.8506 - val_loss: 0.3361 - val_accuracy: 0.8604\n",
      "Epoch 235/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8492 - val_loss: 0.3339 - val_accuracy: 0.8636\n",
      "Epoch 236/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3532 - accuracy: 0.8478 - val_loss: 0.3349 - val_accuracy: 0.8636\n",
      "Epoch 237/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3533 - accuracy: 0.8492 - val_loss: 0.3346 - val_accuracy: 0.8636\n",
      "Epoch 238/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3535 - accuracy: 0.8492 - val_loss: 0.3344 - val_accuracy: 0.8636\n",
      "Epoch 239/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3531 - accuracy: 0.8478 - val_loss: 0.3336 - val_accuracy: 0.8636\n",
      "Epoch 240/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3529 - accuracy: 0.8478 - val_loss: 0.3338 - val_accuracy: 0.8636\n",
      "Epoch 241/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8478 - val_loss: 0.3335 - val_accuracy: 0.8636\n",
      "Epoch 242/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3541 - accuracy: 0.8520 - val_loss: 0.3348 - val_accuracy: 0.8604\n",
      "Epoch 243/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3532 - accuracy: 0.8506 - val_loss: 0.3319 - val_accuracy: 0.8604\n",
      "Epoch 244/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3526 - accuracy: 0.8492 - val_loss: 0.3339 - val_accuracy: 0.8636\n",
      "Epoch 245/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3531 - accuracy: 0.8478 - val_loss: 0.3333 - val_accuracy: 0.8636\n",
      "Epoch 246/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3526 - accuracy: 0.8534 - val_loss: 0.3352 - val_accuracy: 0.8636\n",
      "Epoch 247/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8520 - val_loss: 0.3344 - val_accuracy: 0.8636\n",
      "Epoch 248/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3523 - accuracy: 0.8492 - val_loss: 0.3319 - val_accuracy: 0.8604\n",
      "Epoch 249/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3527 - accuracy: 0.8506 - val_loss: 0.3331 - val_accuracy: 0.8636\n",
      "Epoch 250/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3522 - accuracy: 0.8492 - val_loss: 0.3320 - val_accuracy: 0.8636\n",
      "Epoch 251/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3519 - accuracy: 0.8506 - val_loss: 0.3360 - val_accuracy: 0.8669\n",
      "Epoch 252/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3519 - accuracy: 0.8534 - val_loss: 0.3325 - val_accuracy: 0.8636\n",
      "Epoch 253/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3520 - accuracy: 0.8520 - val_loss: 0.3317 - val_accuracy: 0.8636\n",
      "Epoch 254/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3518 - accuracy: 0.8534 - val_loss: 0.3332 - val_accuracy: 0.8636\n",
      "Epoch 255/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3517 - accuracy: 0.8506 - val_loss: 0.3333 - val_accuracy: 0.8636\n",
      "Epoch 256/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3520 - accuracy: 0.8520 - val_loss: 0.3318 - val_accuracy: 0.8636\n",
      "Epoch 257/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3516 - accuracy: 0.8520 - val_loss: 0.3315 - val_accuracy: 0.8636\n",
      "Epoch 258/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3521 - accuracy: 0.8506 - val_loss: 0.3312 - val_accuracy: 0.8636\n",
      "Epoch 259/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.8575 - val_loss: 0.3359 - val_accuracy: 0.8604\n",
      "Epoch 260/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3518 - accuracy: 0.8547 - val_loss: 0.3323 - val_accuracy: 0.8636\n",
      "Epoch 261/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.8520 - val_loss: 0.3331 - val_accuracy: 0.8636\n",
      "Epoch 262/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.8534 - val_loss: 0.3336 - val_accuracy: 0.8636\n",
      "Epoch 263/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.8534 - val_loss: 0.3325 - val_accuracy: 0.8636\n",
      "Epoch 264/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.8534 - val_loss: 0.3302 - val_accuracy: 0.8604\n",
      "Epoch 265/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.8547 - val_loss: 0.3316 - val_accuracy: 0.8636\n",
      "Epoch 266/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3505 - accuracy: 0.8561 - val_loss: 0.3321 - val_accuracy: 0.8636\n",
      "Epoch 267/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.8534 - val_loss: 0.3326 - val_accuracy: 0.8636\n",
      "Epoch 268/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3507 - accuracy: 0.8561 - val_loss: 0.3294 - val_accuracy: 0.8604\n",
      "Epoch 269/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3507 - accuracy: 0.8547 - val_loss: 0.3315 - val_accuracy: 0.8636\n",
      "Epoch 270/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3505 - accuracy: 0.8534 - val_loss: 0.3313 - val_accuracy: 0.8636\n",
      "Epoch 271/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3504 - accuracy: 0.8561 - val_loss: 0.3306 - val_accuracy: 0.8669\n",
      "Epoch 272/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3503 - accuracy: 0.8534 - val_loss: 0.3298 - val_accuracy: 0.8669\n",
      "Epoch 273/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.8603 - val_loss: 0.3309 - val_accuracy: 0.8669\n",
      "Epoch 274/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3502 - accuracy: 0.8575 - val_loss: 0.3305 - val_accuracy: 0.8669\n",
      "Epoch 275/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3500 - accuracy: 0.8534 - val_loss: 0.3306 - val_accuracy: 0.8636\n",
      "Epoch 276/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3499 - accuracy: 0.8547 - val_loss: 0.3295 - val_accuracy: 0.8669\n",
      "Epoch 277/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3497 - accuracy: 0.8603 - val_loss: 0.3298 - val_accuracy: 0.8669\n",
      "Epoch 278/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3506 - accuracy: 0.8547 - val_loss: 0.3306 - val_accuracy: 0.8636\n",
      "Epoch 279/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3497 - accuracy: 0.8561 - val_loss: 0.3301 - val_accuracy: 0.8636\n",
      "Epoch 280/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3498 - accuracy: 0.8547 - val_loss: 0.3294 - val_accuracy: 0.8669\n",
      "Epoch 281/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3499 - accuracy: 0.8575 - val_loss: 0.3309 - val_accuracy: 0.8636\n",
      "Epoch 282/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.8561 - val_loss: 0.3275 - val_accuracy: 0.8701\n",
      "Epoch 283/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3487 - accuracy: 0.8589 - val_loss: 0.3326 - val_accuracy: 0.8604\n",
      "Epoch 284/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3493 - accuracy: 0.8603 - val_loss: 0.3303 - val_accuracy: 0.8636\n",
      "Epoch 285/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3498 - accuracy: 0.8603 - val_loss: 0.3279 - val_accuracy: 0.8734\n",
      "Epoch 286/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8575 - val_loss: 0.3293 - val_accuracy: 0.8636\n",
      "Epoch 287/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3488 - accuracy: 0.8534 - val_loss: 0.3299 - val_accuracy: 0.8636\n",
      "Epoch 288/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8575 - val_loss: 0.3289 - val_accuracy: 0.8636\n",
      "Epoch 289/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3486 - accuracy: 0.8575 - val_loss: 0.3289 - val_accuracy: 0.8636\n",
      "Epoch 290/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3491 - accuracy: 0.8561 - val_loss: 0.3297 - val_accuracy: 0.8636\n",
      "Epoch 291/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3484 - accuracy: 0.8603 - val_loss: 0.3269 - val_accuracy: 0.8734\n",
      "Epoch 292/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3491 - accuracy: 0.8617 - val_loss: 0.3278 - val_accuracy: 0.8734\n",
      "Epoch 293/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.8589 - val_loss: 0.3285 - val_accuracy: 0.8636\n",
      "Epoch 294/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3485 - accuracy: 0.8547 - val_loss: 0.3291 - val_accuracy: 0.8636\n",
      "Epoch 295/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3481 - accuracy: 0.8575 - val_loss: 0.3283 - val_accuracy: 0.8636\n",
      "Epoch 296/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3489 - accuracy: 0.8575 - val_loss: 0.3285 - val_accuracy: 0.8636\n",
      "Epoch 297/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3489 - accuracy: 0.8547 - val_loss: 0.3272 - val_accuracy: 0.8701\n",
      "Epoch 298/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3491 - accuracy: 0.8603 - val_loss: 0.3299 - val_accuracy: 0.8571\n",
      "Epoch 299/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3481 - accuracy: 0.8603 - val_loss: 0.3267 - val_accuracy: 0.8701\n",
      "Epoch 300/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3482 - accuracy: 0.8603 - val_loss: 0.3291 - val_accuracy: 0.8571\n",
      "Epoch 301/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3479 - accuracy: 0.8631 - val_loss: 0.3271 - val_accuracy: 0.8701\n",
      "Epoch 302/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3481 - accuracy: 0.8575 - val_loss: 0.3271 - val_accuracy: 0.8701\n",
      "Epoch 303/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8617 - val_loss: 0.3259 - val_accuracy: 0.8701\n",
      "Epoch 304/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3475 - accuracy: 0.8589 - val_loss: 0.3273 - val_accuracy: 0.8636\n",
      "Epoch 305/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3478 - accuracy: 0.8575 - val_loss: 0.3266 - val_accuracy: 0.8701\n",
      "Epoch 306/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3473 - accuracy: 0.8575 - val_loss: 0.3280 - val_accuracy: 0.8636\n",
      "Epoch 307/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8575 - val_loss: 0.3264 - val_accuracy: 0.8701\n",
      "Epoch 308/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8478 - val_loss: 0.3250 - val_accuracy: 0.8701\n",
      "Epoch 309/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3473 - accuracy: 0.8561 - val_loss: 0.3312 - val_accuracy: 0.8604\n",
      "Epoch 310/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8617 - val_loss: 0.3261 - val_accuracy: 0.8701\n",
      "Epoch 311/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8617 - val_loss: 0.3271 - val_accuracy: 0.8636\n",
      "Epoch 312/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3461 - accuracy: 0.8631 - val_loss: 0.3255 - val_accuracy: 0.8701\n",
      "Epoch 313/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8617 - val_loss: 0.3246 - val_accuracy: 0.8701\n",
      "Epoch 314/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3463 - accuracy: 0.8617 - val_loss: 0.3248 - val_accuracy: 0.8701\n",
      "Epoch 315/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3466 - accuracy: 0.8617 - val_loss: 0.3254 - val_accuracy: 0.8701\n",
      "Epoch 316/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8617 - val_loss: 0.3270 - val_accuracy: 0.8669\n",
      "Epoch 317/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3460 - accuracy: 0.8645 - val_loss: 0.3251 - val_accuracy: 0.8701\n",
      "Epoch 318/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3459 - accuracy: 0.8617 - val_loss: 0.3244 - val_accuracy: 0.8701\n",
      "Epoch 319/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8617 - val_loss: 0.3248 - val_accuracy: 0.8701\n",
      "Epoch 320/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3456 - accuracy: 0.8617 - val_loss: 0.3248 - val_accuracy: 0.8734\n",
      "Epoch 321/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8645 - val_loss: 0.3230 - val_accuracy: 0.8734\n",
      "Epoch 322/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8659 - val_loss: 0.3251 - val_accuracy: 0.8734\n",
      "Epoch 323/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8617 - val_loss: 0.3237 - val_accuracy: 0.8701\n",
      "Epoch 324/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8631 - val_loss: 0.3243 - val_accuracy: 0.8734\n",
      "Epoch 325/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8617 - val_loss: 0.3225 - val_accuracy: 0.8734\n",
      "Epoch 326/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8645 - val_loss: 0.3243 - val_accuracy: 0.8734\n",
      "Epoch 327/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3452 - accuracy: 0.8659 - val_loss: 0.3249 - val_accuracy: 0.8636\n",
      "Epoch 328/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3448 - accuracy: 0.8631 - val_loss: 0.3231 - val_accuracy: 0.8701\n",
      "Epoch 329/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3445 - accuracy: 0.8617 - val_loss: 0.3234 - val_accuracy: 0.8701\n",
      "Epoch 330/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8617 - val_loss: 0.3228 - val_accuracy: 0.8734\n",
      "Epoch 331/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3446 - accuracy: 0.8673 - val_loss: 0.3241 - val_accuracy: 0.8669\n",
      "Epoch 332/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3445 - accuracy: 0.8687 - val_loss: 0.3247 - val_accuracy: 0.8669\n",
      "Epoch 333/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8645 - val_loss: 0.3221 - val_accuracy: 0.8701\n",
      "Epoch 334/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8547 - val_loss: 0.3216 - val_accuracy: 0.8701\n",
      "Epoch 335/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3444 - accuracy: 0.8631 - val_loss: 0.3228 - val_accuracy: 0.8766\n",
      "Epoch 336/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3440 - accuracy: 0.8673 - val_loss: 0.3231 - val_accuracy: 0.8669\n",
      "Epoch 337/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3441 - accuracy: 0.8631 - val_loss: 0.3222 - val_accuracy: 0.8766\n",
      "Epoch 338/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3446 - accuracy: 0.8687 - val_loss: 0.3234 - val_accuracy: 0.8669\n",
      "Epoch 339/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3436 - accuracy: 0.8659 - val_loss: 0.3212 - val_accuracy: 0.8701\n",
      "Epoch 340/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3438 - accuracy: 0.8659 - val_loss: 0.3211 - val_accuracy: 0.8701\n",
      "Epoch 341/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3435 - accuracy: 0.8617 - val_loss: 0.3201 - val_accuracy: 0.8701\n",
      "Epoch 342/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3438 - accuracy: 0.8631 - val_loss: 0.3197 - val_accuracy: 0.8701\n",
      "Epoch 343/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3432 - accuracy: 0.8617 - val_loss: 0.3197 - val_accuracy: 0.8701\n",
      "Epoch 344/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3436 - accuracy: 0.8631 - val_loss: 0.3242 - val_accuracy: 0.8701\n",
      "Epoch 345/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8659 - val_loss: 0.3188 - val_accuracy: 0.8701\n",
      "Epoch 346/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8659 - val_loss: 0.3213 - val_accuracy: 0.8669\n",
      "Epoch 347/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3426 - accuracy: 0.8659 - val_loss: 0.3193 - val_accuracy: 0.8701\n",
      "Epoch 348/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8659 - val_loss: 0.3208 - val_accuracy: 0.8766\n",
      "Epoch 349/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.8659 - val_loss: 0.3199 - val_accuracy: 0.8734\n",
      "Epoch 350/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3422 - accuracy: 0.8617 - val_loss: 0.3189 - val_accuracy: 0.8701\n",
      "Epoch 351/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.8617 - val_loss: 0.3188 - val_accuracy: 0.8701\n",
      "Epoch 352/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8673 - val_loss: 0.3213 - val_accuracy: 0.8669\n",
      "Epoch 353/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3427 - accuracy: 0.8617 - val_loss: 0.3181 - val_accuracy: 0.8701\n",
      "Epoch 354/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3419 - accuracy: 0.8673 - val_loss: 0.3196 - val_accuracy: 0.8766\n",
      "Epoch 355/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.8659 - val_loss: 0.3186 - val_accuracy: 0.8734\n",
      "Epoch 356/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3420 - accuracy: 0.8547 - val_loss: 0.3182 - val_accuracy: 0.8734\n",
      "Epoch 357/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.8673 - val_loss: 0.3191 - val_accuracy: 0.8766\n",
      "Epoch 358/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8687 - val_loss: 0.3196 - val_accuracy: 0.8669\n",
      "Epoch 359/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.8659 - val_loss: 0.3192 - val_accuracy: 0.8669\n",
      "Epoch 360/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.8603 - val_loss: 0.3180 - val_accuracy: 0.8734\n",
      "Epoch 361/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3408 - accuracy: 0.8673 - val_loss: 0.3192 - val_accuracy: 0.8669\n",
      "Epoch 362/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8645 - val_loss: 0.3188 - val_accuracy: 0.8734\n",
      "Epoch 363/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.8659 - val_loss: 0.3178 - val_accuracy: 0.8701\n",
      "Epoch 364/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8589 - val_loss: 0.3171 - val_accuracy: 0.8701\n",
      "Epoch 365/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8547 - val_loss: 0.3173 - val_accuracy: 0.8701\n",
      "Epoch 366/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3406 - accuracy: 0.8589 - val_loss: 0.3173 - val_accuracy: 0.8701\n",
      "Epoch 367/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3406 - accuracy: 0.8673 - val_loss: 0.3193 - val_accuracy: 0.8669\n",
      "Epoch 368/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3407 - accuracy: 0.8589 - val_loss: 0.3164 - val_accuracy: 0.8669\n",
      "Epoch 369/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.8617 - val_loss: 0.3173 - val_accuracy: 0.8734\n",
      "Epoch 370/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8561 - val_loss: 0.3155 - val_accuracy: 0.8669\n",
      "Epoch 371/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8603 - val_loss: 0.3163 - val_accuracy: 0.8701\n",
      "Epoch 372/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8534 - val_loss: 0.3154 - val_accuracy: 0.8669\n",
      "Epoch 373/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8631 - val_loss: 0.3158 - val_accuracy: 0.8701\n",
      "Epoch 374/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8631 - val_loss: 0.3156 - val_accuracy: 0.8701\n",
      "Epoch 375/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3394 - accuracy: 0.8645 - val_loss: 0.3161 - val_accuracy: 0.8766\n",
      "Epoch 376/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3390 - accuracy: 0.8589 - val_loss: 0.3143 - val_accuracy: 0.8669\n",
      "Epoch 377/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3396 - accuracy: 0.8659 - val_loss: 0.3165 - val_accuracy: 0.8669\n",
      "Epoch 378/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8547 - val_loss: 0.3139 - val_accuracy: 0.8701\n",
      "Epoch 379/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8547 - val_loss: 0.3135 - val_accuracy: 0.8669\n",
      "Epoch 380/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8561 - val_loss: 0.3143 - val_accuracy: 0.8701\n",
      "Epoch 381/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3384 - accuracy: 0.8534 - val_loss: 0.3138 - val_accuracy: 0.8701\n",
      "Epoch 382/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8617 - val_loss: 0.3157 - val_accuracy: 0.8669\n",
      "Epoch 383/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3383 - accuracy: 0.8589 - val_loss: 0.3139 - val_accuracy: 0.8701\n",
      "Epoch 384/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8617 - val_loss: 0.3137 - val_accuracy: 0.8701\n",
      "Epoch 385/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3379 - accuracy: 0.8534 - val_loss: 0.3126 - val_accuracy: 0.8669\n",
      "Epoch 386/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8575 - val_loss: 0.3137 - val_accuracy: 0.8701\n",
      "Epoch 387/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8575 - val_loss: 0.3144 - val_accuracy: 0.8636\n",
      "Epoch 388/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8547 - val_loss: 0.3133 - val_accuracy: 0.8701\n",
      "Epoch 389/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3378 - accuracy: 0.8631 - val_loss: 0.3129 - val_accuracy: 0.8701\n",
      "Epoch 390/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8547 - val_loss: 0.3127 - val_accuracy: 0.8701\n",
      "Epoch 391/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8547 - val_loss: 0.3127 - val_accuracy: 0.8701\n",
      "Epoch 392/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8575 - val_loss: 0.3131 - val_accuracy: 0.8701\n",
      "Epoch 393/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.8673 - val_loss: 0.3146 - val_accuracy: 0.8669\n",
      "Epoch 394/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.8673 - val_loss: 0.3127 - val_accuracy: 0.8701\n",
      "Epoch 395/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.8631 - val_loss: 0.3120 - val_accuracy: 0.8701\n",
      "Epoch 396/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3366 - accuracy: 0.8575 - val_loss: 0.3116 - val_accuracy: 0.8701\n",
      "Epoch 397/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3363 - accuracy: 0.8547 - val_loss: 0.3114 - val_accuracy: 0.8734\n",
      "Epoch 398/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3363 - accuracy: 0.8617 - val_loss: 0.3122 - val_accuracy: 0.8701\n",
      "Epoch 399/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.8603 - val_loss: 0.3124 - val_accuracy: 0.8604\n",
      "Epoch 400/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8589 - val_loss: 0.3102 - val_accuracy: 0.8734\n",
      "Epoch 401/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8561 - val_loss: 0.3108 - val_accuracy: 0.8734\n",
      "Epoch 402/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.8659 - val_loss: 0.3122 - val_accuracy: 0.8636\n",
      "Epoch 403/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8603 - val_loss: 0.3096 - val_accuracy: 0.8701\n",
      "Epoch 404/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3353 - accuracy: 0.8561 - val_loss: 0.3096 - val_accuracy: 0.8734\n",
      "Epoch 405/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3362 - accuracy: 0.8645 - val_loss: 0.3117 - val_accuracy: 0.8604\n",
      "Epoch 406/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8589 - val_loss: 0.3099 - val_accuracy: 0.8734\n",
      "Epoch 407/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8603 - val_loss: 0.3103 - val_accuracy: 0.8734\n",
      "Epoch 408/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3348 - accuracy: 0.8603 - val_loss: 0.3091 - val_accuracy: 0.8734\n",
      "Epoch 409/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8561 - val_loss: 0.3081 - val_accuracy: 0.8734\n",
      "Epoch 410/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3354 - accuracy: 0.8603 - val_loss: 0.3114 - val_accuracy: 0.8636\n",
      "Epoch 411/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8617 - val_loss: 0.3082 - val_accuracy: 0.8734\n",
      "Epoch 412/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8561 - val_loss: 0.3073 - val_accuracy: 0.8701\n",
      "Epoch 413/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8631 - val_loss: 0.3122 - val_accuracy: 0.8636\n",
      "Epoch 414/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8673 - val_loss: 0.3089 - val_accuracy: 0.8734\n",
      "Epoch 415/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8603 - val_loss: 0.3077 - val_accuracy: 0.8734\n",
      "Epoch 416/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3337 - accuracy: 0.8603 - val_loss: 0.3091 - val_accuracy: 0.8636\n",
      "Epoch 417/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8547 - val_loss: 0.3069 - val_accuracy: 0.8734\n",
      "Epoch 418/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8603 - val_loss: 0.3079 - val_accuracy: 0.8734\n",
      "Epoch 419/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3337 - accuracy: 0.8645 - val_loss: 0.3097 - val_accuracy: 0.8669\n",
      "Epoch 420/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3333 - accuracy: 0.8687 - val_loss: 0.3072 - val_accuracy: 0.8734\n",
      "Epoch 421/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3333 - accuracy: 0.8603 - val_loss: 0.3070 - val_accuracy: 0.8734\n",
      "Epoch 422/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.8589 - val_loss: 0.3061 - val_accuracy: 0.8734\n",
      "Epoch 423/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.8589 - val_loss: 0.3058 - val_accuracy: 0.8734\n",
      "Epoch 424/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8603 - val_loss: 0.3073 - val_accuracy: 0.8636\n",
      "Epoch 425/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8603 - val_loss: 0.3061 - val_accuracy: 0.8734\n",
      "Epoch 426/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8575 - val_loss: 0.3052 - val_accuracy: 0.8734\n",
      "Epoch 427/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8603 - val_loss: 0.3066 - val_accuracy: 0.8636\n",
      "Epoch 428/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8645 - val_loss: 0.3059 - val_accuracy: 0.8734\n",
      "Epoch 429/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.8603 - val_loss: 0.3064 - val_accuracy: 0.8636\n",
      "Epoch 430/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.8603 - val_loss: 0.3037 - val_accuracy: 0.8799\n",
      "Epoch 431/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3318 - accuracy: 0.8589 - val_loss: 0.3047 - val_accuracy: 0.8734\n",
      "Epoch 432/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8603 - val_loss: 0.3052 - val_accuracy: 0.8636\n",
      "Epoch 433/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8603 - val_loss: 0.3059 - val_accuracy: 0.8636\n",
      "Epoch 434/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8617 - val_loss: 0.3042 - val_accuracy: 0.8734\n",
      "Epoch 435/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8603 - val_loss: 0.3040 - val_accuracy: 0.8734\n",
      "Epoch 436/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8603 - val_loss: 0.3038 - val_accuracy: 0.8734\n",
      "Epoch 437/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3321 - accuracy: 0.8631 - val_loss: 0.3060 - val_accuracy: 0.8669\n",
      "Epoch 438/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8617 - val_loss: 0.3041 - val_accuracy: 0.8734\n",
      "Epoch 439/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8617 - val_loss: 0.3052 - val_accuracy: 0.8636\n",
      "Epoch 440/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8603 - val_loss: 0.3030 - val_accuracy: 0.8734\n",
      "Epoch 441/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3305 - accuracy: 0.8603 - val_loss: 0.3036 - val_accuracy: 0.8734\n",
      "Epoch 442/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8603 - val_loss: 0.3027 - val_accuracy: 0.8734\n",
      "Epoch 443/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3310 - accuracy: 0.8534 - val_loss: 0.3016 - val_accuracy: 0.8799\n",
      "Epoch 444/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8603 - val_loss: 0.3031 - val_accuracy: 0.8636\n",
      "Epoch 445/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8631 - val_loss: 0.3032 - val_accuracy: 0.8636\n",
      "Epoch 446/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8617 - val_loss: 0.3018 - val_accuracy: 0.8734\n",
      "Epoch 447/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8617 - val_loss: 0.3012 - val_accuracy: 0.8734\n",
      "Epoch 448/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8603 - val_loss: 0.3015 - val_accuracy: 0.8734\n",
      "Epoch 449/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3302 - accuracy: 0.8603 - val_loss: 0.3024 - val_accuracy: 0.8636\n",
      "Epoch 450/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8603 - val_loss: 0.3008 - val_accuracy: 0.8734\n",
      "Epoch 451/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3292 - accuracy: 0.8603 - val_loss: 0.3003 - val_accuracy: 0.8799\n",
      "Epoch 452/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8617 - val_loss: 0.3000 - val_accuracy: 0.8799\n",
      "Epoch 453/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8617 - val_loss: 0.3042 - val_accuracy: 0.8669\n",
      "Epoch 454/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3294 - accuracy: 0.8575 - val_loss: 0.2990 - val_accuracy: 0.8799\n",
      "Epoch 455/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8547 - val_loss: 0.2995 - val_accuracy: 0.8799\n",
      "Epoch 456/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8617 - val_loss: 0.3016 - val_accuracy: 0.8636\n",
      "Epoch 457/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8603 - val_loss: 0.3016 - val_accuracy: 0.8669\n",
      "Epoch 458/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8631 - val_loss: 0.3005 - val_accuracy: 0.8734\n",
      "Epoch 459/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3292 - accuracy: 0.8603 - val_loss: 0.2989 - val_accuracy: 0.8799\n",
      "Epoch 460/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3285 - accuracy: 0.8631 - val_loss: 0.3021 - val_accuracy: 0.8669\n",
      "Epoch 461/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.8617 - val_loss: 0.2996 - val_accuracy: 0.8734\n",
      "Epoch 462/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.8603 - val_loss: 0.2988 - val_accuracy: 0.8734\n",
      "Epoch 463/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8603 - val_loss: 0.3002 - val_accuracy: 0.8734\n",
      "Epoch 464/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8603 - val_loss: 0.2991 - val_accuracy: 0.8734\n",
      "Epoch 465/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8603 - val_loss: 0.2977 - val_accuracy: 0.8734\n",
      "Epoch 466/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3270 - accuracy: 0.8603 - val_loss: 0.2979 - val_accuracy: 0.8734\n",
      "Epoch 467/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.8631 - val_loss: 0.2995 - val_accuracy: 0.8669\n",
      "Epoch 468/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3262 - accuracy: 0.8603 - val_loss: 0.2970 - val_accuracy: 0.8799\n",
      "Epoch 469/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3261 - accuracy: 0.8617 - val_loss: 0.2973 - val_accuracy: 0.8734\n",
      "Epoch 470/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.8617 - val_loss: 0.2966 - val_accuracy: 0.8799\n",
      "Epoch 471/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3261 - accuracy: 0.8617 - val_loss: 0.2987 - val_accuracy: 0.8669\n",
      "Epoch 472/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3257 - accuracy: 0.8631 - val_loss: 0.2958 - val_accuracy: 0.8799\n",
      "Epoch 473/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3258 - accuracy: 0.8603 - val_loss: 0.2972 - val_accuracy: 0.8734\n",
      "Epoch 474/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8603 - val_loss: 0.2964 - val_accuracy: 0.8734\n",
      "Epoch 475/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3257 - accuracy: 0.8603 - val_loss: 0.2963 - val_accuracy: 0.8734\n",
      "Epoch 476/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8603 - val_loss: 0.2971 - val_accuracy: 0.8734\n",
      "Epoch 477/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3251 - accuracy: 0.8617 - val_loss: 0.2961 - val_accuracy: 0.8734\n",
      "Epoch 478/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8603 - val_loss: 0.2951 - val_accuracy: 0.8799\n",
      "Epoch 479/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8617 - val_loss: 0.2971 - val_accuracy: 0.8734\n",
      "Epoch 480/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3256 - accuracy: 0.8617 - val_loss: 0.2941 - val_accuracy: 0.8799\n",
      "Epoch 481/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3253 - accuracy: 0.8631 - val_loss: 0.2982 - val_accuracy: 0.8669\n",
      "Epoch 482/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3246 - accuracy: 0.8603 - val_loss: 0.2950 - val_accuracy: 0.8734\n",
      "Epoch 483/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8603 - val_loss: 0.2948 - val_accuracy: 0.8734\n",
      "Epoch 484/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3243 - accuracy: 0.8603 - val_loss: 0.2957 - val_accuracy: 0.8734\n",
      "Epoch 485/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3240 - accuracy: 0.8603 - val_loss: 0.2943 - val_accuracy: 0.8799\n",
      "Epoch 486/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3239 - accuracy: 0.8617 - val_loss: 0.2931 - val_accuracy: 0.8799\n",
      "Epoch 487/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3232 - accuracy: 0.8617 - val_loss: 0.2954 - val_accuracy: 0.8734\n",
      "Epoch 488/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3237 - accuracy: 0.8631 - val_loss: 0.2962 - val_accuracy: 0.8669\n",
      "Epoch 489/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.8603 - val_loss: 0.2938 - val_accuracy: 0.8799\n",
      "Epoch 490/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3236 - accuracy: 0.8603 - val_loss: 0.2933 - val_accuracy: 0.8799\n",
      "Epoch 491/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3235 - accuracy: 0.8617 - val_loss: 0.2921 - val_accuracy: 0.8799\n",
      "Epoch 492/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3233 - accuracy: 0.8603 - val_loss: 0.2932 - val_accuracy: 0.8799\n",
      "Epoch 493/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3236 - accuracy: 0.8617 - val_loss: 0.2917 - val_accuracy: 0.8799\n",
      "Epoch 494/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3235 - accuracy: 0.8617 - val_loss: 0.2921 - val_accuracy: 0.8799\n",
      "Epoch 495/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8617 - val_loss: 0.2936 - val_accuracy: 0.8734\n",
      "Epoch 496/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3222 - accuracy: 0.8631 - val_loss: 0.2937 - val_accuracy: 0.8766\n",
      "Epoch 497/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8617 - val_loss: 0.2912 - val_accuracy: 0.8799\n",
      "Epoch 498/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3225 - accuracy: 0.8631 - val_loss: 0.2933 - val_accuracy: 0.8766\n",
      "Epoch 499/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8603 - val_loss: 0.2893 - val_accuracy: 0.8799\n",
      "Epoch 500/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3222 - accuracy: 0.8645 - val_loss: 0.2918 - val_accuracy: 0.8799\n",
      "Epoch 501/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8645 - val_loss: 0.2914 - val_accuracy: 0.8799\n",
      "Epoch 502/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8617 - val_loss: 0.2900 - val_accuracy: 0.8799\n",
      "Epoch 503/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8617 - val_loss: 0.2899 - val_accuracy: 0.8799\n",
      "Epoch 504/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3209 - accuracy: 0.8617 - val_loss: 0.2912 - val_accuracy: 0.8799\n",
      "Epoch 505/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8631 - val_loss: 0.2910 - val_accuracy: 0.8799\n",
      "Epoch 506/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3210 - accuracy: 0.8645 - val_loss: 0.2902 - val_accuracy: 0.8799\n",
      "Epoch 507/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3209 - accuracy: 0.8617 - val_loss: 0.2895 - val_accuracy: 0.8799\n",
      "Epoch 508/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3204 - accuracy: 0.8603 - val_loss: 0.2909 - val_accuracy: 0.8766\n",
      "Epoch 509/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3204 - accuracy: 0.8617 - val_loss: 0.2902 - val_accuracy: 0.8831\n",
      "Epoch 510/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3205 - accuracy: 0.8645 - val_loss: 0.2899 - val_accuracy: 0.8831\n",
      "Epoch 511/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8617 - val_loss: 0.2885 - val_accuracy: 0.8799\n",
      "Epoch 512/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3205 - accuracy: 0.8631 - val_loss: 0.2885 - val_accuracy: 0.8799\n",
      "Epoch 513/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8617 - val_loss: 0.2889 - val_accuracy: 0.8799\n",
      "Epoch 514/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3196 - accuracy: 0.8631 - val_loss: 0.2887 - val_accuracy: 0.8799\n",
      "Epoch 515/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3200 - accuracy: 0.8617 - val_loss: 0.2879 - val_accuracy: 0.8799\n",
      "Epoch 516/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3193 - accuracy: 0.8617 - val_loss: 0.2884 - val_accuracy: 0.8799\n",
      "Epoch 517/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3201 - accuracy: 0.8617 - val_loss: 0.2901 - val_accuracy: 0.8669\n",
      "Epoch 518/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3193 - accuracy: 0.8603 - val_loss: 0.2880 - val_accuracy: 0.8831\n",
      "Epoch 519/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3193 - accuracy: 0.8617 - val_loss: 0.2886 - val_accuracy: 0.8831\n",
      "Epoch 520/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3183 - accuracy: 0.8617 - val_loss: 0.2855 - val_accuracy: 0.8799\n",
      "Epoch 521/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3187 - accuracy: 0.8617 - val_loss: 0.2866 - val_accuracy: 0.8799\n",
      "Epoch 522/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3184 - accuracy: 0.8617 - val_loss: 0.2881 - val_accuracy: 0.8831\n",
      "Epoch 523/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.8617 - val_loss: 0.2862 - val_accuracy: 0.8799\n",
      "Epoch 524/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3188 - accuracy: 0.8617 - val_loss: 0.2860 - val_accuracy: 0.8799\n",
      "Epoch 525/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3180 - accuracy: 0.8617 - val_loss: 0.2862 - val_accuracy: 0.8799\n",
      "Epoch 526/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3180 - accuracy: 0.8603 - val_loss: 0.2885 - val_accuracy: 0.8604\n",
      "Epoch 527/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8631 - val_loss: 0.2855 - val_accuracy: 0.8799\n",
      "Epoch 528/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3175 - accuracy: 0.8617 - val_loss: 0.2861 - val_accuracy: 0.8799\n",
      "Epoch 529/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3175 - accuracy: 0.8617 - val_loss: 0.2849 - val_accuracy: 0.8799\n",
      "Epoch 530/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3179 - accuracy: 0.8617 - val_loss: 0.2855 - val_accuracy: 0.8799\n",
      "Epoch 531/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3170 - accuracy: 0.8617 - val_loss: 0.2857 - val_accuracy: 0.8831\n",
      "Epoch 532/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3169 - accuracy: 0.8631 - val_loss: 0.2852 - val_accuracy: 0.8799\n",
      "Epoch 533/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3174 - accuracy: 0.8603 - val_loss: 0.2854 - val_accuracy: 0.8831\n",
      "Epoch 534/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3165 - accuracy: 0.8617 - val_loss: 0.2839 - val_accuracy: 0.8799\n",
      "Epoch 535/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3164 - accuracy: 0.8617 - val_loss: 0.2837 - val_accuracy: 0.8799\n",
      "Epoch 536/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3161 - accuracy: 0.8617 - val_loss: 0.2849 - val_accuracy: 0.8766\n",
      "Epoch 537/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3166 - accuracy: 0.8561 - val_loss: 0.2850 - val_accuracy: 0.8766\n",
      "Epoch 538/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3163 - accuracy: 0.8589 - val_loss: 0.2854 - val_accuracy: 0.8766\n",
      "Epoch 539/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3163 - accuracy: 0.8617 - val_loss: 0.2833 - val_accuracy: 0.8799\n",
      "Epoch 540/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3159 - accuracy: 0.8617 - val_loss: 0.2832 - val_accuracy: 0.8799\n",
      "Epoch 541/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.8603 - val_loss: 0.2842 - val_accuracy: 0.8766\n",
      "Epoch 542/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3153 - accuracy: 0.8617 - val_loss: 0.2830 - val_accuracy: 0.8799\n",
      "Epoch 543/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8617 - val_loss: 0.2824 - val_accuracy: 0.8799\n",
      "Epoch 544/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8617 - val_loss: 0.2832 - val_accuracy: 0.8799\n",
      "Epoch 545/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3161 - accuracy: 0.8547 - val_loss: 0.2854 - val_accuracy: 0.8701\n",
      "Epoch 546/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3149 - accuracy: 0.8603 - val_loss: 0.2820 - val_accuracy: 0.8799\n",
      "Epoch 547/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3154 - accuracy: 0.8589 - val_loss: 0.2814 - val_accuracy: 0.8799\n",
      "Epoch 548/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8617 - val_loss: 0.2823 - val_accuracy: 0.8766\n",
      "Epoch 549/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3146 - accuracy: 0.8575 - val_loss: 0.2820 - val_accuracy: 0.8766\n",
      "Epoch 550/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3149 - accuracy: 0.8561 - val_loss: 0.2816 - val_accuracy: 0.8766\n",
      "Epoch 551/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3139 - accuracy: 0.8617 - val_loss: 0.2799 - val_accuracy: 0.8799\n",
      "Epoch 552/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8575 - val_loss: 0.2822 - val_accuracy: 0.8766\n",
      "Epoch 553/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3143 - accuracy: 0.8520 - val_loss: 0.2811 - val_accuracy: 0.8766\n",
      "Epoch 554/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3141 - accuracy: 0.8617 - val_loss: 0.2791 - val_accuracy: 0.8799\n",
      "Epoch 555/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.8631 - val_loss: 0.2804 - val_accuracy: 0.8766\n",
      "Epoch 556/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3135 - accuracy: 0.8617 - val_loss: 0.2804 - val_accuracy: 0.8766\n",
      "Epoch 557/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3134 - accuracy: 0.8547 - val_loss: 0.2807 - val_accuracy: 0.8766\n",
      "Epoch 558/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3132 - accuracy: 0.8575 - val_loss: 0.2797 - val_accuracy: 0.8766\n",
      "Epoch 559/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3130 - accuracy: 0.8534 - val_loss: 0.2816 - val_accuracy: 0.8766\n",
      "Epoch 560/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3132 - accuracy: 0.8603 - val_loss: 0.2800 - val_accuracy: 0.8766\n",
      "Epoch 561/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.8575 - val_loss: 0.2789 - val_accuracy: 0.8766\n",
      "Epoch 562/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3127 - accuracy: 0.8547 - val_loss: 0.2790 - val_accuracy: 0.8766\n",
      "Epoch 563/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3131 - accuracy: 0.8603 - val_loss: 0.2787 - val_accuracy: 0.8766\n",
      "Epoch 564/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3123 - accuracy: 0.8561 - val_loss: 0.2796 - val_accuracy: 0.8766\n",
      "Epoch 565/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8575 - val_loss: 0.2787 - val_accuracy: 0.8766\n",
      "Epoch 566/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8534 - val_loss: 0.2797 - val_accuracy: 0.8766\n",
      "Epoch 567/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8534 - val_loss: 0.2786 - val_accuracy: 0.8766\n",
      "Epoch 568/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8534 - val_loss: 0.2800 - val_accuracy: 0.8766\n",
      "Epoch 569/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8534 - val_loss: 0.2782 - val_accuracy: 0.8766\n",
      "Epoch 570/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3116 - accuracy: 0.8547 - val_loss: 0.2788 - val_accuracy: 0.8766\n",
      "Epoch 571/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3113 - accuracy: 0.8534 - val_loss: 0.2784 - val_accuracy: 0.8766\n",
      "Epoch 572/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.8534 - val_loss: 0.2790 - val_accuracy: 0.8766\n",
      "Epoch 573/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3109 - accuracy: 0.8506 - val_loss: 0.2769 - val_accuracy: 0.8766\n",
      "Epoch 574/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3109 - accuracy: 0.8561 - val_loss: 0.2771 - val_accuracy: 0.8766\n",
      "Epoch 575/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3107 - accuracy: 0.8534 - val_loss: 0.2781 - val_accuracy: 0.8766\n",
      "Epoch 576/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3105 - accuracy: 0.8534 - val_loss: 0.2784 - val_accuracy: 0.8766\n",
      "Epoch 577/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.8534 - val_loss: 0.2762 - val_accuracy: 0.8766\n",
      "Epoch 578/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.8575 - val_loss: 0.2757 - val_accuracy: 0.8734\n",
      "Epoch 579/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3099 - accuracy: 0.8520 - val_loss: 0.2769 - val_accuracy: 0.8766\n",
      "Epoch 580/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3100 - accuracy: 0.8534 - val_loss: 0.2781 - val_accuracy: 0.8766\n",
      "Epoch 581/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3102 - accuracy: 0.8520 - val_loss: 0.2783 - val_accuracy: 0.8701\n",
      "Epoch 582/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.8534 - val_loss: 0.2759 - val_accuracy: 0.8766\n",
      "Epoch 583/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3106 - accuracy: 0.8659 - val_loss: 0.2753 - val_accuracy: 0.8766\n",
      "Epoch 584/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3100 - accuracy: 0.8534 - val_loss: 0.2757 - val_accuracy: 0.8766\n",
      "Epoch 585/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3092 - accuracy: 0.8534 - val_loss: 0.2759 - val_accuracy: 0.8766\n",
      "Epoch 586/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3093 - accuracy: 0.8534 - val_loss: 0.2748 - val_accuracy: 0.8766\n",
      "Epoch 587/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8534 - val_loss: 0.2748 - val_accuracy: 0.8766\n",
      "Epoch 588/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3088 - accuracy: 0.8534 - val_loss: 0.2749 - val_accuracy: 0.8766\n",
      "Epoch 589/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8534 - val_loss: 0.2754 - val_accuracy: 0.8766\n",
      "Epoch 590/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3083 - accuracy: 0.8534 - val_loss: 0.2743 - val_accuracy: 0.8766\n",
      "Epoch 591/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8645 - val_loss: 0.2727 - val_accuracy: 0.8799\n",
      "Epoch 592/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.8520 - val_loss: 0.2766 - val_accuracy: 0.8701\n",
      "Epoch 593/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3081 - accuracy: 0.8534 - val_loss: 0.2737 - val_accuracy: 0.8766\n",
      "Epoch 594/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8534 - val_loss: 0.2734 - val_accuracy: 0.8766\n",
      "Epoch 595/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3080 - accuracy: 0.8534 - val_loss: 0.2748 - val_accuracy: 0.8766\n",
      "Epoch 596/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8575 - val_loss: 0.2717 - val_accuracy: 0.8831\n",
      "Epoch 597/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3071 - accuracy: 0.8561 - val_loss: 0.2747 - val_accuracy: 0.8766\n",
      "Epoch 598/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3080 - accuracy: 0.8534 - val_loss: 0.2723 - val_accuracy: 0.8766\n",
      "Epoch 599/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.8534 - val_loss: 0.2735 - val_accuracy: 0.8766\n",
      "Epoch 600/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8547 - val_loss: 0.2723 - val_accuracy: 0.8766\n",
      "Epoch 601/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3070 - accuracy: 0.8534 - val_loss: 0.2732 - val_accuracy: 0.8766\n",
      "Epoch 602/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3066 - accuracy: 0.8534 - val_loss: 0.2721 - val_accuracy: 0.8766\n",
      "Epoch 603/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3067 - accuracy: 0.8534 - val_loss: 0.2725 - val_accuracy: 0.8766\n",
      "Epoch 604/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8561 - val_loss: 0.2710 - val_accuracy: 0.8831\n",
      "Epoch 605/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3060 - accuracy: 0.8534 - val_loss: 0.2734 - val_accuracy: 0.8701\n",
      "Epoch 606/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8534 - val_loss: 0.2717 - val_accuracy: 0.8766\n",
      "Epoch 607/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8534 - val_loss: 0.2717 - val_accuracy: 0.8766\n",
      "Epoch 608/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8547 - val_loss: 0.2705 - val_accuracy: 0.8831\n",
      "Epoch 609/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3063 - accuracy: 0.8534 - val_loss: 0.2740 - val_accuracy: 0.8734\n",
      "Epoch 610/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3065 - accuracy: 0.8520 - val_loss: 0.2714 - val_accuracy: 0.8766\n",
      "Epoch 611/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.8547 - val_loss: 0.2709 - val_accuracy: 0.8766\n",
      "Epoch 612/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.8534 - val_loss: 0.2729 - val_accuracy: 0.8734\n",
      "Epoch 613/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3053 - accuracy: 0.8534 - val_loss: 0.2696 - val_accuracy: 0.8831\n",
      "Epoch 614/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3053 - accuracy: 0.8561 - val_loss: 0.2705 - val_accuracy: 0.8766\n",
      "Epoch 615/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3051 - accuracy: 0.8547 - val_loss: 0.2698 - val_accuracy: 0.8831\n",
      "Epoch 616/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3046 - accuracy: 0.8547 - val_loss: 0.2707 - val_accuracy: 0.8766\n",
      "Epoch 617/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3059 - accuracy: 0.8561 - val_loss: 0.2724 - val_accuracy: 0.8734\n",
      "Epoch 618/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8547 - val_loss: 0.2698 - val_accuracy: 0.8831\n",
      "Epoch 619/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3044 - accuracy: 0.8561 - val_loss: 0.2699 - val_accuracy: 0.8831\n",
      "Epoch 620/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3046 - accuracy: 0.8561 - val_loss: 0.2698 - val_accuracy: 0.8831\n",
      "Epoch 621/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3045 - accuracy: 0.8561 - val_loss: 0.2686 - val_accuracy: 0.8831\n",
      "Epoch 622/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3039 - accuracy: 0.8547 - val_loss: 0.2700 - val_accuracy: 0.8766\n",
      "Epoch 623/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.8561 - val_loss: 0.2697 - val_accuracy: 0.8766\n",
      "Epoch 624/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3038 - accuracy: 0.8561 - val_loss: 0.2680 - val_accuracy: 0.8831\n",
      "Epoch 625/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3034 - accuracy: 0.8561 - val_loss: 0.2696 - val_accuracy: 0.8766\n",
      "Epoch 626/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3035 - accuracy: 0.8547 - val_loss: 0.2694 - val_accuracy: 0.8766\n",
      "Epoch 627/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.8561 - val_loss: 0.2675 - val_accuracy: 0.8831\n",
      "Epoch 628/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3033 - accuracy: 0.8561 - val_loss: 0.2686 - val_accuracy: 0.8831\n",
      "Epoch 629/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3030 - accuracy: 0.8561 - val_loss: 0.2681 - val_accuracy: 0.8831\n",
      "Epoch 630/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3030 - accuracy: 0.8534 - val_loss: 0.2697 - val_accuracy: 0.8734\n",
      "Epoch 631/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3030 - accuracy: 0.8561 - val_loss: 0.2685 - val_accuracy: 0.8831\n",
      "Epoch 632/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3024 - accuracy: 0.8561 - val_loss: 0.2666 - val_accuracy: 0.8831\n",
      "Epoch 633/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.8561 - val_loss: 0.2675 - val_accuracy: 0.8831\n",
      "Epoch 634/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3027 - accuracy: 0.8589 - val_loss: 0.2678 - val_accuracy: 0.8831\n",
      "Epoch 635/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.8561 - val_loss: 0.2669 - val_accuracy: 0.8831\n",
      "Epoch 636/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3022 - accuracy: 0.8561 - val_loss: 0.2672 - val_accuracy: 0.8831\n",
      "Epoch 637/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3019 - accuracy: 0.8561 - val_loss: 0.2676 - val_accuracy: 0.8864\n",
      "Epoch 638/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3016 - accuracy: 0.8561 - val_loss: 0.2661 - val_accuracy: 0.8831\n",
      "Epoch 639/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3014 - accuracy: 0.8561 - val_loss: 0.2671 - val_accuracy: 0.8831\n",
      "Epoch 640/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3015 - accuracy: 0.8575 - val_loss: 0.2671 - val_accuracy: 0.8864\n",
      "Epoch 641/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3014 - accuracy: 0.8589 - val_loss: 0.2671 - val_accuracy: 0.8864\n",
      "Epoch 642/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3012 - accuracy: 0.8589 - val_loss: 0.2676 - val_accuracy: 0.8799\n",
      "Epoch 643/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.8589 - val_loss: 0.2666 - val_accuracy: 0.8864\n",
      "Epoch 644/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3014 - accuracy: 0.8575 - val_loss: 0.2651 - val_accuracy: 0.8831\n",
      "Epoch 645/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3002 - accuracy: 0.8603 - val_loss: 0.2675 - val_accuracy: 0.8799\n",
      "Epoch 646/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3021 - accuracy: 0.8547 - val_loss: 0.2642 - val_accuracy: 0.8831\n",
      "Epoch 647/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3006 - accuracy: 0.8575 - val_loss: 0.2661 - val_accuracy: 0.8864\n",
      "Epoch 648/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3001 - accuracy: 0.8603 - val_loss: 0.2655 - val_accuracy: 0.8864\n",
      "Epoch 649/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3002 - accuracy: 0.8575 - val_loss: 0.2643 - val_accuracy: 0.8831\n",
      "Epoch 650/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3010 - accuracy: 0.8561 - val_loss: 0.2659 - val_accuracy: 0.8799\n",
      "Epoch 651/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2999 - accuracy: 0.8589 - val_loss: 0.2648 - val_accuracy: 0.8831\n",
      "Epoch 652/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2999 - accuracy: 0.8603 - val_loss: 0.2656 - val_accuracy: 0.8864\n",
      "Epoch 653/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3001 - accuracy: 0.8561 - val_loss: 0.2636 - val_accuracy: 0.8831\n",
      "Epoch 654/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.8561 - val_loss: 0.2659 - val_accuracy: 0.8799\n",
      "Epoch 655/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8561 - val_loss: 0.2641 - val_accuracy: 0.8831\n",
      "Epoch 656/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8561 - val_loss: 0.2637 - val_accuracy: 0.8831\n",
      "Epoch 657/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8561 - val_loss: 0.2636 - val_accuracy: 0.8831\n",
      "Epoch 658/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.8589 - val_loss: 0.2641 - val_accuracy: 0.8864\n",
      "Epoch 659/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.8561 - val_loss: 0.2637 - val_accuracy: 0.8831\n",
      "Epoch 660/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2987 - accuracy: 0.8589 - val_loss: 0.2647 - val_accuracy: 0.8799\n",
      "Epoch 661/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2989 - accuracy: 0.8561 - val_loss: 0.2629 - val_accuracy: 0.8831\n",
      "Epoch 662/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2987 - accuracy: 0.8589 - val_loss: 0.2647 - val_accuracy: 0.8799\n",
      "Epoch 663/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2985 - accuracy: 0.8589 - val_loss: 0.2632 - val_accuracy: 0.8864\n",
      "Epoch 664/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2981 - accuracy: 0.8561 - val_loss: 0.2625 - val_accuracy: 0.8831\n",
      "Epoch 665/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2982 - accuracy: 0.8575 - val_loss: 0.2627 - val_accuracy: 0.8864\n",
      "Epoch 666/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2981 - accuracy: 0.8589 - val_loss: 0.2632 - val_accuracy: 0.8799\n",
      "Epoch 667/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2977 - accuracy: 0.8547 - val_loss: 0.2614 - val_accuracy: 0.8831\n",
      "Epoch 668/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.8575 - val_loss: 0.2630 - val_accuracy: 0.8799\n",
      "Epoch 669/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8589 - val_loss: 0.2639 - val_accuracy: 0.8799\n",
      "Epoch 670/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.8547 - val_loss: 0.2610 - val_accuracy: 0.8831\n",
      "Epoch 671/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2970 - accuracy: 0.8603 - val_loss: 0.2642 - val_accuracy: 0.8799\n",
      "Epoch 672/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2969 - accuracy: 0.8589 - val_loss: 0.2618 - val_accuracy: 0.8864\n",
      "Epoch 673/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.8603 - val_loss: 0.2626 - val_accuracy: 0.8799\n",
      "Epoch 674/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.8589 - val_loss: 0.2621 - val_accuracy: 0.8864\n",
      "Epoch 675/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2969 - accuracy: 0.8589 - val_loss: 0.2605 - val_accuracy: 0.8896\n",
      "Epoch 676/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2970 - accuracy: 0.8575 - val_loss: 0.2631 - val_accuracy: 0.8799\n",
      "Epoch 677/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.8589 - val_loss: 0.2607 - val_accuracy: 0.8929\n",
      "Epoch 678/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8589 - val_loss: 0.2619 - val_accuracy: 0.8864\n",
      "Epoch 679/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2980 - accuracy: 0.8617 - val_loss: 0.2599 - val_accuracy: 0.8896\n",
      "Epoch 680/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2958 - accuracy: 0.8603 - val_loss: 0.2629 - val_accuracy: 0.8799\n",
      "Epoch 681/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2955 - accuracy: 0.8617 - val_loss: 0.2610 - val_accuracy: 0.8929\n",
      "Epoch 682/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2955 - accuracy: 0.8631 - val_loss: 0.2608 - val_accuracy: 0.8929\n",
      "Epoch 683/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2965 - accuracy: 0.8589 - val_loss: 0.2598 - val_accuracy: 0.8929\n",
      "Epoch 684/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2952 - accuracy: 0.8631 - val_loss: 0.2613 - val_accuracy: 0.8864\n",
      "Epoch 685/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2953 - accuracy: 0.8589 - val_loss: 0.2611 - val_accuracy: 0.8929\n",
      "Epoch 686/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2956 - accuracy: 0.8659 - val_loss: 0.2583 - val_accuracy: 0.8896\n",
      "Epoch 687/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8617 - val_loss: 0.2622 - val_accuracy: 0.8864\n",
      "Epoch 688/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2949 - accuracy: 0.8631 - val_loss: 0.2601 - val_accuracy: 0.8929\n",
      "Epoch 689/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2953 - accuracy: 0.8659 - val_loss: 0.2587 - val_accuracy: 0.8929\n",
      "Epoch 690/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2945 - accuracy: 0.8631 - val_loss: 0.2617 - val_accuracy: 0.8799\n",
      "Epoch 691/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2945 - accuracy: 0.8617 - val_loss: 0.2600 - val_accuracy: 0.8864\n",
      "Epoch 692/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2940 - accuracy: 0.8617 - val_loss: 0.2594 - val_accuracy: 0.8929\n",
      "Epoch 693/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2939 - accuracy: 0.8617 - val_loss: 0.2594 - val_accuracy: 0.8864\n",
      "Epoch 694/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2938 - accuracy: 0.8631 - val_loss: 0.2590 - val_accuracy: 0.8864\n",
      "Epoch 695/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2940 - accuracy: 0.8631 - val_loss: 0.2578 - val_accuracy: 0.8929\n",
      "Epoch 696/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.8589 - val_loss: 0.2616 - val_accuracy: 0.8929\n",
      "Epoch 697/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.8617 - val_loss: 0.2584 - val_accuracy: 0.8929\n",
      "Epoch 698/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8631 - val_loss: 0.2593 - val_accuracy: 0.8864\n",
      "Epoch 699/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2947 - accuracy: 0.8631 - val_loss: 0.2597 - val_accuracy: 0.8864\n",
      "Epoch 700/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2935 - accuracy: 0.8617 - val_loss: 0.2575 - val_accuracy: 0.8929\n",
      "Epoch 701/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2933 - accuracy: 0.8631 - val_loss: 0.2590 - val_accuracy: 0.8864\n",
      "Epoch 702/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2926 - accuracy: 0.8617 - val_loss: 0.2576 - val_accuracy: 0.8929\n",
      "Epoch 703/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2926 - accuracy: 0.8617 - val_loss: 0.2583 - val_accuracy: 0.8864\n",
      "Epoch 704/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2926 - accuracy: 0.8659 - val_loss: 0.2569 - val_accuracy: 0.8929\n",
      "Epoch 705/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2921 - accuracy: 0.8631 - val_loss: 0.2586 - val_accuracy: 0.8864\n",
      "Epoch 706/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2921 - accuracy: 0.8617 - val_loss: 0.2578 - val_accuracy: 0.8864\n",
      "Epoch 707/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2918 - accuracy: 0.8631 - val_loss: 0.2571 - val_accuracy: 0.8929\n",
      "Epoch 708/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2917 - accuracy: 0.8645 - val_loss: 0.2573 - val_accuracy: 0.8864\n",
      "Epoch 709/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2916 - accuracy: 0.8617 - val_loss: 0.2581 - val_accuracy: 0.8864\n",
      "Epoch 710/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2915 - accuracy: 0.8617 - val_loss: 0.2572 - val_accuracy: 0.8864\n",
      "Epoch 711/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2913 - accuracy: 0.8617 - val_loss: 0.2575 - val_accuracy: 0.8864\n",
      "Epoch 712/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2918 - accuracy: 0.8645 - val_loss: 0.2560 - val_accuracy: 0.8929\n",
      "Epoch 713/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2906 - accuracy: 0.8617 - val_loss: 0.2583 - val_accuracy: 0.8864\n",
      "Epoch 714/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2913 - accuracy: 0.8617 - val_loss: 0.2572 - val_accuracy: 0.8864\n",
      "Epoch 715/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2908 - accuracy: 0.8617 - val_loss: 0.2574 - val_accuracy: 0.8864\n",
      "Epoch 716/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2908 - accuracy: 0.8673 - val_loss: 0.2555 - val_accuracy: 0.8929\n",
      "Epoch 717/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8617 - val_loss: 0.2586 - val_accuracy: 0.8929\n",
      "Epoch 718/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8645 - val_loss: 0.2562 - val_accuracy: 0.8864\n",
      "Epoch 719/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2903 - accuracy: 0.8673 - val_loss: 0.2549 - val_accuracy: 0.8929\n",
      "Epoch 720/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2906 - accuracy: 0.8673 - val_loss: 0.2549 - val_accuracy: 0.8929\n",
      "Epoch 721/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2900 - accuracy: 0.8617 - val_loss: 0.2564 - val_accuracy: 0.8864\n",
      "Epoch 722/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2898 - accuracy: 0.8617 - val_loss: 0.2558 - val_accuracy: 0.8864\n",
      "Epoch 723/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2898 - accuracy: 0.8617 - val_loss: 0.2566 - val_accuracy: 0.8864\n",
      "Epoch 724/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2899 - accuracy: 0.8659 - val_loss: 0.2548 - val_accuracy: 0.8929\n",
      "Epoch 725/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2895 - accuracy: 0.8659 - val_loss: 0.2570 - val_accuracy: 0.8929\n",
      "Epoch 726/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2894 - accuracy: 0.8631 - val_loss: 0.2564 - val_accuracy: 0.8864\n",
      "Epoch 727/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2897 - accuracy: 0.8631 - val_loss: 0.2554 - val_accuracy: 0.8864\n",
      "Epoch 728/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2895 - accuracy: 0.8687 - val_loss: 0.2549 - val_accuracy: 0.8864\n",
      "Epoch 729/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8645 - val_loss: 0.2555 - val_accuracy: 0.8864\n",
      "Epoch 730/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8631 - val_loss: 0.2545 - val_accuracy: 0.8864\n",
      "Epoch 731/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2886 - accuracy: 0.8659 - val_loss: 0.2544 - val_accuracy: 0.8864\n",
      "Epoch 732/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2881 - accuracy: 0.8631 - val_loss: 0.2556 - val_accuracy: 0.8929\n",
      "Epoch 733/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8673 - val_loss: 0.2549 - val_accuracy: 0.8864\n",
      "Epoch 734/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2885 - accuracy: 0.8617 - val_loss: 0.2540 - val_accuracy: 0.8864\n",
      "Epoch 735/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.8659 - val_loss: 0.2543 - val_accuracy: 0.8864\n",
      "Epoch 736/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2896 - accuracy: 0.8631 - val_loss: 0.2555 - val_accuracy: 0.8961\n",
      "Epoch 737/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2877 - accuracy: 0.8701 - val_loss: 0.2526 - val_accuracy: 0.8929\n",
      "Epoch 738/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.8659 - val_loss: 0.2543 - val_accuracy: 0.8864\n",
      "Epoch 739/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2877 - accuracy: 0.8673 - val_loss: 0.2543 - val_accuracy: 0.8864\n",
      "Epoch 740/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2893 - accuracy: 0.8687 - val_loss: 0.2530 - val_accuracy: 0.8864\n",
      "Epoch 741/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2873 - accuracy: 0.8659 - val_loss: 0.2558 - val_accuracy: 0.8961\n",
      "Epoch 742/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2872 - accuracy: 0.8687 - val_loss: 0.2538 - val_accuracy: 0.8896\n",
      "Epoch 743/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2874 - accuracy: 0.8687 - val_loss: 0.2521 - val_accuracy: 0.8929\n",
      "Epoch 744/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8659 - val_loss: 0.2538 - val_accuracy: 0.8896\n",
      "Epoch 745/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.8659 - val_loss: 0.2539 - val_accuracy: 0.8961\n",
      "Epoch 746/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2875 - accuracy: 0.8631 - val_loss: 0.2517 - val_accuracy: 0.8929\n",
      "Epoch 747/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2869 - accuracy: 0.8687 - val_loss: 0.2546 - val_accuracy: 0.8961\n",
      "Epoch 748/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8687 - val_loss: 0.2520 - val_accuracy: 0.8929\n",
      "Epoch 749/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2861 - accuracy: 0.8659 - val_loss: 0.2518 - val_accuracy: 0.8864\n",
      "Epoch 750/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2864 - accuracy: 0.8645 - val_loss: 0.2519 - val_accuracy: 0.8864\n",
      "Epoch 751/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8659 - val_loss: 0.2523 - val_accuracy: 0.8864\n",
      "Epoch 752/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.8673 - val_loss: 0.2512 - val_accuracy: 0.8896\n",
      "Epoch 753/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2857 - accuracy: 0.8687 - val_loss: 0.2522 - val_accuracy: 0.8896\n",
      "Epoch 754/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.8673 - val_loss: 0.2517 - val_accuracy: 0.8864\n",
      "Epoch 755/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2860 - accuracy: 0.8659 - val_loss: 0.2534 - val_accuracy: 0.9058\n",
      "Epoch 756/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.8701 - val_loss: 0.2506 - val_accuracy: 0.8896\n",
      "Epoch 757/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2847 - accuracy: 0.8645 - val_loss: 0.2517 - val_accuracy: 0.8896\n",
      "Epoch 758/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8701 - val_loss: 0.2513 - val_accuracy: 0.8896\n",
      "Epoch 759/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2849 - accuracy: 0.8715 - val_loss: 0.2524 - val_accuracy: 0.8961\n",
      "Epoch 760/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.8673 - val_loss: 0.2500 - val_accuracy: 0.8896\n",
      "Epoch 761/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8687 - val_loss: 0.2515 - val_accuracy: 0.8896\n",
      "Epoch 762/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.8701 - val_loss: 0.2538 - val_accuracy: 0.9058\n",
      "Epoch 763/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.8687 - val_loss: 0.2505 - val_accuracy: 0.8961\n",
      "Epoch 764/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2838 - accuracy: 0.8701 - val_loss: 0.2506 - val_accuracy: 0.8896\n",
      "Epoch 765/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8659 - val_loss: 0.2499 - val_accuracy: 0.8896\n",
      "Epoch 766/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2835 - accuracy: 0.8715 - val_loss: 0.2521 - val_accuracy: 0.9058\n",
      "Epoch 767/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2835 - accuracy: 0.8729 - val_loss: 0.2511 - val_accuracy: 0.8961\n",
      "Epoch 768/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.8715 - val_loss: 0.2498 - val_accuracy: 0.8896\n",
      "Epoch 769/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2838 - accuracy: 0.8729 - val_loss: 0.2503 - val_accuracy: 0.8896\n",
      "Epoch 770/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2833 - accuracy: 0.8659 - val_loss: 0.2495 - val_accuracy: 0.8896\n",
      "Epoch 771/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2830 - accuracy: 0.8715 - val_loss: 0.2507 - val_accuracy: 0.9058\n",
      "Epoch 772/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8715 - val_loss: 0.2495 - val_accuracy: 0.8896\n",
      "Epoch 773/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2828 - accuracy: 0.8701 - val_loss: 0.2498 - val_accuracy: 0.8896\n",
      "Epoch 774/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8729 - val_loss: 0.2503 - val_accuracy: 0.8961\n",
      "Epoch 775/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2823 - accuracy: 0.8701 - val_loss: 0.2487 - val_accuracy: 0.8929\n",
      "Epoch 776/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.8715 - val_loss: 0.2505 - val_accuracy: 0.9058\n",
      "Epoch 777/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2834 - accuracy: 0.8631 - val_loss: 0.2491 - val_accuracy: 0.8896\n",
      "Epoch 778/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.8701 - val_loss: 0.2524 - val_accuracy: 0.9058\n",
      "Epoch 779/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8729 - val_loss: 0.2481 - val_accuracy: 0.8929\n",
      "Epoch 780/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2817 - accuracy: 0.8701 - val_loss: 0.2488 - val_accuracy: 0.8896\n",
      "Epoch 781/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.8701 - val_loss: 0.2491 - val_accuracy: 0.9058\n",
      "Epoch 782/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.8687 - val_loss: 0.2479 - val_accuracy: 0.8929\n",
      "Epoch 783/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8687 - val_loss: 0.2483 - val_accuracy: 0.8864\n",
      "Epoch 784/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2814 - accuracy: 0.8771 - val_loss: 0.2501 - val_accuracy: 0.9058\n",
      "Epoch 785/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.8729 - val_loss: 0.2480 - val_accuracy: 0.8864\n",
      "Epoch 786/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2808 - accuracy: 0.8673 - val_loss: 0.2492 - val_accuracy: 0.9058\n",
      "Epoch 787/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2807 - accuracy: 0.8715 - val_loss: 0.2484 - val_accuracy: 0.8864\n",
      "Epoch 788/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2809 - accuracy: 0.8729 - val_loss: 0.2491 - val_accuracy: 0.9058\n",
      "Epoch 789/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2807 - accuracy: 0.8715 - val_loss: 0.2482 - val_accuracy: 0.8994\n",
      "Epoch 790/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2806 - accuracy: 0.8771 - val_loss: 0.2492 - val_accuracy: 0.9058\n",
      "Epoch 791/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2802 - accuracy: 0.8729 - val_loss: 0.2476 - val_accuracy: 0.8864\n",
      "Epoch 792/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2805 - accuracy: 0.8673 - val_loss: 0.2476 - val_accuracy: 0.8961\n",
      "Epoch 793/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.8673 - val_loss: 0.2472 - val_accuracy: 0.8831\n",
      "Epoch 794/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2797 - accuracy: 0.8701 - val_loss: 0.2487 - val_accuracy: 0.9058\n",
      "Epoch 795/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.8673 - val_loss: 0.2466 - val_accuracy: 0.8831\n",
      "Epoch 796/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2800 - accuracy: 0.8687 - val_loss: 0.2489 - val_accuracy: 0.9058\n",
      "Epoch 797/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8729 - val_loss: 0.2477 - val_accuracy: 0.9026\n",
      "Epoch 798/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8687 - val_loss: 0.2464 - val_accuracy: 0.8896\n",
      "Epoch 799/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2800 - accuracy: 0.8729 - val_loss: 0.2472 - val_accuracy: 0.9026\n",
      "Epoch 800/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2796 - accuracy: 0.8687 - val_loss: 0.2464 - val_accuracy: 0.8929\n",
      "Epoch 801/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.8687 - val_loss: 0.2477 - val_accuracy: 0.9058\n",
      "Epoch 802/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.8701 - val_loss: 0.2470 - val_accuracy: 0.9026\n",
      "Epoch 803/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.8757 - val_loss: 0.2474 - val_accuracy: 0.9058\n",
      "Epoch 804/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2783 - accuracy: 0.8757 - val_loss: 0.2470 - val_accuracy: 0.9026\n",
      "Epoch 805/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2784 - accuracy: 0.8687 - val_loss: 0.2463 - val_accuracy: 0.9026\n",
      "Epoch 806/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8743 - val_loss: 0.2480 - val_accuracy: 0.9058\n",
      "Epoch 807/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.8673 - val_loss: 0.2467 - val_accuracy: 0.9026\n",
      "Epoch 808/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2784 - accuracy: 0.8729 - val_loss: 0.2458 - val_accuracy: 0.8994\n",
      "Epoch 809/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2779 - accuracy: 0.8715 - val_loss: 0.2464 - val_accuracy: 0.9026\n",
      "Epoch 810/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2777 - accuracy: 0.8715 - val_loss: 0.2457 - val_accuracy: 0.8929\n",
      "Epoch 811/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2778 - accuracy: 0.8687 - val_loss: 0.2458 - val_accuracy: 0.9026\n",
      "Epoch 812/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2773 - accuracy: 0.8701 - val_loss: 0.2460 - val_accuracy: 0.9026\n",
      "Epoch 813/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2770 - accuracy: 0.8687 - val_loss: 0.2454 - val_accuracy: 0.8929\n",
      "Epoch 814/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.8687 - val_loss: 0.2443 - val_accuracy: 0.8896\n",
      "Epoch 815/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2767 - accuracy: 0.8813 - val_loss: 0.2479 - val_accuracy: 0.9091\n",
      "Epoch 816/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8701 - val_loss: 0.2448 - val_accuracy: 0.8929\n",
      "Epoch 817/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2776 - accuracy: 0.8785 - val_loss: 0.2480 - val_accuracy: 0.9123\n",
      "Epoch 818/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2764 - accuracy: 0.8827 - val_loss: 0.2458 - val_accuracy: 0.9026\n",
      "Epoch 819/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2767 - accuracy: 0.8645 - val_loss: 0.2447 - val_accuracy: 0.8929\n",
      "Epoch 820/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.8659 - val_loss: 0.2452 - val_accuracy: 0.9026\n",
      "Epoch 821/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2768 - accuracy: 0.8771 - val_loss: 0.2449 - val_accuracy: 0.8994\n",
      "Epoch 822/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.8701 - val_loss: 0.2444 - val_accuracy: 0.8929\n",
      "Epoch 823/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.8687 - val_loss: 0.2446 - val_accuracy: 0.8994\n",
      "Epoch 824/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.8799 - val_loss: 0.2470 - val_accuracy: 0.9091\n",
      "Epoch 825/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2758 - accuracy: 0.8729 - val_loss: 0.2440 - val_accuracy: 0.8929\n",
      "Epoch 826/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.8673 - val_loss: 0.2442 - val_accuracy: 0.8994\n",
      "Epoch 827/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.8729 - val_loss: 0.2438 - val_accuracy: 0.8929\n",
      "Epoch 828/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2753 - accuracy: 0.8673 - val_loss: 0.2444 - val_accuracy: 0.8994\n",
      "Epoch 829/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2769 - accuracy: 0.8799 - val_loss: 0.2448 - val_accuracy: 0.9026\n",
      "Epoch 830/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2751 - accuracy: 0.8687 - val_loss: 0.2435 - val_accuracy: 0.8929\n",
      "Epoch 831/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2750 - accuracy: 0.8771 - val_loss: 0.2455 - val_accuracy: 0.9058\n",
      "Epoch 832/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2744 - accuracy: 0.8813 - val_loss: 0.2436 - val_accuracy: 0.8994\n",
      "Epoch 833/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.8715 - val_loss: 0.2433 - val_accuracy: 0.8929\n",
      "Epoch 834/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2743 - accuracy: 0.8799 - val_loss: 0.2454 - val_accuracy: 0.9058\n",
      "Epoch 835/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2748 - accuracy: 0.8799 - val_loss: 0.2434 - val_accuracy: 0.8994\n",
      "Epoch 836/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2739 - accuracy: 0.8743 - val_loss: 0.2433 - val_accuracy: 0.8994\n",
      "Epoch 837/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2738 - accuracy: 0.8715 - val_loss: 0.2431 - val_accuracy: 0.8994\n",
      "Epoch 838/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2736 - accuracy: 0.8771 - val_loss: 0.2440 - val_accuracy: 0.8994\n",
      "Epoch 839/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2738 - accuracy: 0.8771 - val_loss: 0.2437 - val_accuracy: 0.8994\n",
      "Epoch 840/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2733 - accuracy: 0.8771 - val_loss: 0.2442 - val_accuracy: 0.8994\n",
      "Epoch 841/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8743 - val_loss: 0.2442 - val_accuracy: 0.8994\n",
      "Epoch 842/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2738 - accuracy: 0.8771 - val_loss: 0.2444 - val_accuracy: 0.9026\n",
      "Epoch 843/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2726 - accuracy: 0.8771 - val_loss: 0.2424 - val_accuracy: 0.8929\n",
      "Epoch 844/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2737 - accuracy: 0.8785 - val_loss: 0.2431 - val_accuracy: 0.8994\n",
      "Epoch 845/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2731 - accuracy: 0.8757 - val_loss: 0.2426 - val_accuracy: 0.8994\n",
      "Epoch 846/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2734 - accuracy: 0.8729 - val_loss: 0.2425 - val_accuracy: 0.8994\n",
      "Epoch 847/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2731 - accuracy: 0.8785 - val_loss: 0.2439 - val_accuracy: 0.9026\n",
      "Epoch 848/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2724 - accuracy: 0.8771 - val_loss: 0.2418 - val_accuracy: 0.8929\n",
      "Epoch 849/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2725 - accuracy: 0.8785 - val_loss: 0.2418 - val_accuracy: 0.8994\n",
      "Epoch 850/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2724 - accuracy: 0.8771 - val_loss: 0.2421 - val_accuracy: 0.8994\n",
      "Epoch 851/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2718 - accuracy: 0.8757 - val_loss: 0.2418 - val_accuracy: 0.8994\n",
      "Epoch 852/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2719 - accuracy: 0.8813 - val_loss: 0.2427 - val_accuracy: 0.9026\n",
      "Epoch 853/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2718 - accuracy: 0.8771 - val_loss: 0.2418 - val_accuracy: 0.8994\n",
      "Epoch 854/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2713 - accuracy: 0.8771 - val_loss: 0.2417 - val_accuracy: 0.8994\n",
      "Epoch 855/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2716 - accuracy: 0.8813 - val_loss: 0.2427 - val_accuracy: 0.9026\n",
      "Epoch 856/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2716 - accuracy: 0.8785 - val_loss: 0.2413 - val_accuracy: 0.8994\n",
      "Epoch 857/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2711 - accuracy: 0.8771 - val_loss: 0.2415 - val_accuracy: 0.8994\n",
      "Epoch 858/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2713 - accuracy: 0.8813 - val_loss: 0.2423 - val_accuracy: 0.9026\n",
      "Epoch 859/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2714 - accuracy: 0.8757 - val_loss: 0.2414 - val_accuracy: 0.8994\n",
      "Epoch 860/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2711 - accuracy: 0.8813 - val_loss: 0.2429 - val_accuracy: 0.9026\n",
      "Epoch 861/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2715 - accuracy: 0.8813 - val_loss: 0.2406 - val_accuracy: 0.9058\n",
      "Epoch 862/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2711 - accuracy: 0.8799 - val_loss: 0.2413 - val_accuracy: 0.9026\n",
      "Epoch 863/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2707 - accuracy: 0.8771 - val_loss: 0.2409 - val_accuracy: 0.8994\n",
      "Epoch 864/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2709 - accuracy: 0.8799 - val_loss: 0.2412 - val_accuracy: 0.9026\n",
      "Epoch 865/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2703 - accuracy: 0.8785 - val_loss: 0.2408 - val_accuracy: 0.9026\n",
      "Epoch 866/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8827 - val_loss: 0.2398 - val_accuracy: 0.9058\n",
      "Epoch 867/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2699 - accuracy: 0.8785 - val_loss: 0.2412 - val_accuracy: 0.9026\n",
      "Epoch 868/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2703 - accuracy: 0.8799 - val_loss: 0.2419 - val_accuracy: 0.9026\n",
      "Epoch 869/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2695 - accuracy: 0.8799 - val_loss: 0.2406 - val_accuracy: 0.8994\n",
      "Epoch 870/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8827 - val_loss: 0.2404 - val_accuracy: 0.9058\n",
      "Epoch 871/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2690 - accuracy: 0.8799 - val_loss: 0.2410 - val_accuracy: 0.9026\n",
      "Epoch 872/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.8813 - val_loss: 0.2402 - val_accuracy: 0.8994\n",
      "Epoch 873/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.8827 - val_loss: 0.2395 - val_accuracy: 0.8994\n",
      "Epoch 874/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2693 - accuracy: 0.8785 - val_loss: 0.2407 - val_accuracy: 0.9026\n",
      "Epoch 875/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2687 - accuracy: 0.8813 - val_loss: 0.2399 - val_accuracy: 0.9058\n",
      "Epoch 876/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.8841 - val_loss: 0.2391 - val_accuracy: 0.9058\n",
      "Epoch 877/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2685 - accuracy: 0.8841 - val_loss: 0.2403 - val_accuracy: 0.9026\n",
      "Epoch 878/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8813 - val_loss: 0.2397 - val_accuracy: 0.9091\n",
      "Epoch 879/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2684 - accuracy: 0.8827 - val_loss: 0.2390 - val_accuracy: 0.9058\n",
      "Epoch 880/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.8771 - val_loss: 0.2404 - val_accuracy: 0.9026\n",
      "Epoch 881/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2680 - accuracy: 0.8771 - val_loss: 0.2404 - val_accuracy: 0.9026\n",
      "Epoch 882/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2680 - accuracy: 0.8827 - val_loss: 0.2391 - val_accuracy: 0.9058\n",
      "Epoch 883/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2678 - accuracy: 0.8841 - val_loss: 0.2386 - val_accuracy: 0.9058\n",
      "Epoch 884/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.8757 - val_loss: 0.2408 - val_accuracy: 0.9026\n",
      "Epoch 885/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2670 - accuracy: 0.8841 - val_loss: 0.2384 - val_accuracy: 0.9058\n",
      "Epoch 886/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2675 - accuracy: 0.8841 - val_loss: 0.2387 - val_accuracy: 0.9091\n",
      "Epoch 887/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.8827 - val_loss: 0.2396 - val_accuracy: 0.9026\n",
      "Epoch 888/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2668 - accuracy: 0.8841 - val_loss: 0.2399 - val_accuracy: 0.9026\n",
      "Epoch 889/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2673 - accuracy: 0.8841 - val_loss: 0.2384 - val_accuracy: 0.9091\n",
      "Epoch 890/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2666 - accuracy: 0.8869 - val_loss: 0.2390 - val_accuracy: 0.9091\n",
      "Epoch 891/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2669 - accuracy: 0.8743 - val_loss: 0.2394 - val_accuracy: 0.9026\n",
      "Epoch 892/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.8841 - val_loss: 0.2375 - val_accuracy: 0.9058\n",
      "Epoch 893/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.8841 - val_loss: 0.2395 - val_accuracy: 0.9026\n",
      "Epoch 894/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2658 - accuracy: 0.8813 - val_loss: 0.2380 - val_accuracy: 0.9091\n",
      "Epoch 895/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.8883 - val_loss: 0.2378 - val_accuracy: 0.9091\n",
      "Epoch 896/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.8827 - val_loss: 0.2386 - val_accuracy: 0.9091\n",
      "Epoch 897/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.8869 - val_loss: 0.2380 - val_accuracy: 0.9091\n",
      "Epoch 898/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8827 - val_loss: 0.2389 - val_accuracy: 0.9026\n",
      "Epoch 899/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.8813 - val_loss: 0.2386 - val_accuracy: 0.9026\n",
      "Epoch 900/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8869 - val_loss: 0.2370 - val_accuracy: 0.9091\n",
      "Epoch 901/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.8813 - val_loss: 0.2392 - val_accuracy: 0.9026\n",
      "Epoch 902/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.8855 - val_loss: 0.2375 - val_accuracy: 0.9091\n",
      "Epoch 903/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8841 - val_loss: 0.2377 - val_accuracy: 0.9091\n",
      "Epoch 904/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.8827 - val_loss: 0.2371 - val_accuracy: 0.9091\n",
      "Epoch 905/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2648 - accuracy: 0.8841 - val_loss: 0.2377 - val_accuracy: 0.9091\n",
      "Epoch 906/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2649 - accuracy: 0.8855 - val_loss: 0.2366 - val_accuracy: 0.9091\n",
      "Epoch 907/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.8813 - val_loss: 0.2375 - val_accuracy: 0.9091\n",
      "Epoch 908/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2645 - accuracy: 0.8841 - val_loss: 0.2373 - val_accuracy: 0.9091\n",
      "Epoch 909/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2637 - accuracy: 0.8883 - val_loss: 0.2363 - val_accuracy: 0.9091\n",
      "Epoch 910/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2645 - accuracy: 0.8883 - val_loss: 0.2358 - val_accuracy: 0.9058\n",
      "Epoch 911/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8869 - val_loss: 0.2364 - val_accuracy: 0.9091\n",
      "Epoch 912/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.8827 - val_loss: 0.2369 - val_accuracy: 0.9091\n",
      "Epoch 913/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2641 - accuracy: 0.8841 - val_loss: 0.2358 - val_accuracy: 0.9091\n",
      "Epoch 914/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.8869 - val_loss: 0.2370 - val_accuracy: 0.9091\n",
      "Epoch 915/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.8799 - val_loss: 0.2369 - val_accuracy: 0.9091\n",
      "Epoch 916/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.8841 - val_loss: 0.2364 - val_accuracy: 0.9091\n",
      "Epoch 917/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2629 - accuracy: 0.8883 - val_loss: 0.2363 - val_accuracy: 0.9091\n",
      "Epoch 918/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.8869 - val_loss: 0.2373 - val_accuracy: 0.9091\n",
      "Epoch 919/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2629 - accuracy: 0.8883 - val_loss: 0.2359 - val_accuracy: 0.9091\n",
      "Epoch 920/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2627 - accuracy: 0.8883 - val_loss: 0.2363 - val_accuracy: 0.9091\n",
      "Epoch 921/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2632 - accuracy: 0.8799 - val_loss: 0.2368 - val_accuracy: 0.9091\n",
      "Epoch 922/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2626 - accuracy: 0.8855 - val_loss: 0.2348 - val_accuracy: 0.9123\n",
      "Epoch 923/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.8869 - val_loss: 0.2370 - val_accuracy: 0.9091\n",
      "Epoch 924/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2623 - accuracy: 0.8841 - val_loss: 0.2357 - val_accuracy: 0.9091\n",
      "Epoch 925/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2640 - accuracy: 0.8855 - val_loss: 0.2379 - val_accuracy: 0.9058\n",
      "Epoch 926/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2632 - accuracy: 0.8813 - val_loss: 0.2342 - val_accuracy: 0.9156\n",
      "Epoch 927/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2628 - accuracy: 0.8799 - val_loss: 0.2368 - val_accuracy: 0.9026\n",
      "Epoch 928/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2613 - accuracy: 0.8869 - val_loss: 0.2352 - val_accuracy: 0.9091\n",
      "Epoch 929/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2612 - accuracy: 0.8883 - val_loss: 0.2348 - val_accuracy: 0.9091\n",
      "Epoch 930/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2612 - accuracy: 0.8883 - val_loss: 0.2349 - val_accuracy: 0.9091\n",
      "Epoch 931/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2609 - accuracy: 0.8841 - val_loss: 0.2358 - val_accuracy: 0.9091\n",
      "Epoch 932/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2610 - accuracy: 0.8869 - val_loss: 0.2358 - val_accuracy: 0.9091\n",
      "Epoch 933/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2607 - accuracy: 0.8841 - val_loss: 0.2359 - val_accuracy: 0.9026\n",
      "Epoch 934/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.8883 - val_loss: 0.2345 - val_accuracy: 0.9091\n",
      "Epoch 935/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2604 - accuracy: 0.8883 - val_loss: 0.2345 - val_accuracy: 0.9091\n",
      "Epoch 936/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2607 - accuracy: 0.8883 - val_loss: 0.2346 - val_accuracy: 0.9091\n",
      "Epoch 937/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2607 - accuracy: 0.8883 - val_loss: 0.2339 - val_accuracy: 0.9091\n",
      "Epoch 938/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2603 - accuracy: 0.8855 - val_loss: 0.2359 - val_accuracy: 0.9058\n",
      "Epoch 939/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2600 - accuracy: 0.8883 - val_loss: 0.2332 - val_accuracy: 0.9156\n",
      "Epoch 940/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.8883 - val_loss: 0.2344 - val_accuracy: 0.9091\n",
      "Epoch 941/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2599 - accuracy: 0.8883 - val_loss: 0.2330 - val_accuracy: 0.9156\n",
      "Epoch 942/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2600 - accuracy: 0.8883 - val_loss: 0.2347 - val_accuracy: 0.9026\n",
      "Epoch 943/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.8883 - val_loss: 0.2344 - val_accuracy: 0.9091\n",
      "Epoch 944/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2594 - accuracy: 0.8883 - val_loss: 0.2334 - val_accuracy: 0.9091\n",
      "Epoch 945/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.8883 - val_loss: 0.2340 - val_accuracy: 0.9091\n",
      "Epoch 946/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.8897 - val_loss: 0.2339 - val_accuracy: 0.9091\n",
      "Epoch 947/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2591 - accuracy: 0.8883 - val_loss: 0.2333 - val_accuracy: 0.9091\n",
      "Epoch 948/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.8869 - val_loss: 0.2343 - val_accuracy: 0.9026\n",
      "Epoch 949/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2582 - accuracy: 0.8883 - val_loss: 0.2328 - val_accuracy: 0.9091\n",
      "Epoch 950/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2589 - accuracy: 0.8883 - val_loss: 0.2327 - val_accuracy: 0.9156\n",
      "Epoch 951/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.8883 - val_loss: 0.2345 - val_accuracy: 0.9058\n",
      "Epoch 952/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2583 - accuracy: 0.8897 - val_loss: 0.2327 - val_accuracy: 0.9123\n",
      "Epoch 953/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.8897 - val_loss: 0.2333 - val_accuracy: 0.9091\n",
      "Epoch 954/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2586 - accuracy: 0.8911 - val_loss: 0.2328 - val_accuracy: 0.9091\n",
      "Epoch 955/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2580 - accuracy: 0.8869 - val_loss: 0.2338 - val_accuracy: 0.9026\n",
      "Epoch 956/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2576 - accuracy: 0.8883 - val_loss: 0.2335 - val_accuracy: 0.9026\n",
      "Epoch 957/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2581 - accuracy: 0.8869 - val_loss: 0.2324 - val_accuracy: 0.9188\n",
      "Epoch 958/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.8883 - val_loss: 0.2336 - val_accuracy: 0.9026\n",
      "Epoch 959/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2577 - accuracy: 0.8911 - val_loss: 0.2313 - val_accuracy: 0.9188\n",
      "Epoch 960/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2572 - accuracy: 0.8883 - val_loss: 0.2337 - val_accuracy: 0.9091\n",
      "Epoch 961/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2575 - accuracy: 0.8911 - val_loss: 0.2317 - val_accuracy: 0.9091\n",
      "Epoch 962/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2567 - accuracy: 0.8883 - val_loss: 0.2321 - val_accuracy: 0.9091\n",
      "Epoch 963/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2571 - accuracy: 0.8897 - val_loss: 0.2322 - val_accuracy: 0.9091\n",
      "Epoch 964/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.8869 - val_loss: 0.2333 - val_accuracy: 0.9091\n",
      "Epoch 965/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.8939 - val_loss: 0.2325 - val_accuracy: 0.9026\n",
      "Epoch 966/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.8841 - val_loss: 0.2315 - val_accuracy: 0.9091\n",
      "Epoch 967/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2561 - accuracy: 0.8883 - val_loss: 0.2320 - val_accuracy: 0.9091\n",
      "Epoch 968/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.8883 - val_loss: 0.2315 - val_accuracy: 0.9091\n",
      "Epoch 969/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2565 - accuracy: 0.8911 - val_loss: 0.2324 - val_accuracy: 0.9058\n",
      "Epoch 970/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2555 - accuracy: 0.8897 - val_loss: 0.2307 - val_accuracy: 0.9188\n",
      "Epoch 971/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2558 - accuracy: 0.8869 - val_loss: 0.2311 - val_accuracy: 0.9156\n",
      "Epoch 972/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2556 - accuracy: 0.8897 - val_loss: 0.2306 - val_accuracy: 0.9156\n",
      "Epoch 973/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.8883 - val_loss: 0.2314 - val_accuracy: 0.9123\n",
      "Epoch 974/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2553 - accuracy: 0.8911 - val_loss: 0.2317 - val_accuracy: 0.9091\n",
      "Epoch 975/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.8869 - val_loss: 0.2307 - val_accuracy: 0.9091\n",
      "Epoch 976/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2562 - accuracy: 0.8855 - val_loss: 0.2340 - val_accuracy: 0.9156\n",
      "Epoch 977/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2550 - accuracy: 0.8939 - val_loss: 0.2302 - val_accuracy: 0.9253\n",
      "Epoch 978/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2548 - accuracy: 0.8911 - val_loss: 0.2304 - val_accuracy: 0.9253\n",
      "Epoch 979/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2550 - accuracy: 0.8883 - val_loss: 0.2304 - val_accuracy: 0.9188\n",
      "Epoch 980/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2544 - accuracy: 0.8911 - val_loss: 0.2323 - val_accuracy: 0.9156\n",
      "Epoch 981/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2545 - accuracy: 0.8925 - val_loss: 0.2305 - val_accuracy: 0.9091\n",
      "Epoch 982/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.8911 - val_loss: 0.2301 - val_accuracy: 0.9091\n",
      "Epoch 983/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2556 - accuracy: 0.8897 - val_loss: 0.2322 - val_accuracy: 0.9156\n",
      "Epoch 984/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2538 - accuracy: 0.8897 - val_loss: 0.2293 - val_accuracy: 0.9253\n",
      "Epoch 985/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2542 - accuracy: 0.8897 - val_loss: 0.2296 - val_accuracy: 0.9188\n",
      "Epoch 986/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2536 - accuracy: 0.8897 - val_loss: 0.2303 - val_accuracy: 0.9091\n",
      "Epoch 987/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.8911 - val_loss: 0.2303 - val_accuracy: 0.9058\n",
      "Epoch 988/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.8897 - val_loss: 0.2298 - val_accuracy: 0.9091\n",
      "Epoch 989/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.8911 - val_loss: 0.2304 - val_accuracy: 0.9091\n",
      "Epoch 990/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.8897 - val_loss: 0.2301 - val_accuracy: 0.9058\n",
      "Epoch 991/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.8911 - val_loss: 0.2298 - val_accuracy: 0.9058\n",
      "Epoch 992/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2528 - accuracy: 0.8883 - val_loss: 0.2313 - val_accuracy: 0.9156\n",
      "Epoch 993/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.8911 - val_loss: 0.2291 - val_accuracy: 0.9221\n",
      "Epoch 994/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2524 - accuracy: 0.8939 - val_loss: 0.2302 - val_accuracy: 0.9058\n",
      "Epoch 995/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.8925 - val_loss: 0.2294 - val_accuracy: 0.9156\n",
      "Epoch 996/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.8939 - val_loss: 0.2291 - val_accuracy: 0.9221\n",
      "Epoch 997/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.8911 - val_loss: 0.2298 - val_accuracy: 0.9091\n",
      "Epoch 998/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2524 - accuracy: 0.8939 - val_loss: 0.2291 - val_accuracy: 0.9156\n",
      "Epoch 999/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.8953 - val_loss: 0.2282 - val_accuracy: 0.9286\n",
      "Epoch 1000/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.8925 - val_loss: 0.2292 - val_accuracy: 0.9091\n",
      "Epoch 1001/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2523 - accuracy: 0.8925 - val_loss: 0.2286 - val_accuracy: 0.9221\n",
      "Epoch 1002/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.8939 - val_loss: 0.2289 - val_accuracy: 0.9156\n",
      "Epoch 1003/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.8939 - val_loss: 0.2280 - val_accuracy: 0.9286\n",
      "Epoch 1004/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2509 - accuracy: 0.8911 - val_loss: 0.2289 - val_accuracy: 0.9091\n",
      "Epoch 1005/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.8939 - val_loss: 0.2276 - val_accuracy: 0.9286\n",
      "Epoch 1006/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.8897 - val_loss: 0.2292 - val_accuracy: 0.9123\n",
      "Epoch 1007/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2508 - accuracy: 0.8939 - val_loss: 0.2271 - val_accuracy: 0.9286\n",
      "Epoch 1008/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2517 - accuracy: 0.8953 - val_loss: 0.2293 - val_accuracy: 0.9156\n",
      "Epoch 1009/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2508 - accuracy: 0.8994 - val_loss: 0.2273 - val_accuracy: 0.9286\n",
      "Epoch 1010/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2508 - accuracy: 0.8953 - val_loss: 0.2276 - val_accuracy: 0.9156\n",
      "Epoch 1011/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.8939 - val_loss: 0.2268 - val_accuracy: 0.9253\n",
      "Epoch 1012/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.8953 - val_loss: 0.2283 - val_accuracy: 0.9091\n",
      "Epoch 1013/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2501 - accuracy: 0.8980 - val_loss: 0.2266 - val_accuracy: 0.9286\n",
      "Epoch 1014/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2496 - accuracy: 0.8966 - val_loss: 0.2276 - val_accuracy: 0.9156\n",
      "Epoch 1015/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2494 - accuracy: 0.8980 - val_loss: 0.2286 - val_accuracy: 0.9188\n",
      "Epoch 1016/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2496 - accuracy: 0.8980 - val_loss: 0.2272 - val_accuracy: 0.9221\n",
      "Epoch 1017/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2498 - accuracy: 0.8980 - val_loss: 0.2267 - val_accuracy: 0.9286\n",
      "Epoch 1018/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2486 - accuracy: 0.8980 - val_loss: 0.2282 - val_accuracy: 0.9253\n",
      "Epoch 1019/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2490 - accuracy: 0.8953 - val_loss: 0.2289 - val_accuracy: 0.9188\n",
      "Epoch 1020/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2495 - accuracy: 0.8994 - val_loss: 0.2291 - val_accuracy: 0.9253\n",
      "Epoch 1021/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2483 - accuracy: 0.8994 - val_loss: 0.2268 - val_accuracy: 0.9221\n",
      "Epoch 1022/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.8994 - val_loss: 0.2263 - val_accuracy: 0.9221\n",
      "Epoch 1023/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2486 - accuracy: 0.8994 - val_loss: 0.2266 - val_accuracy: 0.9188\n",
      "Epoch 1024/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2481 - accuracy: 0.8980 - val_loss: 0.2262 - val_accuracy: 0.9156\n",
      "Epoch 1025/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.9008 - val_loss: 0.2269 - val_accuracy: 0.9253\n",
      "Epoch 1026/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.9036 - val_loss: 0.2273 - val_accuracy: 0.9253\n",
      "Epoch 1027/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.9022 - val_loss: 0.2254 - val_accuracy: 0.9286\n",
      "Epoch 1028/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.9022 - val_loss: 0.2279 - val_accuracy: 0.9253\n",
      "Epoch 1029/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.9036 - val_loss: 0.2268 - val_accuracy: 0.9253\n",
      "Epoch 1030/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.9036 - val_loss: 0.2261 - val_accuracy: 0.9156\n",
      "Epoch 1031/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.8980 - val_loss: 0.2257 - val_accuracy: 0.9221\n",
      "Epoch 1032/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.9022 - val_loss: 0.2283 - val_accuracy: 0.9253\n",
      "Epoch 1033/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2474 - accuracy: 0.9022 - val_loss: 0.2259 - val_accuracy: 0.9188\n",
      "Epoch 1034/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.8953 - val_loss: 0.2255 - val_accuracy: 0.9156\n",
      "Epoch 1035/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2465 - accuracy: 0.8980 - val_loss: 0.2262 - val_accuracy: 0.9221\n",
      "Epoch 1036/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.8980 - val_loss: 0.2255 - val_accuracy: 0.9221\n",
      "Epoch 1037/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.8994 - val_loss: 0.2257 - val_accuracy: 0.9221\n",
      "Epoch 1038/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2473 - accuracy: 0.8994 - val_loss: 0.2254 - val_accuracy: 0.9221\n",
      "Epoch 1039/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.8953 - val_loss: 0.2250 - val_accuracy: 0.9221\n",
      "Epoch 1040/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.9008 - val_loss: 0.2246 - val_accuracy: 0.9221\n",
      "Epoch 1041/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2461 - accuracy: 0.8994 - val_loss: 0.2256 - val_accuracy: 0.9286\n",
      "Epoch 1042/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2458 - accuracy: 0.9022 - val_loss: 0.2253 - val_accuracy: 0.9318\n",
      "Epoch 1043/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2459 - accuracy: 0.8966 - val_loss: 0.2243 - val_accuracy: 0.9221\n",
      "Epoch 1044/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2454 - accuracy: 0.9022 - val_loss: 0.2253 - val_accuracy: 0.9253\n",
      "Epoch 1045/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9036 - val_loss: 0.2258 - val_accuracy: 0.9253\n",
      "Epoch 1046/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9036 - val_loss: 0.2257 - val_accuracy: 0.9253\n",
      "Epoch 1047/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2454 - accuracy: 0.9036 - val_loss: 0.2245 - val_accuracy: 0.9253\n",
      "Epoch 1048/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2458 - accuracy: 0.8980 - val_loss: 0.2246 - val_accuracy: 0.9253\n",
      "Epoch 1049/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2445 - accuracy: 0.9036 - val_loss: 0.2253 - val_accuracy: 0.9253\n",
      "Epoch 1050/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2449 - accuracy: 0.9036 - val_loss: 0.2248 - val_accuracy: 0.9253\n",
      "Epoch 1051/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2443 - accuracy: 0.9022 - val_loss: 0.2235 - val_accuracy: 0.9188\n",
      "Epoch 1052/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2442 - accuracy: 0.9022 - val_loss: 0.2242 - val_accuracy: 0.9253\n",
      "Epoch 1053/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.9064 - val_loss: 0.2247 - val_accuracy: 0.9253\n",
      "Epoch 1054/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2441 - accuracy: 0.9008 - val_loss: 0.2234 - val_accuracy: 0.9253\n",
      "Epoch 1055/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.9022 - val_loss: 0.2245 - val_accuracy: 0.9253\n",
      "Epoch 1056/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2447 - accuracy: 0.9022 - val_loss: 0.2247 - val_accuracy: 0.9253\n",
      "Epoch 1057/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.9022 - val_loss: 0.2223 - val_accuracy: 0.9188\n",
      "Epoch 1058/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2436 - accuracy: 0.8953 - val_loss: 0.2233 - val_accuracy: 0.9318\n",
      "Epoch 1059/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2443 - accuracy: 0.9022 - val_loss: 0.2254 - val_accuracy: 0.9188\n",
      "Epoch 1060/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2436 - accuracy: 0.8980 - val_loss: 0.2227 - val_accuracy: 0.9253\n",
      "Epoch 1061/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2430 - accuracy: 0.9036 - val_loss: 0.2232 - val_accuracy: 0.9318\n",
      "Epoch 1062/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.8980 - val_loss: 0.2227 - val_accuracy: 0.9318\n",
      "Epoch 1063/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2430 - accuracy: 0.9022 - val_loss: 0.2239 - val_accuracy: 0.9253\n",
      "Epoch 1064/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2428 - accuracy: 0.9036 - val_loss: 0.2228 - val_accuracy: 0.9318\n",
      "Epoch 1065/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.9022 - val_loss: 0.2239 - val_accuracy: 0.9188\n",
      "Epoch 1066/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.8953 - val_loss: 0.2221 - val_accuracy: 0.9286\n",
      "Epoch 1067/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2443 - accuracy: 0.8980 - val_loss: 0.2247 - val_accuracy: 0.9188\n",
      "Epoch 1068/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.8994 - val_loss: 0.2213 - val_accuracy: 0.9221\n",
      "Epoch 1069/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2419 - accuracy: 0.8939 - val_loss: 0.2215 - val_accuracy: 0.9253\n",
      "Epoch 1070/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2421 - accuracy: 0.9036 - val_loss: 0.2231 - val_accuracy: 0.9253\n",
      "Epoch 1071/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.9050 - val_loss: 0.2219 - val_accuracy: 0.9318\n",
      "Epoch 1072/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2417 - accuracy: 0.8966 - val_loss: 0.2215 - val_accuracy: 0.9123\n",
      "Epoch 1073/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2415 - accuracy: 0.8966 - val_loss: 0.2221 - val_accuracy: 0.9188\n",
      "Epoch 1074/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2421 - accuracy: 0.9064 - val_loss: 0.2238 - val_accuracy: 0.9188\n",
      "Epoch 1075/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.8953 - val_loss: 0.2206 - val_accuracy: 0.9188\n",
      "Epoch 1076/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.8939 - val_loss: 0.2209 - val_accuracy: 0.9188\n",
      "Epoch 1077/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2409 - accuracy: 0.9022 - val_loss: 0.2232 - val_accuracy: 0.9188\n",
      "Epoch 1078/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2411 - accuracy: 0.9050 - val_loss: 0.2219 - val_accuracy: 0.9188\n",
      "Epoch 1079/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.8939 - val_loss: 0.2201 - val_accuracy: 0.9188\n",
      "Epoch 1080/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.8939 - val_loss: 0.2204 - val_accuracy: 0.9286\n",
      "Epoch 1081/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.9036 - val_loss: 0.2219 - val_accuracy: 0.9188\n",
      "Epoch 1082/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.8980 - val_loss: 0.2201 - val_accuracy: 0.9253\n",
      "Epoch 1083/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.9036 - val_loss: 0.2223 - val_accuracy: 0.9188\n",
      "Epoch 1084/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.8939 - val_loss: 0.2197 - val_accuracy: 0.9188\n",
      "Epoch 1085/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.8966 - val_loss: 0.2202 - val_accuracy: 0.9253\n",
      "Epoch 1086/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.9022 - val_loss: 0.2231 - val_accuracy: 0.9156\n",
      "Epoch 1087/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.9064 - val_loss: 0.2209 - val_accuracy: 0.9253\n",
      "Epoch 1088/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.8966 - val_loss: 0.2195 - val_accuracy: 0.9123\n",
      "Epoch 1089/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2391 - accuracy: 0.8953 - val_loss: 0.2203 - val_accuracy: 0.9253\n",
      "Epoch 1090/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2392 - accuracy: 0.9036 - val_loss: 0.2208 - val_accuracy: 0.9253\n",
      "Epoch 1091/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.8966 - val_loss: 0.2197 - val_accuracy: 0.9253\n",
      "Epoch 1092/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2397 - accuracy: 0.9036 - val_loss: 0.2215 - val_accuracy: 0.9188\n",
      "Epoch 1093/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2400 - accuracy: 0.8980 - val_loss: 0.2187 - val_accuracy: 0.9156\n",
      "Epoch 1094/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9036 - val_loss: 0.2209 - val_accuracy: 0.9188\n",
      "Epoch 1095/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.8994 - val_loss: 0.2190 - val_accuracy: 0.9221\n",
      "Epoch 1096/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9022 - val_loss: 0.2219 - val_accuracy: 0.9156\n",
      "Epoch 1097/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.8911 - val_loss: 0.2185 - val_accuracy: 0.9221\n",
      "Epoch 1098/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2383 - accuracy: 0.9078 - val_loss: 0.2206 - val_accuracy: 0.9188\n",
      "Epoch 1099/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2378 - accuracy: 0.8953 - val_loss: 0.2184 - val_accuracy: 0.9156\n",
      "Epoch 1100/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9008 - val_loss: 0.2208 - val_accuracy: 0.9188\n",
      "Epoch 1101/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2382 - accuracy: 0.8994 - val_loss: 0.2192 - val_accuracy: 0.9221\n",
      "Epoch 1102/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9008 - val_loss: 0.2202 - val_accuracy: 0.9253\n",
      "Epoch 1103/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2377 - accuracy: 0.9008 - val_loss: 0.2187 - val_accuracy: 0.9156\n",
      "Epoch 1104/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9036 - val_loss: 0.2191 - val_accuracy: 0.9253\n",
      "Epoch 1105/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2373 - accuracy: 0.8980 - val_loss: 0.2185 - val_accuracy: 0.9253\n",
      "Epoch 1106/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.8994 - val_loss: 0.2175 - val_accuracy: 0.9188\n",
      "Epoch 1107/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.8994 - val_loss: 0.2180 - val_accuracy: 0.9221\n",
      "Epoch 1108/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2369 - accuracy: 0.9022 - val_loss: 0.2180 - val_accuracy: 0.9221\n",
      "Epoch 1109/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2367 - accuracy: 0.8994 - val_loss: 0.2179 - val_accuracy: 0.9253\n",
      "Epoch 1110/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2383 - accuracy: 0.9036 - val_loss: 0.2181 - val_accuracy: 0.9253\n",
      "Epoch 1111/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.8980 - val_loss: 0.2175 - val_accuracy: 0.9156\n",
      "Epoch 1112/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2363 - accuracy: 0.9022 - val_loss: 0.2188 - val_accuracy: 0.9253\n",
      "Epoch 1113/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.9008 - val_loss: 0.2172 - val_accuracy: 0.9156\n",
      "Epoch 1114/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.9022 - val_loss: 0.2187 - val_accuracy: 0.9188\n",
      "Epoch 1115/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2357 - accuracy: 0.8980 - val_loss: 0.2169 - val_accuracy: 0.9221\n",
      "Epoch 1116/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2357 - accuracy: 0.9022 - val_loss: 0.2175 - val_accuracy: 0.9156\n",
      "Epoch 1117/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2355 - accuracy: 0.9036 - val_loss: 0.2191 - val_accuracy: 0.9156\n",
      "Epoch 1118/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.9064 - val_loss: 0.2173 - val_accuracy: 0.9156\n",
      "Epoch 1119/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.8966 - val_loss: 0.2176 - val_accuracy: 0.9156\n",
      "Epoch 1120/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.9022 - val_loss: 0.2177 - val_accuracy: 0.9253\n",
      "Epoch 1121/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2347 - accuracy: 0.9022 - val_loss: 0.2169 - val_accuracy: 0.9156\n",
      "Epoch 1122/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2363 - accuracy: 0.8897 - val_loss: 0.2163 - val_accuracy: 0.9156\n",
      "Epoch 1123/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9022 - val_loss: 0.2193 - val_accuracy: 0.9156\n",
      "Epoch 1124/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2351 - accuracy: 0.9008 - val_loss: 0.2175 - val_accuracy: 0.9221\n",
      "Epoch 1125/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.8994 - val_loss: 0.2165 - val_accuracy: 0.9156\n",
      "Epoch 1126/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2344 - accuracy: 0.8994 - val_loss: 0.2165 - val_accuracy: 0.9253\n",
      "Epoch 1127/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.8994 - val_loss: 0.2167 - val_accuracy: 0.9221\n",
      "Epoch 1128/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.9008 - val_loss: 0.2170 - val_accuracy: 0.9188\n",
      "Epoch 1129/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2344 - accuracy: 0.8994 - val_loss: 0.2154 - val_accuracy: 0.9221\n",
      "Epoch 1130/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.8994 - val_loss: 0.2168 - val_accuracy: 0.9221\n",
      "Epoch 1131/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2335 - accuracy: 0.9008 - val_loss: 0.2166 - val_accuracy: 0.9221\n",
      "Epoch 1132/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.9036 - val_loss: 0.2151 - val_accuracy: 0.9156\n",
      "Epoch 1133/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.8966 - val_loss: 0.2154 - val_accuracy: 0.9221\n",
      "Epoch 1134/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2332 - accuracy: 0.8980 - val_loss: 0.2157 - val_accuracy: 0.9221\n",
      "Epoch 1135/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2341 - accuracy: 0.8966 - val_loss: 0.2150 - val_accuracy: 0.9221\n",
      "Epoch 1136/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2337 - accuracy: 0.9008 - val_loss: 0.2195 - val_accuracy: 0.9156\n",
      "Epoch 1137/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2329 - accuracy: 0.8980 - val_loss: 0.2141 - val_accuracy: 0.9188\n",
      "Epoch 1138/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2326 - accuracy: 0.8966 - val_loss: 0.2162 - val_accuracy: 0.9221\n",
      "Epoch 1139/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2325 - accuracy: 0.9050 - val_loss: 0.2155 - val_accuracy: 0.9123\n",
      "Epoch 1140/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2339 - accuracy: 0.9036 - val_loss: 0.2146 - val_accuracy: 0.9156\n",
      "Epoch 1141/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2332 - accuracy: 0.8911 - val_loss: 0.2149 - val_accuracy: 0.9188\n",
      "Epoch 1142/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2322 - accuracy: 0.8966 - val_loss: 0.2151 - val_accuracy: 0.9188\n",
      "Epoch 1143/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2329 - accuracy: 0.8911 - val_loss: 0.2144 - val_accuracy: 0.9123\n",
      "Epoch 1144/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2327 - accuracy: 0.9022 - val_loss: 0.2149 - val_accuracy: 0.9221\n",
      "Epoch 1145/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9008 - val_loss: 0.2139 - val_accuracy: 0.9221\n",
      "Epoch 1146/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.8939 - val_loss: 0.2134 - val_accuracy: 0.9123\n",
      "Epoch 1147/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.8925 - val_loss: 0.2143 - val_accuracy: 0.9188\n",
      "Epoch 1148/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2313 - accuracy: 0.8953 - val_loss: 0.2153 - val_accuracy: 0.9221\n",
      "Epoch 1149/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.9008 - val_loss: 0.2159 - val_accuracy: 0.9221\n",
      "Epoch 1150/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.8939 - val_loss: 0.2134 - val_accuracy: 0.9123\n",
      "Epoch 1151/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2308 - accuracy: 0.8939 - val_loss: 0.2148 - val_accuracy: 0.9221\n",
      "Epoch 1152/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2320 - accuracy: 0.9008 - val_loss: 0.2156 - val_accuracy: 0.9221\n",
      "Epoch 1153/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2311 - accuracy: 0.8953 - val_loss: 0.2134 - val_accuracy: 0.9188\n",
      "Epoch 1154/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2310 - accuracy: 0.8966 - val_loss: 0.2139 - val_accuracy: 0.9188\n",
      "Epoch 1155/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.8994 - val_loss: 0.2129 - val_accuracy: 0.9221\n",
      "Epoch 1156/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.8966 - val_loss: 0.2129 - val_accuracy: 0.9156\n",
      "Epoch 1157/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.8994 - val_loss: 0.2130 - val_accuracy: 0.9188\n",
      "Epoch 1158/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.8966 - val_loss: 0.2132 - val_accuracy: 0.9188\n",
      "Epoch 1159/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9008 - val_loss: 0.2132 - val_accuracy: 0.9188\n",
      "Epoch 1160/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2297 - accuracy: 0.8994 - val_loss: 0.2123 - val_accuracy: 0.9188\n",
      "Epoch 1161/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2300 - accuracy: 0.8966 - val_loss: 0.2120 - val_accuracy: 0.9286\n",
      "Epoch 1162/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2292 - accuracy: 0.8966 - val_loss: 0.2143 - val_accuracy: 0.9221\n",
      "Epoch 1163/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.9022 - val_loss: 0.2130 - val_accuracy: 0.9188\n",
      "Epoch 1164/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2292 - accuracy: 0.8939 - val_loss: 0.2118 - val_accuracy: 0.9123\n",
      "Epoch 1165/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2292 - accuracy: 0.8966 - val_loss: 0.2127 - val_accuracy: 0.9188\n",
      "Epoch 1166/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.8966 - val_loss: 0.2127 - val_accuracy: 0.9188\n",
      "Epoch 1167/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2308 - accuracy: 0.8980 - val_loss: 0.2138 - val_accuracy: 0.9221\n",
      "Epoch 1168/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2292 - accuracy: 0.8953 - val_loss: 0.2108 - val_accuracy: 0.9123\n",
      "Epoch 1169/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9022 - val_loss: 0.2128 - val_accuracy: 0.9221\n",
      "Epoch 1170/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2298 - accuracy: 0.8980 - val_loss: 0.2118 - val_accuracy: 0.9188\n",
      "Epoch 1171/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.9022 - val_loss: 0.2129 - val_accuracy: 0.9221\n",
      "Epoch 1172/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9008 - val_loss: 0.2132 - val_accuracy: 0.9221\n",
      "Epoch 1173/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.8953 - val_loss: 0.2110 - val_accuracy: 0.9188\n",
      "Epoch 1174/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9008 - val_loss: 0.2125 - val_accuracy: 0.9221\n",
      "Epoch 1175/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2277 - accuracy: 0.9008 - val_loss: 0.2121 - val_accuracy: 0.9221\n",
      "Epoch 1176/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2276 - accuracy: 0.9008 - val_loss: 0.2115 - val_accuracy: 0.9221\n",
      "Epoch 1177/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2280 - accuracy: 0.8980 - val_loss: 0.2105 - val_accuracy: 0.9188\n",
      "Epoch 1178/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.8980 - val_loss: 0.2119 - val_accuracy: 0.9221\n",
      "Epoch 1179/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2280 - accuracy: 0.9022 - val_loss: 0.2108 - val_accuracy: 0.9188\n",
      "Epoch 1180/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2280 - accuracy: 0.8911 - val_loss: 0.2115 - val_accuracy: 0.9188\n",
      "Epoch 1181/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2270 - accuracy: 0.8980 - val_loss: 0.2115 - val_accuracy: 0.9188\n",
      "Epoch 1182/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2269 - accuracy: 0.8966 - val_loss: 0.2118 - val_accuracy: 0.9221\n",
      "Epoch 1183/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.8939 - val_loss: 0.2103 - val_accuracy: 0.9188\n",
      "Epoch 1184/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.8953 - val_loss: 0.2117 - val_accuracy: 0.9188\n",
      "Epoch 1185/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2269 - accuracy: 0.9050 - val_loss: 0.2118 - val_accuracy: 0.9188\n",
      "Epoch 1186/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2270 - accuracy: 0.8925 - val_loss: 0.2118 - val_accuracy: 0.9123\n",
      "Epoch 1187/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.8939 - val_loss: 0.2105 - val_accuracy: 0.9123\n",
      "Epoch 1188/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2260 - accuracy: 0.8939 - val_loss: 0.2109 - val_accuracy: 0.9123\n",
      "Epoch 1189/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2261 - accuracy: 0.8953 - val_loss: 0.2112 - val_accuracy: 0.9188\n",
      "Epoch 1190/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2267 - accuracy: 0.8994 - val_loss: 0.2119 - val_accuracy: 0.9091\n",
      "Epoch 1191/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.8939 - val_loss: 0.2100 - val_accuracy: 0.9123\n",
      "Epoch 1192/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2258 - accuracy: 0.8994 - val_loss: 0.2109 - val_accuracy: 0.9188\n",
      "Epoch 1193/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2260 - accuracy: 0.9036 - val_loss: 0.2103 - val_accuracy: 0.9188\n",
      "Epoch 1194/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.8939 - val_loss: 0.2098 - val_accuracy: 0.9188\n",
      "Epoch 1195/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2264 - accuracy: 0.8966 - val_loss: 0.2113 - val_accuracy: 0.9123\n",
      "Epoch 1196/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.8966 - val_loss: 0.2084 - val_accuracy: 0.9123\n",
      "Epoch 1197/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2256 - accuracy: 0.8980 - val_loss: 0.2104 - val_accuracy: 0.9188\n",
      "Epoch 1198/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2248 - accuracy: 0.8966 - val_loss: 0.2099 - val_accuracy: 0.9188\n",
      "Epoch 1199/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.8966 - val_loss: 0.2089 - val_accuracy: 0.9188\n",
      "Epoch 1200/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.8966 - val_loss: 0.2096 - val_accuracy: 0.9188\n",
      "Epoch 1201/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.8994 - val_loss: 0.2124 - val_accuracy: 0.9123\n",
      "Epoch 1202/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2231 - accuracy: 0.9008 - val_loss: 0.2079 - val_accuracy: 0.9188\n",
      "Epoch 1203/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.8925 - val_loss: 0.2076 - val_accuracy: 0.9123\n",
      "Epoch 1204/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.9036 - val_loss: 0.2110 - val_accuracy: 0.9123\n",
      "Epoch 1205/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.8980 - val_loss: 0.2078 - val_accuracy: 0.9253\n",
      "Epoch 1206/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2236 - accuracy: 0.8966 - val_loss: 0.2087 - val_accuracy: 0.9188\n",
      "Epoch 1207/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2239 - accuracy: 0.8966 - val_loss: 0.2106 - val_accuracy: 0.9123\n",
      "Epoch 1208/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2241 - accuracy: 0.9022 - val_loss: 0.2080 - val_accuracy: 0.9253\n",
      "Epoch 1209/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.8994 - val_loss: 0.2084 - val_accuracy: 0.9188\n",
      "Epoch 1210/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.8939 - val_loss: 0.2076 - val_accuracy: 0.9253\n",
      "Epoch 1211/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2250 - accuracy: 0.9036 - val_loss: 0.2084 - val_accuracy: 0.9188\n",
      "Epoch 1212/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2232 - accuracy: 0.8953 - val_loss: 0.2083 - val_accuracy: 0.9188\n",
      "Epoch 1213/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.8939 - val_loss: 0.2083 - val_accuracy: 0.9123\n",
      "Epoch 1214/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2229 - accuracy: 0.8953 - val_loss: 0.2091 - val_accuracy: 0.9091\n",
      "Epoch 1215/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2229 - accuracy: 0.8966 - val_loss: 0.2075 - val_accuracy: 0.9188\n",
      "Epoch 1216/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2225 - accuracy: 0.8966 - val_loss: 0.2087 - val_accuracy: 0.9091\n",
      "Epoch 1217/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.8953 - val_loss: 0.2088 - val_accuracy: 0.9091\n",
      "Epoch 1218/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2231 - accuracy: 0.8980 - val_loss: 0.2079 - val_accuracy: 0.9188\n",
      "Epoch 1219/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2224 - accuracy: 0.8953 - val_loss: 0.2090 - val_accuracy: 0.9091\n",
      "Epoch 1220/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.8966 - val_loss: 0.2080 - val_accuracy: 0.9188\n",
      "Epoch 1221/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2220 - accuracy: 0.8980 - val_loss: 0.2075 - val_accuracy: 0.9188\n",
      "Epoch 1222/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2217 - accuracy: 0.8966 - val_loss: 0.2082 - val_accuracy: 0.9091\n",
      "Epoch 1223/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.8966 - val_loss: 0.2069 - val_accuracy: 0.9253\n",
      "Epoch 1224/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2217 - accuracy: 0.8994 - val_loss: 0.2081 - val_accuracy: 0.9091\n",
      "Epoch 1225/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2233 - accuracy: 0.9050 - val_loss: 0.2109 - val_accuracy: 0.9156\n",
      "Epoch 1226/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.9008 - val_loss: 0.2057 - val_accuracy: 0.9188\n",
      "Epoch 1227/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.8980 - val_loss: 0.2066 - val_accuracy: 0.9253\n",
      "Epoch 1228/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2217 - accuracy: 0.9036 - val_loss: 0.2059 - val_accuracy: 0.9253\n",
      "Epoch 1229/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.9022 - val_loss: 0.2082 - val_accuracy: 0.9123\n",
      "Epoch 1230/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2217 - accuracy: 0.9022 - val_loss: 0.2067 - val_accuracy: 0.9188\n",
      "Epoch 1231/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2202 - accuracy: 0.8980 - val_loss: 0.2061 - val_accuracy: 0.9188\n",
      "Epoch 1232/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2210 - accuracy: 0.9036 - val_loss: 0.2067 - val_accuracy: 0.9123\n",
      "Epoch 1233/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.8911 - val_loss: 0.2058 - val_accuracy: 0.9188\n",
      "Epoch 1234/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.8966 - val_loss: 0.2062 - val_accuracy: 0.9188\n",
      "Epoch 1235/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.8994 - val_loss: 0.2079 - val_accuracy: 0.9123\n",
      "Epoch 1236/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2200 - accuracy: 0.8980 - val_loss: 0.2049 - val_accuracy: 0.9188\n",
      "Epoch 1237/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2196 - accuracy: 0.9022 - val_loss: 0.2064 - val_accuracy: 0.9091\n",
      "Epoch 1238/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.9064 - val_loss: 0.2095 - val_accuracy: 0.9156\n",
      "Epoch 1239/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2194 - accuracy: 0.9050 - val_loss: 0.2045 - val_accuracy: 0.9188\n",
      "Epoch 1240/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2194 - accuracy: 0.9008 - val_loss: 0.2069 - val_accuracy: 0.9123\n",
      "Epoch 1241/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2192 - accuracy: 0.9008 - val_loss: 0.2053 - val_accuracy: 0.9253\n",
      "Epoch 1242/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2197 - accuracy: 0.9036 - val_loss: 0.2062 - val_accuracy: 0.9123\n",
      "Epoch 1243/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.8980 - val_loss: 0.2051 - val_accuracy: 0.9091\n",
      "Epoch 1244/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2190 - accuracy: 0.9022 - val_loss: 0.2059 - val_accuracy: 0.9091\n",
      "Epoch 1245/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2208 - accuracy: 0.8980 - val_loss: 0.2057 - val_accuracy: 0.9091\n",
      "Epoch 1246/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9050 - val_loss: 0.2065 - val_accuracy: 0.9123\n",
      "Epoch 1247/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9050 - val_loss: 0.2055 - val_accuracy: 0.9091\n",
      "Epoch 1248/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9008 - val_loss: 0.2037 - val_accuracy: 0.9188\n",
      "Epoch 1249/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9050 - val_loss: 0.2077 - val_accuracy: 0.9156\n",
      "Epoch 1250/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.9036 - val_loss: 0.2046 - val_accuracy: 0.9091\n",
      "Epoch 1251/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2179 - accuracy: 0.9064 - val_loss: 0.2045 - val_accuracy: 0.9091\n",
      "Epoch 1252/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.8994 - val_loss: 0.2050 - val_accuracy: 0.9091\n",
      "Epoch 1253/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.9036 - val_loss: 0.2037 - val_accuracy: 0.9253\n",
      "Epoch 1254/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9022 - val_loss: 0.2054 - val_accuracy: 0.9123\n",
      "Epoch 1255/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9022 - val_loss: 0.2044 - val_accuracy: 0.9091\n",
      "Epoch 1256/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9078 - val_loss: 0.2037 - val_accuracy: 0.9091\n",
      "Epoch 1257/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.9050 - val_loss: 0.2038 - val_accuracy: 0.9091\n",
      "Epoch 1258/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.8994 - val_loss: 0.2040 - val_accuracy: 0.9091\n",
      "Epoch 1259/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9022 - val_loss: 0.2034 - val_accuracy: 0.9091\n",
      "Epoch 1260/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9050 - val_loss: 0.2031 - val_accuracy: 0.9156\n",
      "Epoch 1261/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2166 - accuracy: 0.9008 - val_loss: 0.2045 - val_accuracy: 0.9091\n",
      "Epoch 1262/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2169 - accuracy: 0.9050 - val_loss: 0.2043 - val_accuracy: 0.9123\n",
      "Epoch 1263/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9050 - val_loss: 0.2031 - val_accuracy: 0.9091\n",
      "Epoch 1264/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2160 - accuracy: 0.9036 - val_loss: 0.2032 - val_accuracy: 0.9091\n",
      "Epoch 1265/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9064 - val_loss: 0.2050 - val_accuracy: 0.9156\n",
      "Epoch 1266/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9064 - val_loss: 0.2033 - val_accuracy: 0.9091\n",
      "Epoch 1267/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2160 - accuracy: 0.9036 - val_loss: 0.2020 - val_accuracy: 0.9188\n",
      "Epoch 1268/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2168 - accuracy: 0.9050 - val_loss: 0.2018 - val_accuracy: 0.9188\n",
      "Epoch 1269/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2168 - accuracy: 0.9064 - val_loss: 0.2038 - val_accuracy: 0.9091\n",
      "Epoch 1270/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9050 - val_loss: 0.2025 - val_accuracy: 0.9091\n",
      "Epoch 1271/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2151 - accuracy: 0.9036 - val_loss: 0.2024 - val_accuracy: 0.9091\n",
      "Epoch 1272/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9050 - val_loss: 0.2034 - val_accuracy: 0.9123\n",
      "Epoch 1273/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9092 - val_loss: 0.2021 - val_accuracy: 0.9091\n",
      "Epoch 1274/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9036 - val_loss: 0.2026 - val_accuracy: 0.9123\n",
      "Epoch 1275/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9008 - val_loss: 0.2029 - val_accuracy: 0.9123\n",
      "Epoch 1276/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9134 - val_loss: 0.2035 - val_accuracy: 0.9123\n",
      "Epoch 1277/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9050 - val_loss: 0.2008 - val_accuracy: 0.9123\n",
      "Epoch 1278/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2151 - accuracy: 0.9120 - val_loss: 0.2059 - val_accuracy: 0.9156\n",
      "Epoch 1279/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9064 - val_loss: 0.2006 - val_accuracy: 0.9188\n",
      "Epoch 1280/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2143 - accuracy: 0.9134 - val_loss: 0.2047 - val_accuracy: 0.9156\n",
      "Epoch 1281/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2140 - accuracy: 0.9120 - val_loss: 0.2015 - val_accuracy: 0.9091\n",
      "Epoch 1282/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9036 - val_loss: 0.1999 - val_accuracy: 0.9188\n",
      "Epoch 1283/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2148 - accuracy: 0.9092 - val_loss: 0.2019 - val_accuracy: 0.9123\n",
      "Epoch 1284/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9120 - val_loss: 0.2019 - val_accuracy: 0.9123\n",
      "Epoch 1285/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.9078 - val_loss: 0.2001 - val_accuracy: 0.9156\n",
      "Epoch 1286/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.9064 - val_loss: 0.2013 - val_accuracy: 0.9123\n",
      "Epoch 1287/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2134 - accuracy: 0.9064 - val_loss: 0.2015 - val_accuracy: 0.9123\n",
      "Epoch 1288/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9106 - val_loss: 0.2013 - val_accuracy: 0.9123\n",
      "Epoch 1289/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2129 - accuracy: 0.9078 - val_loss: 0.2009 - val_accuracy: 0.9123\n",
      "Epoch 1290/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2128 - accuracy: 0.9092 - val_loss: 0.1997 - val_accuracy: 0.9156\n",
      "Epoch 1291/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9092 - val_loss: 0.1997 - val_accuracy: 0.9156\n",
      "Epoch 1292/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9064 - val_loss: 0.1990 - val_accuracy: 0.9188\n",
      "Epoch 1293/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9148 - val_loss: 0.2021 - val_accuracy: 0.9156\n",
      "Epoch 1294/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9092 - val_loss: 0.1992 - val_accuracy: 0.9091\n",
      "Epoch 1295/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2128 - accuracy: 0.9106 - val_loss: 0.2005 - val_accuracy: 0.9123\n",
      "Epoch 1296/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2120 - accuracy: 0.9036 - val_loss: 0.1990 - val_accuracy: 0.9091\n",
      "Epoch 1297/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2127 - accuracy: 0.9134 - val_loss: 0.2006 - val_accuracy: 0.9156\n",
      "Epoch 1298/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9022 - val_loss: 0.2000 - val_accuracy: 0.9156\n",
      "Epoch 1299/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2130 - accuracy: 0.9092 - val_loss: 0.2027 - val_accuracy: 0.9156\n",
      "Epoch 1300/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2118 - accuracy: 0.9162 - val_loss: 0.2003 - val_accuracy: 0.9156\n",
      "Epoch 1301/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9078 - val_loss: 0.2003 - val_accuracy: 0.9156\n",
      "Epoch 1302/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9092 - val_loss: 0.2003 - val_accuracy: 0.9156\n",
      "Epoch 1303/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2112 - accuracy: 0.9148 - val_loss: 0.2013 - val_accuracy: 0.9156\n",
      "Epoch 1304/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.9078 - val_loss: 0.1997 - val_accuracy: 0.9156\n",
      "Epoch 1305/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2108 - accuracy: 0.9064 - val_loss: 0.1999 - val_accuracy: 0.9221\n",
      "Epoch 1306/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2105 - accuracy: 0.9148 - val_loss: 0.2005 - val_accuracy: 0.9156\n",
      "Epoch 1307/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9162 - val_loss: 0.2002 - val_accuracy: 0.9156\n",
      "Epoch 1308/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9148 - val_loss: 0.2002 - val_accuracy: 0.9156\n",
      "Epoch 1309/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.9008 - val_loss: 0.1987 - val_accuracy: 0.9156\n",
      "Epoch 1310/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.9134 - val_loss: 0.2033 - val_accuracy: 0.9156\n",
      "Epoch 1311/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9092 - val_loss: 0.1977 - val_accuracy: 0.9091\n",
      "Epoch 1312/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9092 - val_loss: 0.1997 - val_accuracy: 0.9156\n",
      "Epoch 1313/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2097 - accuracy: 0.9134 - val_loss: 0.1993 - val_accuracy: 0.9156\n",
      "Epoch 1314/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2105 - accuracy: 0.9134 - val_loss: 0.1991 - val_accuracy: 0.9221\n",
      "Epoch 1315/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9148 - val_loss: 0.1995 - val_accuracy: 0.9156\n",
      "Epoch 1316/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2100 - accuracy: 0.9078 - val_loss: 0.1991 - val_accuracy: 0.9156\n",
      "Epoch 1317/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2108 - accuracy: 0.9078 - val_loss: 0.1995 - val_accuracy: 0.9156\n",
      "Epoch 1318/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2095 - accuracy: 0.9078 - val_loss: 0.1995 - val_accuracy: 0.9156\n",
      "Epoch 1319/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.9134 - val_loss: 0.1997 - val_accuracy: 0.9156\n",
      "Epoch 1320/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9162 - val_loss: 0.1994 - val_accuracy: 0.9156\n",
      "Epoch 1321/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9078 - val_loss: 0.1970 - val_accuracy: 0.9188\n",
      "Epoch 1322/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9162 - val_loss: 0.1997 - val_accuracy: 0.9156\n",
      "Epoch 1323/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9162 - val_loss: 0.1984 - val_accuracy: 0.9156\n",
      "Epoch 1324/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9162 - val_loss: 0.1981 - val_accuracy: 0.9156\n",
      "Epoch 1325/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9036 - val_loss: 0.1974 - val_accuracy: 0.9221\n",
      "Epoch 1326/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9162 - val_loss: 0.1983 - val_accuracy: 0.9156\n",
      "Epoch 1327/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9162 - val_loss: 0.1970 - val_accuracy: 0.9221\n",
      "Epoch 1328/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9134 - val_loss: 0.1968 - val_accuracy: 0.9221\n",
      "Epoch 1329/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9050 - val_loss: 0.1970 - val_accuracy: 0.9221\n",
      "Epoch 1330/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9134 - val_loss: 0.1987 - val_accuracy: 0.9156\n",
      "Epoch 1331/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.9162 - val_loss: 0.1977 - val_accuracy: 0.9156\n",
      "Epoch 1332/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9134 - val_loss: 0.1965 - val_accuracy: 0.9221\n",
      "Epoch 1333/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9148 - val_loss: 0.1977 - val_accuracy: 0.9156\n",
      "Epoch 1334/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9162 - val_loss: 0.1982 - val_accuracy: 0.9156\n",
      "Epoch 1335/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9148 - val_loss: 0.1962 - val_accuracy: 0.9221\n",
      "Epoch 1336/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9050 - val_loss: 0.1961 - val_accuracy: 0.9221\n",
      "Epoch 1337/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9176 - val_loss: 0.1967 - val_accuracy: 0.9156\n",
      "Epoch 1338/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9106 - val_loss: 0.1963 - val_accuracy: 0.9156\n",
      "Epoch 1339/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2063 - accuracy: 0.9176 - val_loss: 0.1974 - val_accuracy: 0.9156\n",
      "Epoch 1340/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9134 - val_loss: 0.1954 - val_accuracy: 0.9221\n",
      "Epoch 1341/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2064 - accuracy: 0.9162 - val_loss: 0.1993 - val_accuracy: 0.9156\n",
      "Epoch 1342/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2060 - accuracy: 0.9134 - val_loss: 0.1952 - val_accuracy: 0.9221\n",
      "Epoch 1343/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9134 - val_loss: 0.1962 - val_accuracy: 0.9221\n",
      "Epoch 1344/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9134 - val_loss: 0.1961 - val_accuracy: 0.9156\n",
      "Epoch 1345/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9162 - val_loss: 0.1965 - val_accuracy: 0.9156\n",
      "Epoch 1346/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2063 - accuracy: 0.9162 - val_loss: 0.1967 - val_accuracy: 0.9156\n",
      "Epoch 1347/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9162 - val_loss: 0.1935 - val_accuracy: 0.9123\n",
      "Epoch 1348/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2057 - accuracy: 0.9120 - val_loss: 0.1952 - val_accuracy: 0.9156\n",
      "Epoch 1349/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2057 - accuracy: 0.9148 - val_loss: 0.1949 - val_accuracy: 0.9221\n",
      "Epoch 1350/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9176 - val_loss: 0.1957 - val_accuracy: 0.9221\n",
      "Epoch 1351/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9162 - val_loss: 0.1954 - val_accuracy: 0.9221\n",
      "Epoch 1352/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2048 - accuracy: 0.9092 - val_loss: 0.1947 - val_accuracy: 0.9221\n",
      "Epoch 1353/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9134 - val_loss: 0.1970 - val_accuracy: 0.9156\n",
      "Epoch 1354/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9162 - val_loss: 0.1946 - val_accuracy: 0.9221\n",
      "Epoch 1355/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9120 - val_loss: 0.1943 - val_accuracy: 0.9221\n",
      "Epoch 1356/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2041 - accuracy: 0.9190 - val_loss: 0.1949 - val_accuracy: 0.9221\n",
      "Epoch 1357/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9148 - val_loss: 0.1944 - val_accuracy: 0.9221\n",
      "Epoch 1358/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9162 - val_loss: 0.1974 - val_accuracy: 0.9156\n",
      "Epoch 1359/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9162 - val_loss: 0.1930 - val_accuracy: 0.9123\n",
      "Epoch 1360/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9064 - val_loss: 0.1938 - val_accuracy: 0.9221\n",
      "Epoch 1361/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9162 - val_loss: 0.1947 - val_accuracy: 0.9221\n",
      "Epoch 1362/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9120 - val_loss: 0.1938 - val_accuracy: 0.9221\n",
      "Epoch 1363/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.9162 - val_loss: 0.1929 - val_accuracy: 0.9156\n",
      "Epoch 1364/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2040 - accuracy: 0.9162 - val_loss: 0.1937 - val_accuracy: 0.9221\n",
      "Epoch 1365/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9176 - val_loss: 0.1940 - val_accuracy: 0.9221\n",
      "Epoch 1366/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2030 - accuracy: 0.9148 - val_loss: 0.1929 - val_accuracy: 0.9156\n",
      "Epoch 1367/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9036 - val_loss: 0.1933 - val_accuracy: 0.9221\n",
      "Epoch 1368/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9162 - val_loss: 0.1947 - val_accuracy: 0.9156\n",
      "Epoch 1369/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9092 - val_loss: 0.1931 - val_accuracy: 0.9221\n",
      "Epoch 1370/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9162 - val_loss: 0.1954 - val_accuracy: 0.9156\n",
      "Epoch 1371/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2030 - accuracy: 0.9134 - val_loss: 0.1918 - val_accuracy: 0.9156\n",
      "Epoch 1372/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9120 - val_loss: 0.1940 - val_accuracy: 0.9221\n",
      "Epoch 1373/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9148 - val_loss: 0.1924 - val_accuracy: 0.9156\n",
      "Epoch 1374/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9092 - val_loss: 0.1929 - val_accuracy: 0.9221\n",
      "Epoch 1375/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9176 - val_loss: 0.1924 - val_accuracy: 0.9221\n",
      "Epoch 1376/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9176 - val_loss: 0.1937 - val_accuracy: 0.9221\n",
      "Epoch 1377/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9120 - val_loss: 0.1931 - val_accuracy: 0.9221\n",
      "Epoch 1378/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9176 - val_loss: 0.1961 - val_accuracy: 0.9156\n",
      "Epoch 1379/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9106 - val_loss: 0.1912 - val_accuracy: 0.9058\n",
      "Epoch 1380/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9148 - val_loss: 0.1957 - val_accuracy: 0.9156\n",
      "Epoch 1381/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9092 - val_loss: 0.1936 - val_accuracy: 0.9221\n",
      "Epoch 1382/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9190 - val_loss: 0.1927 - val_accuracy: 0.9221\n",
      "Epoch 1383/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9134 - val_loss: 0.1937 - val_accuracy: 0.9156\n",
      "Epoch 1384/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2011 - accuracy: 0.9162 - val_loss: 0.1928 - val_accuracy: 0.9221\n",
      "Epoch 1385/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9106 - val_loss: 0.1920 - val_accuracy: 0.9156\n",
      "Epoch 1386/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9106 - val_loss: 0.1927 - val_accuracy: 0.9221\n",
      "Epoch 1387/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9162 - val_loss: 0.1948 - val_accuracy: 0.9156\n",
      "Epoch 1388/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9176 - val_loss: 0.1918 - val_accuracy: 0.9156\n",
      "Epoch 1389/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9106 - val_loss: 0.1925 - val_accuracy: 0.9156\n",
      "Epoch 1390/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9176 - val_loss: 0.1930 - val_accuracy: 0.9221\n",
      "Epoch 1391/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9078 - val_loss: 0.1926 - val_accuracy: 0.9221\n",
      "Epoch 1392/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9190 - val_loss: 0.1950 - val_accuracy: 0.9156\n",
      "Epoch 1393/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2001 - accuracy: 0.9148 - val_loss: 0.1933 - val_accuracy: 0.9221\n",
      "Epoch 1394/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1999 - accuracy: 0.9120 - val_loss: 0.1922 - val_accuracy: 0.9221\n",
      "Epoch 1395/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9176 - val_loss: 0.1936 - val_accuracy: 0.9156\n",
      "Epoch 1396/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9162 - val_loss: 0.1928 - val_accuracy: 0.9221\n",
      "Epoch 1397/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9176 - val_loss: 0.1927 - val_accuracy: 0.9221\n",
      "Epoch 1398/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9162 - val_loss: 0.1935 - val_accuracy: 0.9156\n",
      "Epoch 1399/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1989 - accuracy: 0.9190 - val_loss: 0.1907 - val_accuracy: 0.9156\n",
      "Epoch 1400/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9078 - val_loss: 0.1940 - val_accuracy: 0.9156\n",
      "Epoch 1401/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9162 - val_loss: 0.1914 - val_accuracy: 0.9221\n",
      "Epoch 1402/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9148 - val_loss: 0.1917 - val_accuracy: 0.9221\n",
      "Epoch 1403/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9176 - val_loss: 0.1922 - val_accuracy: 0.9221\n",
      "Epoch 1404/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9120 - val_loss: 0.1918 - val_accuracy: 0.9221\n",
      "Epoch 1405/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9134 - val_loss: 0.1900 - val_accuracy: 0.9156\n",
      "Epoch 1406/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9120 - val_loss: 0.1910 - val_accuracy: 0.9221\n",
      "Epoch 1407/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9176 - val_loss: 0.1918 - val_accuracy: 0.9156\n",
      "Epoch 1408/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1989 - accuracy: 0.9120 - val_loss: 0.1902 - val_accuracy: 0.9156\n",
      "Epoch 1409/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9176 - val_loss: 0.1936 - val_accuracy: 0.9156\n",
      "Epoch 1410/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9162 - val_loss: 0.1918 - val_accuracy: 0.9221\n",
      "Epoch 1411/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9162 - val_loss: 0.1914 - val_accuracy: 0.9156\n",
      "Epoch 1412/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9162 - val_loss: 0.1900 - val_accuracy: 0.9221\n",
      "Epoch 1413/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9190 - val_loss: 0.1915 - val_accuracy: 0.9221\n",
      "Epoch 1414/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9134 - val_loss: 0.1910 - val_accuracy: 0.9221\n",
      "Epoch 1415/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9162 - val_loss: 0.1905 - val_accuracy: 0.9221\n",
      "Epoch 1416/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9190 - val_loss: 0.1912 - val_accuracy: 0.9221\n",
      "Epoch 1417/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9162 - val_loss: 0.1908 - val_accuracy: 0.9221\n",
      "Epoch 1418/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9176 - val_loss: 0.1907 - val_accuracy: 0.9221\n",
      "Epoch 1419/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.9176 - val_loss: 0.1903 - val_accuracy: 0.9221\n",
      "Epoch 1420/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9204 - val_loss: 0.1888 - val_accuracy: 0.9188\n",
      "Epoch 1421/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9176 - val_loss: 0.1902 - val_accuracy: 0.9221\n",
      "Epoch 1422/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9148 - val_loss: 0.1881 - val_accuracy: 0.9156\n",
      "Epoch 1423/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9176 - val_loss: 0.1883 - val_accuracy: 0.9156\n",
      "Epoch 1424/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9190 - val_loss: 0.1896 - val_accuracy: 0.9221\n",
      "Epoch 1425/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9148 - val_loss: 0.1894 - val_accuracy: 0.9221\n",
      "Epoch 1426/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.9162 - val_loss: 0.1902 - val_accuracy: 0.9156\n",
      "Epoch 1427/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9162 - val_loss: 0.1889 - val_accuracy: 0.9221\n",
      "Epoch 1428/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9176 - val_loss: 0.1907 - val_accuracy: 0.9156\n",
      "Epoch 1429/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9162 - val_loss: 0.1890 - val_accuracy: 0.9221\n",
      "Epoch 1430/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.9190 - val_loss: 0.1892 - val_accuracy: 0.9221\n",
      "Epoch 1431/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9162 - val_loss: 0.1893 - val_accuracy: 0.9221\n",
      "Epoch 1432/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9176 - val_loss: 0.1882 - val_accuracy: 0.9253\n",
      "Epoch 1433/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1952 - accuracy: 0.9162 - val_loss: 0.1892 - val_accuracy: 0.9221\n",
      "Epoch 1434/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9190 - val_loss: 0.1901 - val_accuracy: 0.9221\n",
      "Epoch 1435/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1943 - accuracy: 0.9190 - val_loss: 0.1886 - val_accuracy: 0.9221\n",
      "Epoch 1436/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9162 - val_loss: 0.1879 - val_accuracy: 0.9156\n",
      "Epoch 1437/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1938 - accuracy: 0.9190 - val_loss: 0.1902 - val_accuracy: 0.9156\n",
      "Epoch 1438/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9162 - val_loss: 0.1884 - val_accuracy: 0.9221\n",
      "Epoch 1439/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9162 - val_loss: 0.1884 - val_accuracy: 0.9253\n",
      "Epoch 1440/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9162 - val_loss: 0.1897 - val_accuracy: 0.9156\n",
      "Epoch 1441/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.9218 - val_loss: 0.1875 - val_accuracy: 0.9188\n",
      "Epoch 1442/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9190 - val_loss: 0.1895 - val_accuracy: 0.9221\n",
      "Epoch 1443/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9148 - val_loss: 0.1888 - val_accuracy: 0.9253\n",
      "Epoch 1444/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9162 - val_loss: 0.1903 - val_accuracy: 0.9156\n",
      "Epoch 1445/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9162 - val_loss: 0.1872 - val_accuracy: 0.9188\n",
      "Epoch 1446/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9134 - val_loss: 0.1880 - val_accuracy: 0.9253\n",
      "Epoch 1447/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1934 - accuracy: 0.9190 - val_loss: 0.1875 - val_accuracy: 0.9253\n",
      "Epoch 1448/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9218 - val_loss: 0.1876 - val_accuracy: 0.9253\n",
      "Epoch 1449/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9204 - val_loss: 0.1869 - val_accuracy: 0.9188\n",
      "Epoch 1450/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9134 - val_loss: 0.1867 - val_accuracy: 0.9188\n",
      "Epoch 1451/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1934 - accuracy: 0.9120 - val_loss: 0.1872 - val_accuracy: 0.9188\n",
      "Epoch 1452/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9218 - val_loss: 0.1904 - val_accuracy: 0.9188\n",
      "Epoch 1453/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9120 - val_loss: 0.1895 - val_accuracy: 0.9156\n",
      "Epoch 1454/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9204 - val_loss: 0.1865 - val_accuracy: 0.9188\n",
      "Epoch 1455/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1921 - accuracy: 0.9204 - val_loss: 0.1865 - val_accuracy: 0.9188\n",
      "Epoch 1456/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1926 - accuracy: 0.9134 - val_loss: 0.1857 - val_accuracy: 0.9188\n",
      "Epoch 1457/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9218 - val_loss: 0.1890 - val_accuracy: 0.9253\n",
      "Epoch 1458/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9204 - val_loss: 0.1863 - val_accuracy: 0.9253\n",
      "Epoch 1459/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9204 - val_loss: 0.1854 - val_accuracy: 0.9188\n",
      "Epoch 1460/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9190 - val_loss: 0.1853 - val_accuracy: 0.9188\n",
      "Epoch 1461/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1914 - accuracy: 0.9162 - val_loss: 0.1859 - val_accuracy: 0.9188\n",
      "Epoch 1462/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9148 - val_loss: 0.1856 - val_accuracy: 0.9188\n",
      "Epoch 1463/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9176 - val_loss: 0.1853 - val_accuracy: 0.9188\n",
      "Epoch 1464/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9190 - val_loss: 0.1869 - val_accuracy: 0.9253\n",
      "Epoch 1465/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9204 - val_loss: 0.1861 - val_accuracy: 0.9253\n",
      "Epoch 1466/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9148 - val_loss: 0.1856 - val_accuracy: 0.9188\n",
      "Epoch 1467/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9190 - val_loss: 0.1870 - val_accuracy: 0.9253\n",
      "Epoch 1468/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9148 - val_loss: 0.1845 - val_accuracy: 0.9188\n",
      "Epoch 1469/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9204 - val_loss: 0.1878 - val_accuracy: 0.9253\n",
      "Epoch 1470/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1908 - accuracy: 0.9218 - val_loss: 0.1849 - val_accuracy: 0.9188\n",
      "Epoch 1471/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1909 - accuracy: 0.9190 - val_loss: 0.1865 - val_accuracy: 0.9253\n",
      "Epoch 1472/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9162 - val_loss: 0.1849 - val_accuracy: 0.9188\n",
      "Epoch 1473/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9190 - val_loss: 0.1856 - val_accuracy: 0.9253\n",
      "Epoch 1474/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.9204 - val_loss: 0.1857 - val_accuracy: 0.9253\n",
      "Epoch 1475/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9204 - val_loss: 0.1864 - val_accuracy: 0.9253\n",
      "Epoch 1476/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1907 - accuracy: 0.9204 - val_loss: 0.1848 - val_accuracy: 0.9188\n",
      "Epoch 1477/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9176 - val_loss: 0.1846 - val_accuracy: 0.9188\n",
      "Epoch 1478/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9134 - val_loss: 0.1842 - val_accuracy: 0.9188\n",
      "Epoch 1479/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9190 - val_loss: 0.1851 - val_accuracy: 0.9253\n",
      "Epoch 1480/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1893 - accuracy: 0.9204 - val_loss: 0.1855 - val_accuracy: 0.9253\n",
      "Epoch 1481/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9218 - val_loss: 0.1855 - val_accuracy: 0.9253\n",
      "Epoch 1482/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9148 - val_loss: 0.1862 - val_accuracy: 0.9253\n",
      "Epoch 1483/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9218 - val_loss: 0.1846 - val_accuracy: 0.9188\n",
      "Epoch 1484/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9176 - val_loss: 0.1857 - val_accuracy: 0.9253\n",
      "Epoch 1485/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9134 - val_loss: 0.1859 - val_accuracy: 0.9253\n",
      "Epoch 1486/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1891 - accuracy: 0.9190 - val_loss: 0.1841 - val_accuracy: 0.9188\n",
      "Epoch 1487/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9204 - val_loss: 0.1833 - val_accuracy: 0.9188\n",
      "Epoch 1488/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9176 - val_loss: 0.1846 - val_accuracy: 0.9253\n",
      "Epoch 1489/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9120 - val_loss: 0.1840 - val_accuracy: 0.9188\n",
      "Epoch 1490/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1883 - accuracy: 0.9176 - val_loss: 0.1838 - val_accuracy: 0.9188\n",
      "Epoch 1491/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1878 - accuracy: 0.9218 - val_loss: 0.1854 - val_accuracy: 0.9253\n",
      "Epoch 1492/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9218 - val_loss: 0.1841 - val_accuracy: 0.9253\n",
      "Epoch 1493/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9190 - val_loss: 0.1845 - val_accuracy: 0.9253\n",
      "Epoch 1494/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9204 - val_loss: 0.1845 - val_accuracy: 0.9253\n",
      "Epoch 1495/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9218 - val_loss: 0.1839 - val_accuracy: 0.9253\n",
      "Epoch 1496/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9218 - val_loss: 0.1826 - val_accuracy: 0.9188\n",
      "Epoch 1497/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9204 - val_loss: 0.1832 - val_accuracy: 0.9188\n",
      "Epoch 1498/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9134 - val_loss: 0.1839 - val_accuracy: 0.9253\n",
      "Epoch 1499/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9204 - val_loss: 0.1837 - val_accuracy: 0.9253\n",
      "Epoch 1500/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9204 - val_loss: 0.1832 - val_accuracy: 0.9188\n",
      "23/23 [==============================] - 0s 478us/step - loss: 0.1859 - accuracy: 0.9190\n",
      "Accuracy: 91.90\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzL0lEQVR4nO3deXyU5bn/8c81M9l3kgBCCAREkB2NoIgFpQputbZyjlbbqrWKv6qtPbZoe9ra2vPStqd1qVqO7bG2tQe17rVYLVoFFWUTlH0NkEDIvu8z1++PZ6AhG0nIZIY81/v1yiuZZ+555gpLvrnv+3nuW1QVY4wx7uUJdwHGGGPCy4LAGGNczoLAGGNczoLAGGNczoLAGGNczoLAGGNczoLAGGNczoLAmC6ISJ6IfDbcdRgTShYExhjjchYExvSQiMSIyEMicjD48ZCIxASfyxCR10SkQkTKRGSliHiCzy0WkQIRqRaR7SIyL7zfiTEOX7gLMOYk9H3gbGAaoMArwH8CPwD+A8gHMoNtzwZURMYBtwFnqepBERkFePu3bGM6Zj0CY3ruWuAnqlqkqsXAj4EvB59rBk4BRqpqs6quVGdBLz8QA0wQkShVzVPV3WGp3pg2LAiM6blhwL5Wj/cFjwH8AtgFvCkie0TkbgBV3QV8C7gXKBKRZ0RkGMZEAAsCY3ruIDCy1ePs4DFUtVpV/0NVRwOXA98+Mhegqv+nqrODr1XgZ/1btjEdsyAw5viiRCT2yAewFPhPEckUkQzgh8DTACJymYicKiICVOEMCflFZJyIXBCcVG4A6oPPGRN2FgTGHN8ynB/cRz5igbXAJ8CnwHrgp8G2Y4HlQA2wCnhcVd/BmR94ACgBCoHBwPf67TswpgtiG9MYY4y7WY/AGGNczoLAGGNczoLAGGNczoLAGGNc7qRbYiIjI0NHjRoV7jKMMeaksm7duhJVzezouZMuCEaNGsXatWvDXYYxxpxURGRfZ8/Z0JAxxricBYExxricBYExxrjcSTdHYIyJLM3NzeTn59PQ0BDuUgwQGxtLVlYWUVFR3X6NBYEx5oTk5+eTlJTEqFGjcNbaM+GiqpSWlpKfn09OTk63X2dDQ8aYE9LQ0EB6erqFQAQQEdLT03vcO7MgMMacMAuByNGbvwvXBMH2wmp++eZ2Smoaw12KMcZEFNcEwa6iGn799i5Ka5rCXYoxpg+VlpYybdo0pk2bxtChQxk+fPjRx01NXf9/X7t2LXfcccdx32PWrFl9Uus777zDZZdd1ifn6kuumSz2BiPPH7D9F4wZSNLT09mwYQMA9957L4mJidx1111Hn29pacHn6/hHXW5uLrm5ucd9jw8++KBPao1UIe0RiMgCEdkuIruObOLd5vk0EXlJRD4RkdUiMilUtXiC42YB24jHmAHv+uuv59vf/jbnn38+ixcvZvXq1cyaNYvp06cza9Ystm/fDhz7G/q9997LjTfeyNy5cxk9ejSPPPLI0fMlJiYebT937lyuuuoqxo8fz7XXXsuRzb2WLVvG+PHjmT17NnfccUePfvNfunQpkydPZtKkSSxevBgAv9/P9ddfz6RJk5g8eTIPPvggAI888ggTJkxgypQpXH311Sf+h0UIewQi4gUeAy4E8oE1IvKqqm5p1ex7wAZVvVJExgfbzwtFPV6PEwTWIzAmdH78181sOVjVp+ecMCyZH10+scev27FjB8uXL8fr9VJVVcWKFSvw+XwsX76c733ve7zwwgvtXrNt2zb++c9/Ul1dzbhx47j11lvbXY//8ccfs3nzZoYNG8a5557L+++/T25uLrfccgsrVqwgJyeHa665ptt1Hjx4kMWLF7Nu3TrS0tK46KKLePnllxkxYgQFBQVs2rQJgIqKCgAeeOAB9u7dS0xMzNFjJyqUPYIZwC5V3aOqTcAzwBVt2kwA3gJQ1W3AKBEZEopiPEeCwHoExrjCwoUL8Xq9AFRWVrJw4UImTZrEnXfeyebNmzt8zaWXXkpMTAwZGRkMHjyYw4cPt2szY8YMsrKy8Hg8TJs2jby8PLZt28bo0aOPXrvfkyBYs2YNc+fOJTMzE5/Px7XXXsuKFSsYPXo0e/bs4fbbb+fvf/87ycnJAEyZMoVrr72Wp59+utMhr54K5RzBcOBAq8f5wMw2bTYCXwDeE5EZwEggCzjmT19EbgZuBsjOzu5VMT7rERgTcr35zT1UEhISjn79gx/8gPPPP5+XXnqJvLw85s6d2+FrYmJijn7t9XppaWnpVpsT2fu9s9empaWxceNG3njjDR577DGee+45nnzySf72t7+xYsUKXn31Ve677z42b958woEQyh5BRxeztv2OHwDSRGQDcDvwMdDuT15Vn1DVXFXNzczscDnt4/KKBYExblVZWcnw4cMBeOqpp/r8/OPHj2fPnj3k5eUB8Oyzz3b7tTNnzuTdd9+lpKQEv9/P0qVLmTNnDiUlJQQCAb74xS9y3333sX79egKBAAcOHOD888/n5z//ORUVFdTU1Jxw/aHsEeQDI1o9zgIOtm6gqlXADQDi3AWxN/jR544MDQUsCIxxne9+97t89atf5Ve/+hUXXHBBn58/Li6Oxx9/nAULFpCRkcGMGTM6bfvWW2+RlZV19PFf/vIX7r//fs4//3xUlUsuuYQrrriCjRs3csMNNxAIBAC4//778fv9XHfddVRWVqKq3HnnnaSmpp5w/XIiXZouTyziA3bgTP4WAGuAL6nq5lZtUoE6VW0Ska8D56nqV7o6b25urvZmY5o1eWUsXLKKP31tBueN7V2vwhjT3tatWzn99NPDXUbY1dTUkJiYiKryjW98g7Fjx3LnnXeGpZaO/k5EZJ2qdnitbMiGhlS1BbgNeAPYCjynqptFZJGILAo2Ox3YLCLbgIuBb4aqHo8NDRljQui3v/0t06ZNY+LEiVRWVnLLLbeEu6RuC+kNZaq6DFjW5tiSVl+vAsaGsoYjjlw+avcRGGNC4c477wxbD+BEuWaJiX9NFoe5EGOMiTCuCQKPLTFhjDEdck0QRNcXMdfzMdJ84pdaGWPMQOKaIEgsXMNT0b8guqYg3KUYY0xEcc3qo+INfqsBf3gLMcb0qdLSUubNc5YoKywsxOv1cuTG09WrVxMdHd3l69955x2io6M7XGr6qaeeYu3atTz66KN9X3gEcV0QqL/9LePGmJPX8ZahPp533nmHxMTEPttz4GTkmqEhj8cJgoAFgTED3rp165gzZw5nnnkm8+fP59ChQ0D7JZzz8vJYsmQJDz74INOmTWPlypXdOv+vfvUrJk2axKRJk3jooYcAqK2t5dJLL2Xq1KlMmjTp6DITd99999H37ElA9SfX9Ag8R3oEAQsCY0Lm9buh8NO+PefQyXDxA91urqrcfvvtvPLKK2RmZvLss8/y/e9/nyeffLLdEs6pqaksWrSoR72IdevW8fvf/56PPvoIVWXmzJnMmTOHPXv2MGzYMP72t78BzvpGZWVlvPTSS2zbtg0R6bNlo/uae3oEPgsCY9ygsbGRTZs2ceGFFzJt2jR++tOfkp+fD/TNEs7vvfceV155JQkJCSQmJvKFL3yBlStXMnnyZJYvX87ixYtZuXIlKSkpJCcnExsby0033cSLL75IfHx8X36rfcY1PQLxBDeXsKEhY0KnB7+5h4qqMnHiRFatWtXuuY6WcO7N+Tty2mmnsW7dOpYtW8Y999zDRRddxA9/+ENWr17NW2+9xTPPPMOjjz7K22+/3eP3DDX39AiCG1RYj8CYgS0mJobi4uKjQdDc3MzmzZs7XcI5KSmJ6urqbp//M5/5DC+//DJ1dXXU1tby0ksvcd5553Hw4EHi4+O57rrruOuuu1i/fj01NTVUVlZyySWX8NBDDx2d1I40rukReLxOj0ADtsaEMQOZx+Ph+eef54477qCyspKWlha+9a1vcdppp3W4hPPll1/OVVddxSuvvMKvf/1rzjvvvGPO99RTT/Hyyy8fffzhhx9y/fXXH11q+qabbmL69Om88cYbfOc738Hj8RAVFcVvfvMbqqurueKKK2hoaEBVj+47HGlCtgx1qPR2GeravatJ+MOFvDHlYeZ/4fq+L8wYl7JlqCNPxCxDHWnsqiFjjOmYi4LgyNCQ3VlsjDGtuSgInB6BWI/AmD53sg0xD2S9+btwTRB4j/YILAiM6UuxsbGUlpZaGEQAVaW0tJTY2Ngevc5FVw05l49aj8CYvpWVlUV+fj7FxcXhLsXgBHNWVlaPXhPSIBCRBcDDgBf4nao+0Ob5FOBpIDtYy3+r6u9DUoznyGSxzREY05eioqLIyckJdxnmBIRsaEhEvMBjOJvSTwCuEZEJbZp9A9iiqlOBucAvRaTrNWN7KxgEqAWBMca0Fso5ghnALlXdo6pNwDPAFW3aKJAkIgIkAmVAaMZuPDY0ZIwxHQllEAwHDrR6nB881tqjwOnAQeBT4Juq2u7WXxG5WUTWisjaXo9DHh0asjuLjTGmtVAGgXRwrO1lBfOBDcAwYBrwqIgkt3uR6hOqmququUd2Huox6xEYY0yHQhkE+cCIVo+zcH7zb+0G4EV17AL2AuNDUs3ROQILAmOMaS2UQbAGGCsiOcEJ4KuBV9u02Q/MAxCRIcA4YE9IqpHg6qO2DLUxxhwjZJePqmqLiNwGvIFz+eiTqrpZRBYFn18C3Ac8JSKf4gwlLVbVkpAUZJePGmNMh0J6H4GqLgOWtTm2pNXXB4GLQlnDUcE5AtuYxhhjjuWaJSYQwY8HbLLYGGOO4Z4gAAJ4ULuhzBhjjuGqIPDjtR6BMca04aogCIgXscliY4w5hruCAI/dUGaMMW24KgiaPdF4Ak3hLsMYYyKKq4KgRWLwqgWBMca05qog8EsUUdYjMMaYY7gqCFo80fisR2CMMcdwWRDE4As0h7sMY4yJKK4KgoAniiisR2CMMa25Kgj8nhiibGjIGGOO4aogCHhjiLYgMMaYY7gqCPyeaHwh2hLZGGNOVq4KAvHFEEMTzX7bt9gYY45wVxBExRJDM7WN1iswxpgjXBUEnmAQVDdYEBhjzBEhDQIRWSAi20Vkl4jc3cHz3xGRDcGPTSLiF5FBoarHFx1HDM3UWI/AGGOOClkQiIgXeAy4GJgAXCMiE1q3UdVfqOo0VZ0G3AO8q6ploarJE5tEjDRTW1cbqrcwxpiTTih7BDOAXaq6R1WbgGeAK7pofw2wNIT14ElMB6CxqiSUb2OMMSeVUAbBcOBAq8f5wWPtiEg8sAB4IYT1EJWYAUBTdWko38YYY04qoQwC6eCYdtL2cuD9zoaFRORmEVkrImuLi4t7XVB8aiYAjZVFvT6HMcYMNKEMgnxgRKvHWcDBTtpeTRfDQqr6hKrmqmpuZmZmrwtKHjQEgKryw70+hzHGDDShDII1wFgRyRGRaJwf9q+2bSQiKcAc4JUQ1uK8V9IwALR8f6jfyhhjThq+UJ1YVVtE5DbgDcALPKmqm0VkUfD5JcGmVwJvqmroL+VJSKcsaihpFZ/iDyheT0ejV8YY4y4hCwIAVV0GLGtzbEmbx08BT4Wyjtbqhp7FmfvfYcO+Es7M6f0wkzHGDBSuurMYIO2Mz5Mu1WxdvTzcpRhjTERwXRAkTJhPM1HE7n493KUYY0xEcF0QEJNEYcZMZjR8wP4Su8PYGGPcFwRA1Lj5ZHuK2b5za7hLMcaYsHNlEKSPmwVAze6PwlyJMcaEnyuDIOqUSQQQKLIegTHGuDIIiIqlPGooiTV7w12JMcaEnTuDAKhOGMnQlnyaWmzbSmOMu7k2CFrSTiVHCikorwt3KcYYE1auDYLozNEkSgMHDxWEuxRjjAkr1wZBUmY2AOWF+8JciTHGhJdrgyB5sBMETWUHjtPSGGMGNtcGgTc1uFla9aHwFmKMMWHm2iAgcQgBBG9NYbgrMcaYsHJvEHijqPKmEddgQWCMcTf3BgFQFzWI2KbycJdhjDFh5eogaI5JI9FfiT+g4S7FGGPCxtVB4I8dRAo1VNU3h7sUY4wJm5AGgYgsEJHtIrJLRO7upM1cEdkgIptF5N1Q1tOWxg9ikFRTYUFgjHGxkO1ZLCJe4DHgQiAfWCMir6rqllZtUoHHgQWqul9EBoeqno544jNIoZa8mjrISOjPtzbGmIgRyh7BDGCXqu5R1SbgGeCKNm2+BLyoqvsBVLUohPW040vKwCNKbWVJf76tMcZElFAGwXCg9W27+cFjrZ0GpInIOyKyTkS+EsJ62olOzgSgobK4P9/WGGMiSsiGhgDp4Fjby3N8wJnAPCAOWCUiH6rqjmNOJHIzcDNAdnZ2nxUYl+KMRDVVWRAYY9wrlD2CfGBEq8dZwMEO2vxdVWtVtQRYAUxteyJVfUJVc1U1NzMzs88KjE9JB6C5pqzPzmmMMSebbgeBiPR0NnUNMFZEckQkGrgaeLVNm1eA80TEJyLxwEyg3/aP9MWnAhCor+yvtzTGmIhz3CAQkVkisoXgD2gRmSoijx/vdaraAtwGvBF87XOqullEFonIomCbrcDfgU+A1cDvVHVTr7+bnopNdWqtt7uLjTHu1Z05ggeB+QR/m1fVjSLyme6cXFWXAcvaHFvS5vEvgF90q9q+FpMMgDRWheXtjTEmEnRraEhV2y7a7w9BLf3P66Ne4vE2WRAYY9yrOz2CAyIyC9DgWP8d9OM4fqjVe5OIbrYgMMa4V3d6BIuAb+DcA5APTAs+HhCafEnEtFSHuwxjjAmb4/YIgpd1XtsPtYRFS3QScXU1BAKKx9PRrQ/GGDOwHTcIROT3tL8RDFW9MSQV9TN/TAoplFLV0ExqfHS4yzHGmH7XnTmC11p9HQtcSfsbw05aGpNCktRRWW9BYIxxp+4MDb3Q+rGILAWWh6yifuaJSyWZWvJsKWpjjEv1ZomJsUDfLfgTZt6ENJKlnoqahnCXYowxYdGdOYJqnDkCCX4uBBaHuK5+E52QCkBtdRkwNKy1GGNMOHRnaCipPwoJl5ikQQA0VNnCc8YYd+o0CETkjK5eqKrr+76c/heX7KxA2lhtQWCMcaeuegS/7OI5BS7o41rCIio+DYDmOlt4zhjjTp0Ggaqe35+FhE1cKgBaVxHWMowxJly6tUOZiEwCJuDcRwCAqv4xVEX1q9gUALShIrx1GGNMmHTnqqEfAXNxgmAZcDHwHjCggkAabXMaY4w7dec+gqtw9hQuVNUbcLaSjAlpVf0pOhE/Hny2J4ExxqW6EwQNqhoAWkQkGSgCRoe2rH4kQoM3iShbgdQY41JdXT76KLAUWC0iqcBvgXVADc62kgNGky+J2PoqVBURW4HUGOMuXc0R7AT+GxiG88N/KXAhkKyqn3Tn5CKyAHgY8OLsR/xAm+fn4mxgvzd46EVV/UkP6u8TzdHJJNbV0tAcIC7a299vb4wxYdXV5aMPAw+LyEjgauD3OFcNLRWRelXd2dWJRcQLPIYTHvnAGhF5VVW3tGm6UlUvO5Fv4kT5Y1JIlWIq6puIi44LZynGGNPvjjtHoKr7VPVnqjod+BLOMtTbunHuGcAuVd2jqk3AM8AVJ1RtqMQ6K5BW1NkKpMYY9zluEIhIlIhcLiJ/Bl4HdgBf7Ma5hwOtN73PDx5r6xwR2Sgir4vIxE5quFlE1orI2uLi4m68dc9IXCopYkFgjHGnriaLLwSuAS7FmRx+BrhZVWu7ee6OZl3b7nS2HhipqjUicgnwMs4y18e+SPUJ4AmA3NzcdrulnShfwiBSqKWyrrGvT22MMRGvqx7B94BVwOmqermq/rkHIQBOD2BEq8dZtNnZTFWrVLUm+PUyIEpEMnrwHn0iKjGNKPFTU233Ehhj3CeUaw2tAcaKSA5QgDPh/KXWDURkKHBYVVVEZuAEU+kJvm+PxSY5K5DWVfX7WxtjTNh1a62h3lDVFhG5DXgD5/LRJ1V1s4gsCj6/BOeu5VtFpAWoB65W1T4f+jme6ERnT4LGagsCY4z7hCwI4Ohwz7I2x5a0+vpR4NFQ1tAdElyBtN6CwBjjQr3Zs3jgiXeGhvw1JWEuxBhj+p8FAUC8Mz/tqbMegTHGfSwI4GiPwNdg21UaY9zHggDAF02DL4mElnIaW/zhrsYYY/qVBUFQU/Qg0qWKkpqmcJdijDH9yoIgKBCfTjpVFFU1hLsUY4zpVxYEQZ6ETAZJNYctCIwxLmNBEBSbMph0qeJAWX24SzHGmH5lQRAUnTKENKnmQGlNuEsxxph+ZUFwREImPgKUlhwOdyXGGNOvLAiOSD4FgKayA8dpaIwxA4sFwREpWQB4qwsIBPp93TtjjAkbC4IjUpytEzK1hIIKmzA2xriHBcER8RkEvNEMl1K2F1aHuxpjjOk3FgRHeDyQPJxhUsK2QtupzBjjHhYErXhSssiJKmfLIQsCY4x7WBC0ljKCEZ4SVu8tIwwbpRljTFhYELSWPobUlhLqayrZftjmCYwx7hDSIBCRBSKyXUR2icjdXbQ7S0T8InJVKOs5rsxxAIyRg7y/yzapMca4Q8iCQES8wGPAxcAE4BoRmdBJu5/hbHIfXhlOEJydVMIHu2zbSmOMO4SyRzAD2KWqe1S1CXgGuKKDdrcDLwBFIaylewblgMfHuamlfLinlGZ/INwVGWNMyIUyCIYDrddryA8eO0pEhgNXAku6OpGI3Cwia0VkbXFxcZ8XepQ3ClJHMiGqkNomP8+vyw/dexljTIQIZRBIB8faXorzELBYVbvcH1JVn1DVXFXNzczM7Kv6OjZqNhlFq8gZFMPLHxeE9r2MMSYChDII8oERrR5nAQfbtMkFnhGRPOAq4HER+XwIazq+rLOQ5lpunujno71l/HNb+EesjDEmlEIZBGuAsSKSIyLRwNXAq60bqGqOqo5S1VHA88D/U9WXQ1jT8Y2YCcDC6I/ITIrhvte2UFnfHNaSjDEmlEIWBKraAtyGczXQVuA5Vd0sIotEZFGo3veEZZ4GmafjK1jNo9dMZ39ZHbcv/ZgWmzg2xgxQIb2PQFWXqeppqjpGVf8reGyJqrabHFbV61X1+VDW022jZkPeSmY2fsBPPz+JFTuKufEPaymvbQp3ZcYY0+fszuKOTL3a+fzSrVw9I5sfXjaBVbtLuOCX77B09X4amruc2zbGmJOKBUFHsnKdz03VUFvCjbNz+OvtsxmTmcg9L37K9J/8g28/t8FWKTXGDAhysi2ulpubq2vXrg39G733ECz/kfP1XbsgMZNAQFm+9TDPr8vnzS3O3sYj0+MBqGvy85dbzmFkejwiHV05a4wx4SMi61Q1t8PnLAg6EQjAnz4Pe991riT66mvgiz76dFF1A29sKuTNLYdZufPY5SgmD0+htrGFG2bnkF9exxfPyGLs4EQLCGNM2FgQ9Ja/BV64Eba8AqdMhXk/gjEXQJsf6KrK+v3lbCqo4pUNBRRWNnCwsuGYNiJw+tBk5ozLJDUuiqEpsUzJSiUrLY5mf4D4aF//fE/GGFeyIDgRgQB88Mi/hokAzv5/kPs1SBsF3o5/gBdVNVBQUc+GAxW8/mkhpbWNFFTU0+xX/IH2f+ZZaXEkRPtIjvMRF+3jzOw0ckelERftJSHaR05GAtE+m9IxxvSOBUFf+Ps98OHjxx4TD8xZDFP+3VmwrpsOlNVRWNVAXkkt2wqrWbp6P7PGZLB+fzllXVyimhYfRXldM+eems6sMRkcqqxnRk4644YkkZ4YTWpcFKW1TQxJju3td2mMGaAsCPqCKpTugqe/CBX7Om+XPQvO+pozFjTh805Y9GBuIBBQSmob2VVUQ12jn4OV9fzhgzwGJUTj83hYtef4+ySIQHpCDGeNSuO0IUnsL6vja7NzmDQ8pdt1GGMGFguCvuZvhqIt0FgNT116/PZpoyA+Hc77Dxg0BgaPP6G3V1WqGlo4UFbHzqJqKuqa2XG4hufXHaDZ3/nfZ2p8FGdkp5GVFkdBeT0j0xOYOCyZ88Zm4PN6SIuPsgltYwYoC4JQK94O/iYo2wv7PoCPftO91w07A0p3w9mLIHUkeHwweaHznOfE5gOaWgK0BAKs2u3srXCgrB6A/WV15JfXUdXQ0u412YPiuWzKKQxPi2N0RiJjBieQnhCD12PhYMzJzoKgv6lC+V7n81+/6VxxtOrRnp8nawZc9iBknAb15ZCQAR4vBPwQaAHxOkNPvQiNA2V17C+rY1NBJfvK6vi/j/aTGOOjvtnfbjI7OdbHZ07L5MIJQ2jxK7mj0hiZntDz78cYEzYWBJFA1fnh7Y1yHuevgy0vO1cknQhfHHzjI0gb6TyuLoSEwb3uUbT4AxysaODjA+Ws2l3KqxsPElClobn9onvDUmJZmDuCaJ+H6dmpzMxJxyPY8JIxEciC4GQR8MMnz8HhTRCXBmt+B9WHeneuq37vTG6PvRB2LYfYVJjx9V6XVtvYwuaDVTyxYjdvbyuigytgifI6ATAlK5WahhbmjMvkksmnMDUrxcLBmDCzIBgoAgHY9Lzzg73qoHN50N4V3X/9xCudy11TssATBVEnfplpcXUjz67ZT22Tnw37Kzq8qikp1sfFk4YyIyedYamxTDwlhZT4qBN+b2NM91kQDHSNNRCd4ITDs9dB/CA4uAHqSo77UuLTIf1UuOYZp9dwgpPUANUNzazNK6ewqoGHlu/glJQ4NhyoaNfu3FPTOTsnnakjUpmRM4jYKO8Jv7cxpmMWBG7mb3GGiIq3wfZl8MmzXbePSYbTL4ezb4Whk/usjPomP9sPV3Owop63thbx5pZCqju4ciktPoqLJ5/CvPGDOSM7jbSE6A7OZozpKQsC015tKVTuhyfmHr/tpb+C4WdAdBJknNp3JTS2UFBRz7vbiymoqGfZp4coqm48ps2UrBSaWgKU1DTyhTOyuP2CU0mKtWElY3rKgsAcX2M1NDdAeZ6z6mpTTedtk06BpKEw+d+cnkMfTgRXNzRT3dDCmrwy/u+j/XhE2s07pMVHMSUrlbNHpzM4KYbU+Chmj80gxmdDS8Z0JmxBICILgIcBL/A7VX2gzfNXAPcBAaAF+JaqvtfVOS0I+lHe+7D+D044VBd2vrTGJf/tDCclDnEe9/EVQqrKzqIath6qYldRDR/uKWVvSS0lNR2vy/Tzq6aQmRhDfnkdV8/IJspri/UZE5YgEBEvsAO4EMgH1gDXqOqWVm0SgVpVVRGZgrPBfZfrL1gQhJEqrP8j/OOH0FDRebuFTzlLaRRvhykLQ1ZOQUU9T3+4j11FNazb1/mCfZdOPoVzxqSTFOtj3ulDSIzxoap2SatxlXAFwTnAvao6P/j4HgBVvb+L9k+q6uldndeCIEL4m53hpEMbnaGkroy/DC78CaTl9MlVSV2pb/Kzq6iG93eXsHJnMe/v6nyRvpyMBD43dRhXzxjB0ORYCwYzoIUrCK4CFqjqTcHHXwZmquptbdpdCdwPDAYuVdVVHZzrZuBmgOzs7DP37eti9U8THkf+HX36F3jzB1BT2Hnb7HOcTX5GntMvpTW1BNhTUsNf1uZTVtvEa58cbLc4X2ZSDFOzUiipaaK8rolrZ2bzuanDGZoSa70HMyCEKwgWAvPbBMEMVb29k/afAX6oqp/t6rzWIzhJBAJQsBYKP4W/fbvzdlEJ8O0tEJfab6XBv5b7/uRAJYcq61mdV86avWUUVjW0azs4KYbvLhjPqYMTGZ2ZQLJdtWROQifF0FCwzV7gLFXt9E4oC4KTVHM91JXCnxc6S3h3ZPAEp80X/xdSs/+1flI/UlXe3HKY9fvKOVTZwOubDrXrPQxPjeOMkWlEeYSN+RWckZ3GHfPGMmJQfL/Xa0x3hSsIfDiTxfOAApzJ4i+p6uZWbU4Fdgcni88A/gpkaRdFWRAMEPUVzjDSsrs6bzPxCzBkgjO3MGTSCe/j0FtNLQE+LajkDx/k0dDsp8kfYE9xLfvL6tq1nTQ8mcFJsXzlnJGcMybdLmk1ESOcl49eAjyEc/nok6r6XyKyCEBVl4jIYuArQDNQD3zHLh91KX8LHPiw641+RsyEcRfDlKudS1VDPPF8PNsLq/nn9iK2F1azrbCarYeq2rVJivURF+XlyjOGc/bodCYPT2FQfDQe2+PB9DO7ocycfJpqYc878P4jTkB05tTPOlcl5d7grLkUk9hvJXakscXPih0l5JXUsuNwNa9vKqSmsf1SGqekxDIiLZ7VeWWcNzaD334ll5KaRrLSbHjJhIYFgTn5FX4KL94CLQ1Qtrvrttf/DWqLnXsZTpnSP/V1obHFT3ltM3mltazaXcoHu0vYVljd4VpLUV7hxtk5NDYH+PI5IxmTGd5gMwOHBYEZmPLegw1LoXCjExSdmXYdzLzZGU5KGtp/9R1HbWMLr28q5B9bCqlr8rNyZ/trJEamx5MaF8WojATGZCZywfjBDE6KsT2mTY9ZEJiBr7bE2e3t/Ye70Vhg2DTnJrfM8ZA4ONTVdVuzP8DWQ1Us31qEAKv3llFU3cDu4toO29998XhOzXQuax2WGkd+eT2nDrZehGnPgsC4U9E253LU1+6Eku3de83o8+HMr8LQKc4e0dGJzj7RYVbf5Of9XSXkl9fxpw/3dRoMAJdPHUZtYwsThyXzrc+ehtcmpg0WBMY4VJ01kna/Dc/f2P3XiceZd0gZAbVFkDEu7JPSR2wrrOK9nSWsyStjZ1ENe9oERFyUl5Hp8eRkJFBQUc+c0zKZP3Eo44cm4bPF+FzFgsCYrjTXO0NKW//q7BfdXWPnO5v3+Bth6jWQNsrZKS7Mmv0BDpTV8Ul+JR/tLWXn4Rp2F9dQXtfcru24IUlMHZHCmMxE5p0+xIaVBjALAmN66uAGSMiET56Bd38BLfU9e73HB19707kMtng7zPh6SMrsrkBA2VNSy5tbCvH7lQ/3lna4IN+k4ckEAjAqw7mM9evnjWZ6dlp/l2tCwILAmL4SCDhbf4oH9r0Hy74D/o6Xv+7U5IXOJPWUf3OGqVKzYchkSMwMTc1daGoJsL+sjj+uyqOyvplDFQ2szitr1y7KK9x+wVjW7y9n3vjBXHD6EIanxvV7vab3LAiM6Q/1FbD/Q9j6qnMV0843enee2d+G5GGQfbaztIZqv95FraocKKvnvV0lVDc08/g7u6msbz+sNH5oEtUNLczIGcSYzATGD01m3NAkW3MpQlkQGBNOAb/zubYE3n8I9q5whp0ObYT69r99dyomxVlvadRsZ17j1HnBnsRg5z28vpCUD044FFY1sHxrER/vLye/vJ5P8itoaA502P5rs3MYlBDNzJxB7C6uIXfUILs5LswsCIyJVIGA89t+TbFzY1x5nvNDfcffnWGjnhCvs9dDdDzsfBOGnQFXLoG4QRCfHpJehary8YEK1uaVsamgivXBkOjIlKwU9pfVMTI9gV8unEpijI+hKbF9XpPpmAWBMSczVWc3uIYK2PUW7PvAuau6+mDvzzliJpxzmzNXsfoJmH4dDD4dfDF9UK5SUtPEyp3FFJTX88t/7Oi0bfagePaX1TFiUBxXn5XN56YOIzMphtio8N+7MdBYEBgz0KlC0VYo3upsI/rSLc7x7HNgf7tN/7omXph1GzRUwVk3OTfU1Vc4cxbNdeCL61XvQlXZX1bHun3lPLP6QIeT0kdMHp7C9sJqMhKj+cXCqeSOSmPn4Rq7/+EEWBAY42aqTjj4G6EyH7a+BkWb4eDHzlBUT8Ukw/QvQ1QsZM+ChHRniCvjVPDF9rhX4Q8oGw5UsPFABT95bQuxUR4aWwJ09KMpxudhzmmZlNQ0ctaoQSyaM4bGloANMXWDBYExpmuqzkdzrbM0x4ePw+YXe3++9FOd5T3m3++sGDvmfOeGu26X40xOl1Q38fSH+/ikoLLD/R6OmJkziL0ltYzKSODGc0dx4YShBFTZW1LLqZmJtv8DFgTGmL7QWOP8tl9dCFtecb5e9RiU7+3+OUbMhAMfwYizYdh0+Oy9zl3dQybA6Zcf9+WqSnF1I79+exd/+nAfgxKiKavt+j6OCycM4da5YxiTmUhCtBef10NDsx+vR4hy0TCTBYExJvQaqqBiv3MfxfsPOz2Bnhg80RmyyvkMXPEYVB2ClOGQNOy4cxKqyge7S3lvVwkVdc28sbmw04CYNDyZTQVO7+K122czLDWO2sYWBifHDOitRS0IjDHhV7YHSnc7l7bGJDlrO5V0fkVRp8QLQyY6ofP5x2F8cHvTgxsgKh4yTzt6WW5TS4B1+8o5UF7H2rwyPtpbRlNLgEOVHYeU1yOMyUwgNT6aH39uIruLa7hsyrDef88RJJx7Fi8AHsbZs/h3qvpAm+evBRYHH9YAt6rqxq7OaUFgzADT3OBMZJfugpKdsHelM7+w4/Xenc8TBYFmOPsbEJsC4y9xFgdspaWpgY8LavmkoIo3NhV2eQWTR5ylvRNjfHz9vNEMS40j2nfyDSmFJQhExAvsAC4E8oE1wDWquqVVm1nAVlUtF5GLgXtVdWZX57UgMMZFAgEQgaYaqC8HDcCbP3CGn3rDFwunzXfmOBD4UfCc9eWoeCisqOXwvu28XT2CHYdr+PvmwuOecs5pmVwxbRh5pXV85ZyRRHk8JMf5Im73uHAFwTk4P9jnBx/fA6Cq93fSPg3YpKrDuzqvBYExpkPFO+CjJc5wU97KEzvXZQ9CyS6Y812aAsLhBh/b1r7Fn/KHkJ0ez9Mf7u/y5RmJ0aTFR7NozhimZ6cyOgKW1whXEFwFLFDVm4KPvwzMVNXbOml/FzD+SPs2z90M3AyQnZ195r59+0JSszFmgGppgqoC2PQCFH4S7BH0UvY5aPk+OPebVE38MmX7N/HO5gIqtr3Ls1xEfV0NdcTSzLFrP0V5hYsnncLccZlMHZHK6Axn74r+6jmEKwgWAvPbBMEMVb29g7bnA48Ds1W1/SLprViPwBjTZ1Sdxf/E46z0evhTeO8hGHcxvHjie0isjTuXqPpi/qPxJvboMAIcO7fg8winDk4kMcbHl2ZmMyghmonDUshIjO7zgIjooSERmQK8BFysqse9hMCCwBjT7/wtznDTx39yrlYq2gZN1X1y6nf9UxjjOcg9zTexMjCFEXKYAzoEgPPGZjA9O43kWB+npMQxf+KQXi+xEa4g8OFMFs8DCnAmi7+kqptbtckG3ga+oqofdOe8FgTGmIhSUwTeaGf5jqShziWsDZXwwSOw8x9QVwLRSVDZ9bxCW3mBIYzyHGZPYCjvBSbzpH8BV104h9suGNurMsN5+eglwEM4l48+qar/JSKLAFR1iYj8DvgicGTQv6WzQo+wIDDGnJQaq52hKG+Uc3d2xX7Y+y5UHYSNS7t1ioOTbmXYVQ8cv2EH7IYyY4w5GQQCzpCTN9oJCH+zExLeaCjaArNud1aB7YWugiB0WxoZY4zpGY/HuQkOIH2M8/nCH4f+bUP+DsYYYyKaBYExxricBYExxricBYExxricBYExxricBYExxricBYExxricBYExxrjcSXdnsYgU868lKXoqAyjpw3JCwWo8cZFeH0R+jZFeH1iNPTVSVTM7euKkC4ITISJrj7eWUbhZjScu0uuDyK8x0usDq7Ev2dCQMca4nAWBMca4nNuC4IlwF9ANVuOJi/T6IPJrjPT6wGrsM66aIzDGGNOe23oExhhj2rAgMMYYl3NNEIjIAhHZLiK7ROTuMNUwQkT+KSJbRWSziHwzeHyQiPxDRHYGP6e1es09wZq3i8j8fqzVKyIfi8hrkVajiKSKyPMisi34Z3lOJNUXfM87g3/Hm0RkqYjEhrtGEXlSRIpEZFOrYz2uSUTOFJFPg889IiISwvp+Efx7/kREXhKR1HDV11mNrZ67S0RURDLCWWOvqOqA/8DZM3k3MBqIBjYCE8JQxynAGcGvk4AdwATg58DdweN3Az8Lfj0hWGsMkBP8Hrz9VOu3gf8DXgs+jpgagT8ANwW/jgZSI6y+4cBeIC74+Dng+nDXCHwGOAPY1OpYj2sCVgPnAAK8DlwcwvouAnzBr38Wzvo6qzF4fATwBs7NrhnhrLE3H27pEcwAdqnqHlVtAp4BrujvIlT1kKquD35dDWzF+aFxBc4PN4KfPx/8+grgGVVtVNW9wC6c7yWkRCQLuBT4XavDEVGjiCTj/Gf8XwBVbVLVikiprxUfECciPiAeOBjuGlV1BVDW5nCPahKRU4BkVV2lzk+0P7Z6TZ/Xp6pvqmpL8OGHQFa46uusxqAHge8Cra++CUuNveGWIBgOHGj1OD94LGxEZBQwHfgIGKKqh8AJC2BwsFm46n4I5x91oNWxSKlxNFAM/D44dPU7EUmIoPpQ1QLgv4H9wCGgUlXfjKQaW+lpTcODX7c93h9uxPntGSKoPhH5HFCgqhvbPBUxNR6PW4Kgo/G3sF03KyKJwAvAt1S1qqumHRwLad0ichlQpKrruvuSDo6FskYfTtf8N6o6HajFGdLoTDj+DNNwfhvMAYYBCSJyXVcv6eBYuK/r7qymsNQqIt8HWoA/HznUSR39Wp+IxAPfB37Y0dOd1BJxf99uCYJ8nDG8I7Jwuur9TkSicELgz6r6YvDw4WB3keDnouDxcNR9LvA5EcnDGUK7QESejqAa84F8Vf0o+Ph5nGCIlPoAPgvsVdViVW0GXgRmRViNR/S0pnz+NTzT+njIiMhXgcuAa4NDKZFU3xicwN8Y/D+TBawXkaERVONxuSUI1gBjRSRHRKKBq4FX+7uI4JUB/wtsVdVftXrqVeCrwa+/CrzS6vjVIhIjIjnAWJxJppBR1XtUNUtVR+H8Ob2tqtdFSo2qWggcEJFxwUPzgC2RUl/QfuBsEYkP/p3Pw5kPiqQaj+hRTcHho2oROTv4vX2l1Wv6nIgsABYDn1PVujZ1h70+Vf1UVQer6qjg/5l8nAtCCiOlxm4J50x1f34Al+BcpbMb+H6YapiN0wX8BNgQ/LgESAfeAnYGPw9q9ZrvB2veTj9fWQDM5V9XDUVMjcA0YG3wz/FlIC2S6gu+54+BbcAm4E84V46EtUZgKc6cRTPOD6yv9aYmIDf4fe0GHiW4QkGI6tuFM85+5P/LknDV11mNbZ7PI3jVULhq7M2HLTFhjDEu55ahIWOMMZ2wIDDGGJezIDDGGJezIDDGGJezIDDGGJezIDCmH4nIXAmu6GpMpLAgMMYYl7MgMKYDInKdiKwWkQ0i8j/i7M9QIyK/FJH1IvKWiGQG204TkQ9brZmfFjx+qogsF5GNwdeMCZ4+Uf61n8Kfw74WvXE9CwJj2hCR04F/B85V1WmAH7gWSADWq+oZwLvAj4Iv+SOwWFWnAJ+2Ov5n4DFVnYqz1tCh4PHpwLdw1qsfjbO+kzFh4wt3AcZEoHnAmcCa4C/rcTiLsQWAZ4NtngZeFJEUIFVV3w0e/wPwFxFJAoar6ksAqtoAEDzfalXNDz7eAIwC3gv5d2VMJywIjGlPgD+o6j3HHBT5QZt2Xa3P0tVwT2Orr/3Y/0MTZjY0ZEx7bwFXichgOLqv70ic/y9XBdt8CXhPVSuBchE5L3j8y8C76uwzkS8inw+eIya4dr0xEcd+EzGmDVXdIiL/CbwpIh6clSa/gbMJzkQRWQdU4swjgLN885LgD/o9wA3B418G/kdEfhI8x8J+/DaM6TZbfdSYbhKRGlVNDHcdxvQ1GxoyxhiXsx6BMca4nPUIjDHG5SwIjDHG5SwIjDHG5SwIjDHG5SwIjDHG5f4/T1sfA90IkJEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       148\n",
      "           1       1.00      0.96      0.98       160\n",
      "\n",
      "    accuracy                           0.98       308\n",
      "   macro avg       0.98      0.98      0.98       308\n",
      "weighted avg       0.98      0.98      0.98       308\n",
      "\n",
      "[[148   0]\n",
      " [  6 154]]\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "#Read target column as y\n",
    "y=dataset.target\n",
    "\n",
    "#Take all of column and drop target column as x\n",
    "x=dataset.drop('target',axis=1)\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler() #Normalizing data to value from 0-1\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "# Split data into training and test \n",
    "X_train,X_test,y_train,y_test=train_test_split(x_scaled,y,test_size=0.3,random_state=4)\n",
    "\n",
    "\n",
    "\n",
    "# Define the keras model\n",
    "model = Sequential()\n",
    "\n",
    "#First hidden layer with 10 neuron, 13 input (using sigmoid activation function)\n",
    "model.add(Dense(10, input_shape=(13,), activation='sigmoid'))\n",
    "\n",
    "#Second hidden layer with 10 neuron (using sigmoid activation function)\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "#Output layer with 1 neuron (using sigmoid activation function)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "trained = model.fit(X_train, y_train, epochs=1500, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "#evaluate the keras model\n",
    "_,accuracy = model.evaluate(X_train, y_train)\n",
    "\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "#Plotting accuracy and loss chart\n",
    "plt.plot(trained.history['accuracy'] )\n",
    "plt.plot(trained.history['val_accuracy'])\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training Accuracy' , 'Test Accuracy'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(trained.history['loss'])\n",
    "plt.plot(trained.history['val_loss'])\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training Loss' , 'Test Loss'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Tak faham coding ni tapi aku rembat lol\n",
    "classifier_tree = DecisionTreeClassifier()\n",
    "y_predict = classifier_tree.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 409us/step\n",
      "[0] [0.25, 0.0, 0.3333333333333333, 0.339622641509434, 0.1780821917808219, 0.0, 0.0, 0.7709923664122137, 0.0, 0.2258064516129032, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[1] [0.8124999999999999, 1.0, 0.6666666666666666, 0.2264150943396227, 0.34474885844748854, 0.0, 0.5, 0.6106870229007634, 0.0, 0.16129032258064516, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[2] [0.6874999999999999, 0.0, 0.0, 0.4339622641509434, 0.6118721461187213, 0.0, 0.0, 0.6564885496183205, 0.0, 0.1935483870967742, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[3] [0.6041666666666666, 0.0, 1.0, 0.5283018867924528, 0.3584474885844749, 1.0, 0.0, 0.6946564885496184, 0.0, 0.16129032258064516, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[4] [0.6249999999999999, 0.0, 0.0, 0.7547169811320754, 0.28082191780821913, 0.0, 0.5, 0.5496183206106869, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[5] [0.7499999999999999, 1.0, 0.0, 0.24528301886792458, 0.11643835616438353, 0.0, 0.5, 0.5267175572519084, 0.0, 0.06451612903225806, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[6] [0.7916666666666666, 1.0, 0.0, 0.05660377358490576, 0.3949771689497717, 0.0, 0.0, 0.4122137404580152, 1.0, 0.14516129032258066, 0.5, 0.5, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[7] [0.1875, 1.0, 0.6666666666666666, 0.41509433962264153, 0.11187214611872148, 0.0, 0.5, 0.7786259541984731, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[8] [0.7916666666666666, 1.0, 0.0, 0.24528301886792458, 0.23515981735159813, 0.0, 0.0, 0.4427480916030534, 1.0, 0.41935483870967744, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[9] [0.7291666666666666, 1.0, 0.0, 0.24528301886792458, 0.27397260273972607, 0.0, 0.0, 0.19083969465648853, 1.0, 0.3548387096774194, 0.0, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[10] [0.9374999999999999, 0.0, 0.3333333333333333, 0.24528301886792458, 0.32648401826484014, 0.0, 0.0, 0.38167938931297707, 1.0, 0.03225806451612903, 1.0, 0.25, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[11] [0.7291666666666666, 1.0, 0.6666666666666666, 0.4339622641509434, 0.4771689497716895, 0.0, 0.5, 0.6641221374045801, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[12] [0.125, 1.0, 0.0, 0.30188679245283023, 0.35616438356164376, 0.0, 0.0, 0.648854961832061, 1.0, 0.0, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[13] [0.5833333333333334, 1.0, 0.0, 0.339622641509434, 0.011415525114155278, 0.0, 0.5, 0.33587786259541985, 1.0, 0.1935483870967742, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[14] [0.5416666666666666, 1.0, 0.0, 0.6226415094339622, 0.37214611872146114, 0.0, 0.0, 0.564885496183206, 1.0, 0.12903225806451613, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[15] [0.5833333333333334, 0.0, 0.0, 0.24528301886792458, 0.5205479452054793, 0.0, 0.5, 0.7022900763358778, 1.0, 0.0967741935483871, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[16] [0.25, 1.0, 0.3333333333333333, 0.15094339622641517, 0.2488584474885845, 0.0, 0.5, 0.6259541984732825, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[17] [0.7708333333333334, 0.0, 0.0, 0.7924528301886792, 0.2328767123287671, 1.0, 0.5, 0.7175572519083969, 1.0, 0.16129032258064516, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[18] [0.33333333333333337, 1.0, 0.3333333333333333, 0.3207547169811321, 0.4155251141552511, 0.0, 0.0, 0.7557251908396946, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[19] [0.5624999999999999, 1.0, 1.0, 0.24528301886792458, 0.15296803652968033, 0.0, 0.0, 0.6946564885496184, 0.0, 0.30645161290322576, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[20] [0.6041666666666666, 0.0, 0.3333333333333333, 0.39622641509433965, 0.4406392694063927, 1.0, 0.0, 0.6183206106870228, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[21] [0.6041666666666666, 1.0, 0.0, 0.3207547169811321, 0.2054794520547945, 0.0, 0.0, 0.4580152671755725, 1.0, 0.3548387096774194, 0.5, 0.75, 1.0] => 0 (expected 1) SALAH\n",
      "[22] [0.375, 1.0, 0.6666666666666666, 0.1320754716981133, 0.2671232876712329, 0.0, 0.5, 0.6183206106870228, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[23] [0.5208333333333334, 1.0, 0.0, 0.26415094339622647, 0.36529680365296807, 0.0, 0.0, 0.3435114503816793, 1.0, 0.5161290322580645, 0.5, 0.5, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[24] [0.8541666666666666, 1.0, 0.0, 0.48113207547169823, 0.1095890410958904, 0.0, 0.5, 0.4122137404580152, 1.0, 0.41935483870967744, 0.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[25] [0.7708333333333334, 0.0, 0.0, 0.7924528301886792, 0.2328767123287671, 1.0, 0.5, 0.7175572519083969, 1.0, 0.16129032258064516, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[26] [0.6041666666666666, 1.0, 0.0, 0.3207547169811321, 0.2054794520547945, 0.0, 0.0, 0.4580152671755725, 1.0, 0.3548387096774194, 0.5, 0.75, 1.0] => 0 (expected 1) SALAH\n",
      "[27] [0.5208333333333334, 1.0, 0.6666666666666666, 0.5283018867924528, 0.2420091324200913, 0.0, 0.0, 0.7175572519083969, 0.0, 0.25806451612903225, 1.0, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[28] [0.6874999999999999, 1.0, 0.0, 0.24528301886792458, 0.3219178082191781, 0.0, 0.5, 0.2137404580152671, 1.0, 0.2903225806451613, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[29] [0.45833333333333337, 1.0, 0.0, 0.4339622641509434, 0.3949771689497717, 0.0, 0.5, 0.7786259541984731, 1.0, 0.25806451612903225, 1.0, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[30] [0.7083333333333334, 0.0, 0.0, 0.1320754716981133, 0.32648401826484014, 0.0, 0.5, 0.7480916030534351, 1.0, 0.2903225806451613, 0.5, 0.5, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[31] [0.6666666666666666, 1.0, 0.0, 0.24528301886792458, 0.3059360730593607, 0.0, 0.5, 0.5267175572519084, 1.0, 0.5806451612903226, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[32] [0.35416666666666663, 1.0, 0.0, 0.24528301886792458, 0.28082191780821913, 0.0, 0.0, 0.5572519083969466, 0.0, 0.12903225806451613, 1.0, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[33] [0.6041666666666666, 1.0, 0.0, 0.49056603773584906, 0.2100456621004566, 0.0, 0.5, 0.2595419847328244, 0.0, 0.3225806451612903, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[34] [0.5624999999999999, 1.0, 0.3333333333333333, 0.24528301886792458, 0.2511415525114155, 0.0, 0.5, 0.816793893129771, 0.0, 0.12903225806451613, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[35] [0.6041666666666666, 0.0, 1.0, 0.5283018867924528, 0.3584474885844749, 1.0, 0.0, 0.6946564885496184, 0.0, 0.16129032258064516, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[36] [0.35416666666666663, 1.0, 0.3333333333333333, 0.0660377358490567, 0.16210045662100453, 1.0, 0.5, 0.648854961832061, 0.0, 0.0, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[37] [0.5833333333333334, 1.0, 0.0, 0.5471698113207547, 0.33789954337899536, 0.0, 0.5, 0.1297709923664122, 1.0, 0.1935483870967742, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[38] [0.6249999999999999, 1.0, 1.0, 0.37735849056603776, 0.1780821917808219, 0.0, 0.5, 0.6946564885496184, 0.0, 0.12903225806451613, 1.0, 0.5, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[39] [0.5208333333333334, 1.0, 0.3333333333333333, 0.9245283018867926, 0.3584474885844749, 0.0, 0.0, 0.9465648854961831, 0.0, 0.0, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[40] [0.3125, 1.0, 0.3333333333333333, 0.339622641509434, 0.2123287671232877, 0.0, 0.0, 0.8931297709923663, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[41] [0.6249999999999999, 1.0, 0.0, 0.4339622641509434, 0.11643835616438353, 0.0, 0.5, 0.6946564885496184, 1.0, 0.0, 1.0, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[42] [0.8749999999999999, 0.0, 0.0, 0.16981132075471705, 0.052511415525114125, 0.0, 0.5, 0.4122137404580152, 0.0, 0.25806451612903225, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[43] [0.29166666666666663, 1.0, 0.6666666666666666, 0.339622641509434, 0.4315068493150685, 0.0, 0.5, 0.6946564885496184, 0.0, 0.30645161290322576, 1.0, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[44] [0.7291666666666666, 1.0, 1.0, 0.7169811320754716, 0.2305936073059361, 0.0, 0.0, 0.6412213740458014, 0.0, 0.0967741935483871, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[45] [0.39583333333333337, 1.0, 0.3333333333333333, 0.15094339622641517, 0.23515981735159813, 0.0, 0.5, 0.7404580152671755, 0.0, 0.16129032258064516, 0.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[46] [0.4374999999999999, 0.0, 0.3333333333333333, 0.24528301886792458, 0.2694063926940639, 0.0, 0.5, 0.6946564885496184, 0.0, 0.1774193548387097, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[47] [0.6249999999999999, 1.0, 1.0, 0.37735849056603776, 0.1780821917808219, 0.0, 0.5, 0.6946564885496184, 0.0, 0.12903225806451613, 1.0, 0.5, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[48] [0.6458333333333334, 1.0, 0.0, 0.4339622641509434, 0.38127853881278534, 0.0, 0.0, 0.7557251908396946, 0.0, 0.1935483870967742, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[49] [0.6249999999999999, 1.0, 0.6666666666666666, 0.30188679245283023, 0.2100456621004566, 1.0, 0.5, 0.48091603053435106, 0.0, 0.3548387096774194, 0.5, 0.25, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[50] [0.7499999999999999, 1.0, 1.0, 0.41509433962264153, 0.35616438356164376, 1.0, 0.0, 0.7862595419847328, 0.0, 0.2258064516129032, 0.5, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[51] [0.20833333333333337, 0.0, 0.6666666666666666, 0.0, 0.16666666666666669, 0.0, 0.5, 0.8244274809160305, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[52] [0.7291666666666666, 1.0, 1.0, 0.7169811320754716, 0.2305936073059361, 0.0, 0.0, 0.6412213740458014, 0.0, 0.0967741935483871, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[53] [0.4374999999999999, 0.0, 0.6666666666666666, 0.24528301886792458, 0.2123287671232877, 0.0, 0.5, 0.6641221374045801, 0.0, 0.25806451612903225, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[54] [0.7916666666666666, 1.0, 0.0, 0.24528301886792458, 0.23515981735159813, 0.0, 0.0, 0.4427480916030534, 1.0, 0.41935483870967744, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[55] [0.6874999999999999, 0.0, 0.0, 0.4339622641509434, 0.3242009132420091, 0.0, 0.0, 0.6793893129770993, 0.0, 0.5806451612903226, 0.0, 0.5, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[56] [0.7499999999999999, 1.0, 1.0, 0.41509433962264153, 0.35616438356164376, 1.0, 0.0, 0.7862595419847328, 0.0, 0.2258064516129032, 0.5, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[57] [0.6041666666666666, 0.0, 0.0, 0.7169811320754716, 0.22602739726027393, 1.0, 0.0, 0.5725190839694655, 1.0, 0.4516129032258064, 0.5, 0.5, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[58] [0.6458333333333334, 0.0, 0.0, 0.5283018867924528, 0.30136986301369856, 0.0, 0.0, 0.6564885496183205, 0.0, 0.41935483870967744, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[59] [0.47916666666666663, 1.0, 0.0, 0.2924528301886792, 0.1963470319634703, 0.0, 0.5, 0.7404580152671755, 0.0, 0.16129032258064516, 1.0, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[60] [0.35416666666666663, 1.0, 0.3333333333333333, 0.0660377358490567, 0.16210045662100453, 1.0, 0.5, 0.648854961832061, 0.0, 0.0, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[61] [0.8541666666666666, 1.0, 0.0, 0.339622641509434, 0.44748858447488576, 0.0, 0.0, 0.2900763358778625, 0.0, 0.3870967741935484, 0.5, 0.75, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[62] [0.29166666666666663, 1.0, 0.0, 0.24528301886792458, 0.11643835616438353, 0.0, 0.0, 0.3740458015267175, 1.0, 0.4032258064516129, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[63] [0.39583333333333337, 1.0, 0.3333333333333333, 0.339622641509434, 0.27168949771689493, 0.0, 0.0, 0.8320610687022899, 0.0, 0.03225806451612903, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[64] [0.7708333333333334, 1.0, 0.0, 0.16981132075471705, 0.1963470319634703, 0.0, 0.0, 0.46564885496183195, 1.0, 0.016129032258064516, 1.0, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[65] [0.5833333333333334, 1.0, 0.6666666666666666, 0.5283018867924528, 0.0, 1.0, 0.5, 0.7786259541984731, 0.0, 0.03225806451612903, 1.0, 0.25, 1.0] => 1 (expected 0) SALAH\n",
      "[66] [0.5624999999999999, 1.0, 0.3333333333333333, 0.24528301886792458, 0.2511415525114155, 0.0, 0.5, 0.816793893129771, 0.0, 0.12903225806451613, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[67] [0.25, 1.0, 0.6666666666666666, 0.339622641509434, 0.2009132420091324, 0.0, 0.0, 0.7404580152671755, 0.0, 0.3225806451612903, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[68] [0.27083333333333337, 1.0, 0.0, 0.4339622641509434, 0.22831050228310495, 0.0, 0.5, 0.816793893129771, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[69] [0.35416666666666663, 1.0, 0.0, 0.4339622641509434, 0.4223744292237443, 0.0, 0.5, 0.3740458015267175, 1.0, 0.2903225806451613, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[70] [0.6041666666666666, 0.0, 0.3333333333333333, 0.39622641509433965, 0.4406392694063927, 1.0, 0.0, 0.6183206106870228, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[71] [0.7083333333333334, 1.0, 0.0, 0.339622641509434, 0.29223744292237447, 0.0, 0.0, 0.5801526717557252, 0.0, 0.2258064516129032, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[72] [0.47916666666666663, 1.0, 1.0, 0.2264150943396227, 0.136986301369863, 0.0, 0.0, 0.9083969465648855, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333] => 1 (expected 0) SALAH\n",
      "[73] [0.7499999999999999, 1.0, 0.0, 0.3867924528301888, 0.29223744292237447, 0.0, 0.0, 0.4274809160305343, 0.0, 0.4516129032258064, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[74] [0.4374999999999999, 1.0, 0.0, 0.5283018867924528, 0.2671232876712329, 0.0, 0.0, 0.43511450381679384, 0.0, 0.41935483870967744, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[75] [0.35416666666666663, 1.0, 0.0, 0.24528301886792458, 0.28082191780821913, 0.0, 0.0, 0.5572519083969466, 0.0, 0.12903225806451613, 1.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[76] [0.6666666666666666, 0.0, 0.0, 0.339622641509434, 0.46575342465753417, 0.0, 0.0, 0.7480916030534351, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[77] [0.22916666666666663, 1.0, 0.0, 0.5471698113207547, 0.2214611872146119, 0.0, 0.5, 0.8396946564885496, 0.0, 0.0, 1.0, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[78] [0.5833333333333334, 1.0, 0.0, 0.669811320754717, 0.37214611872146114, 1.0, 0.0, 0.4045801526717556, 0.0, 0.16129032258064516, 0.5, 0.75, 1.0] => 0 (expected 1) SALAH\n",
      "[79] [0.22916666666666663, 1.0, 1.0, 0.4339622641509434, 0.16666666666666669, 0.0, 0.5, 0.816793893129771, 1.0, 0.2258064516129032, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[80] [0.4999999999999999, 0.0, 0.6666666666666666, 0.3207547169811321, 0.2054794520547945, 0.0, 0.0, 0.33587786259541985, 0.0, 0.0, 1.0, 0.0, 0.0] => 1 (expected 0) SALAH\n",
      "[81] [0.5624999999999999, 1.0, 0.0, 0.339622641509434, 0.3584474885844749, 1.0, 0.0, 0.2442748091603053, 1.0, 0.25806451612903225, 0.0, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[82] [0.33333333333333337, 1.0, 1.0, 0.15094339622641517, 0.3150684931506849, 0.0, 0.5, 0.46564885496183195, 0.0, 0.1935483870967742, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[83] [0.5833333333333334, 0.0, 0.3333333333333333, 0.339622641509434, 0.2511415525114155, 0.0, 0.0, 0.7862595419847328, 0.0, 0.0, 0.5, 0.25, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[84] [0.47916666666666663, 1.0, 0.0, 0.2924528301886792, 0.1963470319634703, 0.0, 0.5, 0.7404580152671755, 0.0, 0.16129032258064516, 1.0, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[85] [0.6666666666666666, 1.0, 0.0, 0.24528301886792458, 0.3059360730593607, 0.0, 0.5, 0.5267175572519084, 1.0, 0.5806451612903226, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[86] [0.20833333333333337, 0.0, 0.6666666666666666, 0.41509433962264153, 0.2146118721461187, 0.0, 0.5, 0.6183206106870228, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[87] [0.27083333333333337, 1.0, 0.0, 0.4339622641509434, 0.22831050228310495, 0.0, 0.5, 0.816793893129771, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[88] [0.25, 1.0, 0.3333333333333333, 0.24528301886792458, 0.07077625570776253, 0.0, 0.5, 0.847328244274809, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[89] [0.4999999999999999, 1.0, 0.0, 0.2735849056603773, 0.35616438356164376, 0.0, 0.5, 0.18320610687022898, 1.0, 0.3225806451612903, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[90] [0.4999999999999999, 0.0, 0.0, 0.339622641509434, 0.3150684931506849, 0.0, 0.0, 0.5496183206106869, 0.0, 0.06451612903225806, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[91] [0.6874999999999999, 0.0, 0.0, 0.41509433962264153, 0.3835616438356165, 1.0, 0.5, 0.26717557251908397, 0.0, 0.30645161290322576, 0.5, 0.75, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[92] [0.4374999999999999, 1.0, 0.6666666666666666, 0.4339622641509434, 0.24429223744292233, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0967741935483871, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[93] [0.35416666666666663, 1.0, 0.6666666666666666, 0.5283018867924528, 0.2397260273972603, 0.0, 0.5, 0.5801526717557252, 0.0, 0.5806451612903226, 0.5, 0.0, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[94] [0.7499999999999999, 0.0, 0.6666666666666666, 0.5754716981132076, 0.32648401826484014, 0.0, 0.5, 0.5877862595419846, 0.0, 0.12903225806451613, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[95] [0.5208333333333334, 1.0, 0.0, 0.15094339622641517, 0.2579908675799087, 0.0, 0.5, 0.41984732824427473, 1.0, 0.4516129032258064, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[96] [0.6249999999999999, 1.0, 1.0, 0.6226415094339622, 0.33561643835616434, 0.0, 0.0, 0.4122137404580152, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[97] [0.6041666666666666, 1.0, 0.6666666666666666, 0.3584905660377359, 0.2237442922374429, 0.0, 0.0, 0.7786259541984731, 0.0, 0.5161290322580645, 1.0, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[98] [0.5416666666666666, 0.0, 0.3333333333333333, 0.3867924528301888, 0.28310502283105016, 0.0, 0.0, 0.6870229007633587, 0.0, 0.2258064516129032, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[99] [0.33333333333333337, 1.0, 0.0, 0.4528301886792453, 0.41780821917808214, 0.0, 0.0, 0.5801526717557252, 1.0, 0.0, 0.5, 0.75, 1.0] => 0 (expected 1) SALAH\n",
      "[100] [0.8333333333333334, 1.0, 1.0, 0.6226415094339622, 0.24657534246575336, 1.0, 0.0, 0.4580152671755725, 0.0, 0.016129032258064516, 0.5, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[101] [0.7083333333333334, 1.0, 1.0, 0.48113207547169823, 0.24429223744292233, 1.0, 0.0, 0.6030534351145037, 0.0, 0.3709677419354838, 0.0, 0.0, 0.3333333333333333] => 1 (expected 1) BETUL\n",
      "[102] [0.7916666666666666, 1.0, 0.0, 0.24528301886792458, 0.25342465753424653, 0.0, 0.5, 0.0, 0.0, 0.16129032258064516, 0.5, 0.0, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[103] [0.4999999999999999, 1.0, 0.0, 0.2735849056603773, 0.35616438356164376, 0.0, 0.5, 0.18320610687022898, 1.0, 0.3225806451612903, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[104] [0.6458333333333334, 0.0, 0.6666666666666666, 0.07547169811320764, 0.4383561643835617, 0.0, 0.5, 0.6793893129770993, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[105] [0.7916666666666666, 1.0, 0.0, 0.24528301886792458, 0.25342465753424653, 0.0, 0.5, 0.0, 0.0, 0.16129032258064516, 0.5, 0.0, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[106] [0.27083333333333337, 1.0, 0.6666666666666666, 0.339622641509434, 0.1232876712328767, 0.0, 0.5, 0.6030534351145037, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[107] [0.4374999999999999, 0.0, 0.3333333333333333, 0.24528301886792458, 0.2694063926940639, 0.0, 0.5, 0.6946564885496184, 0.0, 0.1774193548387097, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[108] [0.5833333333333334, 1.0, 0.3333333333333333, 0.5660377358490566, 0.2420091324200913, 0.0, 0.0, 0.7099236641221373, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[109] [0.7291666666666666, 1.0, 0.0, 0.48113207547169823, 0.1963470319634703, 0.0, 0.0, 0.46564885496183195, 0.0, 0.3225806451612903, 0.5, 0.5, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[110] [0.7708333333333334, 0.0, 0.6666666666666666, 0.49056603773584906, 0.34703196347031967, 0.0, 0.0, 0.6183206106870228, 0.0, 0.0, 0.5, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[111] [0.5624999999999999, 1.0, 0.0, 0.3584905660377359, 0.1324200913242009, 0.0, 0.0, 0.2595419847328244, 1.0, 0.33870967741935487, 0.5, 0.25, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[112] [0.6666666666666666, 0.0, 0.0, 0.48113207547169823, 0.4132420091324201, 0.0, 0.0, 0.5725190839694655, 1.0, 0.16129032258064516, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[113] [0.20833333333333337, 0.0, 0.6666666666666666, 0.41509433962264153, 0.2146118721461187, 0.0, 0.5, 0.6183206106870228, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[114] [0.3125, 1.0, 0.6666666666666666, 0.4339622641509434, 0.2488584474885845, 0.0, 0.0, 0.8320610687022899, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[115] [0.6249999999999999, 1.0, 0.0, 0.660377358490566, 0.1141552511415525, 1.0, 0.0, 0.1450381679389312, 0.0, 0.16129032258064516, 0.5, 0.5, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[116] [0.6666666666666666, 1.0, 0.0, 0.4339622641509434, 0.18493150684931509, 0.0, 0.0, 0.5114503816793893, 1.0, 0.30645161290322576, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[117] [0.5624999999999999, 1.0, 0.0, 0.339622641509434, 0.3584474885844749, 1.0, 0.0, 0.2442748091603053, 1.0, 0.25806451612903225, 0.0, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[118] [0.5208333333333334, 0.0, 0.6666666666666666, 0.3867924528301888, 0.4063926940639269, 1.0, 0.5, 0.7557251908396946, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[119] [0.47916666666666663, 1.0, 0.0, 0.3207547169811321, 0.2945205479452055, 0.0, 0.5, 0.6870229007633587, 1.0, 0.0, 1.0, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[120] [0.6249999999999999, 1.0, 1.0, 0.7169811320754716, 0.3698630136986301, 0.0, 0.0, 0.6717557251908396, 0.0, 0.03225806451612903, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[121] [0.33333333333333337, 0.0, 0.3333333333333333, 0.16981132075471705, 0.0776255707762557, 0.0, 0.5, 0.5114503816793893, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[122] [0.375, 1.0, 0.6666666666666666, 0.41509433962264153, 0.29908675799086754, 0.0, 0.0, 0.648854961832061, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[123] [0.5208333333333334, 0.0, 0.6666666666666666, 0.3867924528301888, 0.4063926940639269, 1.0, 0.5, 0.7557251908396946, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[124] [0.5833333333333334, 1.0, 0.0, 0.4339622641509434, 0.1506849315068493, 0.0, 0.5, 0.5877862595419846, 0.0, 0.06451612903225806, 0.5, 0.0, 0.3333333333333333] => 1 (expected 0) SALAH\n",
      "[125] [0.6874999999999999, 1.0, 0.0, 0.24528301886792458, 0.3219178082191781, 0.0, 0.5, 0.2137404580152671, 1.0, 0.2903225806451613, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[126] [0.5833333333333334, 1.0, 0.6666666666666666, 0.5283018867924528, 0.0, 1.0, 0.5, 0.7786259541984731, 0.0, 0.03225806451612903, 1.0, 0.25, 1.0] => 1 (expected 1) BETUL\n",
      "[127] [0.6249999999999999, 1.0, 0.3333333333333333, 0.4339622641509434, 0.21689497716894973, 0.0, 0.5, 0.7099236641221373, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[128] [0.6041666666666666, 1.0, 0.0, 0.5283018867924528, 0.32876712328767127, 0.0, 0.0, 0.30534351145038163, 1.0, 0.12903225806451613, 1.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[129] [0.29166666666666663, 1.0, 0.0, 0.3584905660377359, 0.2762557077625571, 1.0, 0.0, 0.5496183206106869, 1.0, 0.016129032258064516, 0.5, 1.0, 1.0] => 0 (expected 1) SALAH\n",
      "[130] [0.33333333333333337, 0.0, 0.3333333333333333, 0.339622641509434, 0.24657534246575336, 0.0, 0.0, 0.7938931297709922, 0.0, 0.0967741935483871, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[131] [0.45833333333333337, 1.0, 0.0, 0.4339622641509434, 0.3949771689497717, 0.0, 0.5, 0.7786259541984731, 1.0, 0.25806451612903225, 1.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[132] [0.7708333333333334, 1.0, 0.0, 0.24528301886792458, 0.4018264840182649, 0.0, 0.0, 0.6106870229007634, 0.0, 0.06451612903225806, 0.5, 0.0, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[133] [0.3125, 1.0, 0.0, 0.16981132075471705, 0.37442922374429216, 0.0, 0.0, 0.6259541984732825, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[134] [0.45833333333333337, 0.0, 0.0, 0.339622641509434, 0.40867579908675794, 0.0, 0.5, 0.5419847328244275, 1.0, 0.1935483870967742, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[135] [0.8124999999999999, 1.0, 0.6666666666666666, 0.8113207547169811, 0.33789954337899536, 1.0, 0.0, 0.6030534351145037, 1.0, 0.25806451612903225, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[136] [0.6249999999999999, 1.0, 0.0, 0.41509433962264153, 0.3310502283105023, 0.0, 0.0, 0.847328244274809, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[137] [0.16666666666666663, 0.0, 0.6666666666666666, 0.24528301886792458, 0.2031963470319635, 0.0, 0.5, 0.7557251908396946, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[138] [0.6041666666666666, 1.0, 0.0, 0.05660377358490576, 0.24657534246575336, 0.0, 0.5, 0.648854961832061, 0.0, 0.016129032258064516, 1.0, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[139] [0.5833333333333334, 1.0, 0.0, 0.3584905660377359, 0.18493150684931509, 0.0, 0.5, 0.7404580152671755, 1.0, 0.0, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[140] [0.25, 0.0, 0.6666666666666666, 0.16981132075471705, 0.3242009132420091, 0.0, 0.0, 0.7709923664122137, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[141] [0.7916666666666666, 1.0, 0.0, 0.24528301886792458, 0.25342465753424653, 0.0, 0.5, 0.0, 0.0, 0.16129032258064516, 0.5, 0.0, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[142] [0.7499999999999999, 1.0, 0.0, 0.24528301886792458, 0.11643835616438353, 0.0, 0.5, 0.5267175572519084, 0.0, 0.06451612903225806, 1.0, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[143] [0.5624999999999999, 0.0, 0.0, 1.0, 0.3698630136986301, 1.0, 0.0, 0.4732824427480916, 1.0, 0.6451612903225806, 0.0, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[144] [0.7499999999999999, 1.0, 0.0, 0.3867924528301888, 0.29223744292237447, 0.0, 0.0, 0.4274809160305343, 0.0, 0.4516129032258064, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[145] [0.6666666666666666, 1.0, 0.0, 0.5094339622641509, 0.17579908675799089, 0.0, 0.5, 0.6870229007633587, 0.0, 0.0, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[146] [0.47916666666666663, 1.0, 0.3333333333333333, 0.3207547169811321, 0.18036529680365293, 1.0, 0.5, 0.8625954198473281, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[147] [0.27083333333333337, 1.0, 0.0, 0.39622641509433965, 0.4315068493150685, 0.0, 0.5, 0.4122137404580152, 1.0, 0.2903225806451613, 0.5, 0.0, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[148] [0.7291666666666666, 0.0, 0.0, 0.339622641509434, 0.4041095890410959, 0.0, 0.5, 0.3893129770992366, 0.0, 0.3225806451612903, 0.5, 0.5, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[149] [0.5208333333333334, 1.0, 0.6666666666666666, 0.2924528301886792, 0.33561643835616434, 0.0, 0.0, 0.6183206106870228, 0.0, 0.08064516129032258, 0.0, 0.25, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[150] [0.7916666666666666, 0.0, 0.6666666666666666, 0.5471698113207547, 0.34474885844748854, 0.0, 0.5, 0.7709923664122137, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[151] [0.5208333333333334, 1.0, 0.0, 0.4339622641509434, 0.2579908675799087, 0.0, 0.5, 0.6793893129770993, 0.0, 0.1935483870967742, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[152] [0.33333333333333337, 1.0, 0.0, 0.4528301886792453, 0.41780821917808214, 0.0, 0.0, 0.5801526717557252, 1.0, 0.0, 0.5, 0.75, 1.0] => 0 (expected 0) BETUL\n",
      "[153] [0.7291666666666666, 1.0, 0.0, 0.24528301886792458, 0.27397260273972607, 0.0, 0.0, 0.19083969465648853, 1.0, 0.3548387096774194, 0.0, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[154] [0.6041666666666666, 0.0, 0.0, 0.7169811320754716, 0.22602739726027393, 1.0, 0.0, 0.5725190839694655, 1.0, 0.4516129032258064, 0.5, 0.5, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[155] [0.47916666666666663, 1.0, 1.0, 0.5471698113207547, 0.39269406392694056, 1.0, 0.5, 0.816793893129771, 0.0, 0.1935483870967742, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[156] [0.47916666666666663, 1.0, 0.3333333333333333, 0.24528301886792458, 0.45433789954337894, 0.0, 0.5, 0.7709923664122137, 0.0, 0.03225806451612903, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[157] [0.7916666666666666, 1.0, 0.6666666666666666, 0.5471698113207547, 0.1963470319634703, 0.0, 0.0, 0.6030534351145037, 0.0, 0.12903225806451613, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[158] [0.6249999999999999, 1.0, 1.0, 0.37735849056603776, 0.1780821917808219, 0.0, 0.5, 0.6946564885496184, 0.0, 0.12903225806451613, 1.0, 0.5, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[159] [0.5624999999999999, 1.0, 0.3333333333333333, 0.24528301886792458, 0.2602739726027397, 0.0, 0.5, 0.7480916030534351, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[160] [0.6666666666666666, 1.0, 0.0, 0.5094339622641509, 0.17579908675799089, 0.0, 0.5, 0.6870229007633587, 0.0, 0.0, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[161] [0.5416666666666666, 0.0, 0.3333333333333333, 0.3584905660377359, 0.4931506849315069, 0.0, 0.5, 0.7251908396946564, 0.0, 0.1935483870967742, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[162] [0.6458333333333334, 0.0, 0.6666666666666666, 0.07547169811320764, 0.4383561643835617, 0.0, 0.5, 0.6793893129770993, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[163] [0.29166666666666663, 1.0, 0.0, 0.5283018867924528, 0.2762557077625571, 0.0, 0.5, 0.763358778625954, 0.0, 0.24193548387096775, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[164] [0.375, 1.0, 0.6666666666666666, 0.1320754716981133, 0.2671232876712329, 0.0, 0.5, 0.6183206106870228, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[165] [0.7083333333333334, 1.0, 0.0, 0.339622641509434, 0.46575342465753417, 1.0, 0.0, 0.46564885496183195, 1.0, 0.2903225806451613, 1.0, 0.75, 1.0] => 0 (expected 0) BETUL\n",
      "[166] [0.41666666666666663, 1.0, 0.3333333333333333, 0.339622641509434, 0.31963470319634696, 0.0, 0.5, 0.763358778625954, 0.0, 0.0967741935483871, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[167] [0.5208333333333334, 1.0, 0.6666666666666666, 0.5283018867924528, 0.2420091324200913, 0.0, 0.0, 0.7175572519083969, 0.0, 0.25806451612903225, 1.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[168] [0.33333333333333337, 0.0, 0.3333333333333333, 0.16981132075471705, 0.0776255707762557, 0.0, 0.5, 0.5114503816793893, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[169] [0.22916666666666663, 1.0, 0.0, 0.15094339622641517, 0.09360730593607308, 0.0, 0.0, 0.3282442748091603, 1.0, 0.3225806451612903, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[170] [0.5416666666666666, 1.0, 0.0, 0.6226415094339622, 0.37214611872146114, 0.0, 0.0, 0.564885496183206, 1.0, 0.12903225806451613, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[171] [0.3125, 1.0, 0.0, 0.24528301886792458, 0.09817351598173513, 0.0, 0.5, 0.5572519083969466, 1.0, 0.4516129032258064, 0.0, 0.0, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[172] [0.6249999999999999, 1.0, 0.3333333333333333, 0.4339622641509434, 0.21689497716894973, 0.0, 0.5, 0.7099236641221373, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[173] [0.41666666666666663, 1.0, 0.6666666666666666, 0.2264150943396227, 0.052511415525114125, 0.0, 0.0, 0.41984732824427473, 0.0, 0.12903225806451613, 1.0, 0.75, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[174] [0.375, 1.0, 0.6666666666666666, 0.339622641509434, 0.28995433789954334, 0.0, 0.5, 0.8244274809160305, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[175] [0.27083333333333337, 0.0, 0.6666666666666666, 0.24528301886792458, 0.18949771689497713, 0.0, 0.5, 0.7786259541984731, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[176] [0.125, 1.0, 0.0, 0.24528301886792458, 0.1643835616438356, 0.0, 0.5, 0.45038167938931295, 1.0, 0.25806451612903225, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[177] [0.47916666666666663, 1.0, 1.0, 0.5471698113207547, 0.39269406392694056, 1.0, 0.5, 0.816793893129771, 0.0, 0.1935483870967742, 0.5, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[178] [0.16666666666666663, 1.0, 0.6666666666666666, 0.339622641509434, 0.28310502283105016, 0.0, 0.5, 0.8854961832061069, 0.0, 0.564516129032258, 0.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[179] [0.8124999999999999, 1.0, 0.0, 0.4716981132075472, 0.15296803652968033, 1.0, 0.5, 0.5343511450381678, 0.0, 0.5483870967741935, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[180] [0.6458333333333334, 0.0, 0.6666666666666666, 0.24528301886792458, 0.1187214611872146, 1.0, 0.5, 0.19083969465648853, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[181] [0.6874999999999999, 0.0, 0.0, 0.28301886792452835, 0.18949771689497713, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[182] [0.125, 0.0, 0.0, 0.41509433962264153, 0.13013698630136988, 0.0, 0.5, 0.847328244274809, 0.0, 0.2258064516129032, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[183] [0.10416666666666663, 0.0, 0.3333333333333333, 0.2264150943396227, 0.1917808219178082, 0.0, 0.5, 0.9236641221374046, 0.0, 0.1129032258064516, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[184] [0.7291666666666666, 0.0, 0.0, 0.8113207547169811, 0.45433789954337894, 0.0, 0.5, 0.6335877862595419, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[185] [0.45833333333333337, 1.0, 0.6666666666666666, 0.2924528301886792, 0.27168949771689493, 1.0, 0.0, 0.7251908396946564, 0.0, 0.3870967741935484, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[186] [0.47916666666666663, 1.0, 0.0, 0.3207547169811321, 0.1780821917808219, 1.0, 0.5, 0.648854961832061, 1.0, 0.16129032258064516, 0.5, 0.0, 0.0] => 0 (expected 0) BETUL\n",
      "[187] [0.5208333333333334, 0.0, 0.6666666666666666, 0.15094339622641517, 0.2009132420091324, 0.0, 0.5, 0.6641221374045801, 0.0, 0.25806451612903225, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[188] [0.7708333333333334, 0.0, 0.0, 0.7924528301886792, 0.2328767123287671, 1.0, 0.5, 0.7175572519083969, 1.0, 0.16129032258064516, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[189] [0.6666666666666666, 1.0, 0.0, 0.41509433962264153, 0.091324200913242, 0.0, 0.0, 0.4122137404580152, 1.0, 0.5806451612903226, 0.5, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[190] [0.5833333333333334, 1.0, 0.0, 0.669811320754717, 0.37214611872146114, 1.0, 0.0, 0.4045801526717556, 0.0, 0.16129032258064516, 0.5, 0.75, 1.0] => 0 (expected 1) SALAH\n",
      "[191] [0.6666666666666666, 1.0, 1.0, 0.37735849056603776, 0.24657534246575336, 0.0, 0.5, 0.564885496183206, 0.0, 0.41935483870967744, 0.5, 0.5, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[192] [0.7291666666666666, 1.0, 0.0, 0.48113207547169823, 0.1963470319634703, 0.0, 0.0, 0.46564885496183195, 0.0, 0.3225806451612903, 0.5, 0.5, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[193] [0.375, 1.0, 0.0, 0.16981132075471705, 0.1780821917808219, 0.0, 0.5, 0.5496183206106869, 0.0, 0.016129032258064516, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[194] [0.5208333333333334, 1.0, 0.0, 0.28301886792452835, 0.31963470319634696, 0.0, 0.0, 0.2900763358778625, 1.0, 0.3548387096774194, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[195] [0.7291666666666666, 1.0, 0.0, 0.3207547169811321, 0.3127853881278539, 0.0, 0.5, 0.2595419847328244, 1.0, 0.03225806451612903, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[196] [0.4999999999999999, 1.0, 0.0, 0.4339622641509434, 0.17579908675799089, 1.0, 0.0, 0.6412213740458014, 1.0, 0.5, 0.0, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[197] [0.8333333333333334, 1.0, 0.6666666666666666, 0.4339622641509434, 0.29223744292237447, 0.0, 0.0, 0.5725190839694655, 0.0, 0.3225806451612903, 0.5, 0.75, 1.0] => 0 (expected 1) SALAH\n",
      "[198] [0.41666666666666663, 0.0, 0.0, 0.339622641509434, 0.32648401826484014, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[199] [0.9791666666666666, 0.0, 0.6666666666666666, 0.4339622641509434, 0.16210045662100453, 0.0, 1.0, 0.3435114503816793, 0.0, 0.1774193548387097, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[200] [0.45833333333333337, 1.0, 0.0, 0.4339622641509434, 0.30821917808219174, 0.0, 0.0, 0.8778625954198472, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[201] [0.25, 0.0, 0.3333333333333333, 0.339622641509434, 0.1780821917808219, 0.0, 0.0, 0.7709923664122137, 0.0, 0.2258064516129032, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[202] [0.6041666666666666, 1.0, 0.0, 0.2924528301886792, 0.3972602739726027, 0.0, 0.0, 0.763358778625954, 0.0, 0.0, 1.0, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[203] [0.5208333333333334, 1.0, 0.0, 0.28301886792452835, 0.31963470319634696, 0.0, 0.0, 0.2900763358778625, 1.0, 0.3548387096774194, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[204] [0.47916666666666663, 0.0, 0.6666666666666666, 0.39622641509433965, 0.1598173515981735, 0.0, 0.0, 0.7480916030534351, 0.0, 0.016129032258064516, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[205] [0.45833333333333337, 1.0, 0.0, 0.4339622641509434, 0.3949771689497717, 0.0, 0.5, 0.7786259541984731, 1.0, 0.25806451612903225, 1.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[206] [0.5833333333333334, 0.0, 0.3333333333333333, 0.339622641509434, 0.2511415525114155, 0.0, 0.0, 0.7862595419847328, 0.0, 0.0, 0.5, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[207] [0.7291666666666666, 0.0, 0.6666666666666666, 0.4339622641509434, 0.42694063926940634, 0.0, 0.5, 0.4732824427480916, 0.0, 0.03225806451612903, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[208] [0.7291666666666666, 0.0, 0.6666666666666666, 0.4339622641509434, 0.42694063926940634, 0.0, 0.5, 0.4732824427480916, 0.0, 0.03225806451612903, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[209] [0.27083333333333337, 1.0, 0.0, 0.39622641509433965, 0.4315068493150685, 0.0, 0.5, 0.4122137404580152, 1.0, 0.2903225806451613, 0.5, 0.0, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[210] [0.3125, 1.0, 0.3333333333333333, 0.24528301886792458, 0.3127853881278539, 0.0, 0.5, 0.7786259541984731, 0.0, 0.0, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[211] [0.39583333333333337, 1.0, 0.6666666666666666, 0.28301886792452835, 0.2945205479452055, 1.0, 0.5, 0.7938931297709922, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[212] [0.7499999999999999, 0.0, 0.6666666666666666, 0.4339622641509434, 0.6643835616438356, 1.0, 0.0, 0.6564885496183205, 0.0, 0.12903225806451613, 1.0, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[213] [0.45833333333333337, 0.0, 0.6666666666666666, 0.4339622641509434, 0.4155251141552511, 0.0, 0.0, 0.5419847328244275, 0.0, 0.24193548387096775, 1.0, 0.25, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[214] [0.6041666666666666, 0.0, 1.0, 0.5283018867924528, 0.3584474885844749, 1.0, 0.0, 0.6946564885496184, 0.0, 0.16129032258064516, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[215] [0.7916666666666666, 0.0, 0.6666666666666666, 0.19811320754716977, 0.9999999999999998, 0.0, 0.0, 0.6793893129770993, 0.0, 0.25806451612903225, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[216] [0.9791666666666666, 0.0, 0.6666666666666666, 0.4339622641509434, 0.16210045662100453, 0.0, 1.0, 0.3435114503816793, 0.0, 0.1774193548387097, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[217] [0.7083333333333334, 0.0, 0.6666666666666666, 0.3867924528301888, 0.2876712328767123, 0.0, 0.0, 0.7709923664122137, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[218] [0.29166666666666663, 1.0, 0.6666666666666666, 0.339622641509434, 0.4315068493150685, 0.0, 0.5, 0.6946564885496184, 0.0, 0.30645161290322576, 1.0, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[219] [0.35416666666666663, 0.0, 0.3333333333333333, 0.10377358490566047, 0.1780821917808219, 0.0, 0.5, 0.7709923664122137, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[220] [0.6249999999999999, 1.0, 0.0, 0.15094339622641517, 0.2579908675799087, 0.0, 0.0, 0.5419847328244275, 1.0, 0.1935483870967742, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[221] [0.7708333333333334, 1.0, 0.0, 0.6226415094339622, 0.2328767123287671, 0.0, 0.0, 0.5114503816793893, 0.0, 0.3709677419354838, 1.0, 0.0, 0.3333333333333333] => 1 (expected 0) SALAH\n",
      "[222] [0.6041666666666666, 1.0, 0.0, 0.18867924528301894, 0.4383561643835617, 0.0, 1.0, 0.5267175572519084, 0.0, 0.7096774193548387, 0.0, 0.75, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[223] [0.6874999999999999, 0.0, 0.0, 0.41509433962264153, 0.3835616438356165, 1.0, 0.5, 0.26717557251908397, 0.0, 0.30645161290322576, 0.5, 0.75, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[224] [0.47916666666666663, 1.0, 0.6666666666666666, 0.41509433962264153, 0.2214611872146119, 0.0, 0.5, 0.7480916030534351, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[225] [0.27083333333333337, 1.0, 0.6666666666666666, 0.24528301886792458, 0.2602739726027397, 1.0, 0.5, 0.9389312977099237, 0.0, 0.12903225806451613, 0.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[226] [0.4374999999999999, 0.0, 0.0, 0.15094339622641517, 0.29223744292237447, 0.0, 0.0, 0.6717557251908396, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[227] [0.4999999999999999, 1.0, 0.0, 0.4339622641509434, 0.17579908675799089, 1.0, 0.0, 0.6412213740458014, 1.0, 0.5, 0.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[228] [0.5833333333333334, 1.0, 0.3333333333333333, 0.5660377358490566, 0.2420091324200913, 0.0, 0.0, 0.7099236641221373, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[229] [0.6249999999999999, 1.0, 1.0, 0.7924528301886792, 0.32876712328767127, 0.0, 0.0, 0.564885496183206, 0.0, 0.6774193548387097, 0.0, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[230] [0.3125, 0.0, 0.6666666666666666, 0.1320754716981133, 0.034246575342465724, 0.0, 0.5, 0.7938931297709922, 0.0, 0.0967741935483871, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[231] [0.0, 1.0, 0.3333333333333333, 0.339622641509434, 0.1780821917808219, 0.0, 0.0, 0.9999999999999999, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[232] [0.39583333333333337, 1.0, 0.6666666666666666, 0.28301886792452835, 0.2945205479452055, 1.0, 0.5, 0.7938931297709922, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[233] [0.39583333333333337, 1.0, 0.0, 0.339622641509434, 0.2968036529680365, 1.0, 0.0, 0.6030534351145037, 1.0, 0.0, 1.0, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[234] [0.7499999999999999, 0.0, 0.6666666666666666, 0.6226415094339622, 0.5342465753424657, 0.0, 0.0, 0.6106870229007634, 0.0, 0.12903225806451613, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[235] [0.29166666666666663, 0.0, 0.6666666666666666, 0.26415094339622647, 0.19863013698630133, 0.0, 0.5, 0.7175572519083969, 0.0, 0.03225806451612903, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[236] [0.5833333333333334, 1.0, 0.3333333333333333, 0.5660377358490566, 0.2420091324200913, 0.0, 0.0, 0.7099236641221373, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[237] [0.5208333333333334, 0.0, 0.6666666666666666, 0.15094339622641517, 0.2009132420091324, 0.0, 0.5, 0.6641221374045801, 0.0, 0.25806451612903225, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[238] [0.5416666666666666, 0.0, 0.0, 0.3207547169811321, 0.18036529680365293, 0.0, 1.0, 0.45038167938931295, 1.0, 0.3225806451612903, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[239] [0.5624999999999999, 1.0, 0.6666666666666666, 0.339622641509434, 0.2968036529680365, 1.0, 0.0, 0.5419847328244275, 1.0, 0.0967741935483871, 0.5, 0.25, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[240] [0.5833333333333334, 1.0, 0.0, 0.5283018867924528, 0.3424657534246575, 0.0, 0.0, 0.3129770992366412, 1.0, 0.0967741935483871, 0.5, 0.25, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[241] [0.7083333333333334, 1.0, 1.0, 0.48113207547169823, 0.24429223744292233, 1.0, 0.0, 0.6030534351145037, 0.0, 0.3709677419354838, 0.0, 0.0, 0.3333333333333333] => 1 (expected 0) SALAH\n",
      "[242] [0.375, 1.0, 0.6666666666666666, 0.41509433962264153, 0.29908675799086754, 0.0, 0.0, 0.648854961832061, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[243] [0.6041666666666666, 1.0, 0.0, 0.05660377358490576, 0.24657534246575336, 0.0, 0.5, 0.648854961832061, 0.0, 0.016129032258064516, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[244] [0.6458333333333334, 1.0, 0.0, 0.339622641509434, 0.182648401826484, 0.0, 0.0, 0.46564885496183195, 1.0, 0.3870967741935484, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[245] [0.6249999999999999, 1.0, 0.3333333333333333, 0.4339622641509434, 0.21689497716894973, 0.0, 0.5, 0.7099236641221373, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[246] [0.8333333333333334, 1.0, 0.6666666666666666, 0.4339622641509434, 0.29223744292237447, 0.0, 0.0, 0.5725190839694655, 0.0, 0.3225806451612903, 0.5, 0.75, 1.0] => 0 (expected 0) BETUL\n",
      "[247] [0.5208333333333334, 1.0, 0.3333333333333333, 0.1320754716981133, 0.41780821917808214, 0.0, 0.5, 0.648854961832061, 0.0, 0.0, 1.0, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[248] [0.3125, 1.0, 0.6666666666666666, 0.24528301886792458, 0.22831050228310495, 0.0, 0.5, 0.7480916030534351, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[249] [0.7291666666666666, 1.0, 0.6666666666666666, 0.2924528301886792, 0.41780821917808214, 0.0, 0.5, 0.4580152671755725, 1.0, 0.2903225806451613, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[250] [0.7916666666666666, 0.0, 0.6666666666666666, 0.19811320754716977, 0.9999999999999998, 0.0, 0.0, 0.6793893129770993, 0.0, 0.25806451612903225, 0.5, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[251] [0.6666666666666666, 1.0, 1.0, 0.37735849056603776, 0.24657534246575336, 0.0, 0.5, 0.564885496183206, 0.0, 0.41935483870967744, 0.5, 0.5, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[252] [0.25, 0.0, 0.3333333333333333, 0.10377358490566047, 0.1643835616438356, 0.0, 0.5, 0.7404580152671755, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[253] [0.125, 1.0, 0.3333333333333333, 0.26415094339622647, 0.1506849315068493, 0.0, 0.5, 0.7862595419847328, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[254] [0.375, 1.0, 0.6666666666666666, 0.339622641509434, 0.28995433789954334, 0.0, 0.5, 0.8244274809160305, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[255] [0.45833333333333337, 1.0, 0.6666666666666666, 0.0, 0.2305936073059361, 0.0, 0.5, 0.6335877862595419, 1.0, 0.0, 1.0, 0.25, 1.0] => 1 (expected 1) BETUL\n",
      "[256] [0.7916666666666666, 0.0, 0.6666666666666666, 0.19811320754716977, 0.9999999999999998, 0.0, 0.0, 0.6793893129770993, 0.0, 0.25806451612903225, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[257] [0.5208333333333334, 1.0, 0.0, 0.26415094339622647, 0.36529680365296807, 0.0, 0.0, 0.3435114503816793, 1.0, 0.5161290322580645, 0.5, 0.5, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[258] [0.47916666666666663, 1.0, 0.6666666666666666, 0.41509433962264153, 0.2214611872146119, 0.0, 0.5, 0.7480916030534351, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[259] [0.9999999999999999, 1.0, 0.0, 0.2924528301886792, 0.4063926940639269, 0.0, 0.0, 0.6946564885496184, 1.0, 0.0, 1.0, 0.75, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[260] [0.6249999999999999, 1.0, 0.0, 0.660377358490566, 0.1141552511415525, 1.0, 0.0, 0.1450381679389312, 0.0, 0.16129032258064516, 0.5, 0.5, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[261] [0.39583333333333337, 1.0, 0.0, 0.339622641509434, 0.2968036529680365, 1.0, 0.0, 0.6030534351145037, 1.0, 0.0, 1.0, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[262] [0.35416666666666663, 1.0, 0.0, 0.4339622641509434, 0.4223744292237443, 0.0, 0.5, 0.3740458015267175, 1.0, 0.2903225806451613, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[263] [0.6666666666666666, 0.0, 0.0, 0.339622641509434, 0.46575342465753417, 0.0, 0.0, 0.7480916030534351, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[264] [0.375, 1.0, 0.0, 0.15094339622641517, 0.3401826484018265, 0.0, 0.0, 0.3587786259541984, 1.0, 0.16129032258064516, 0.5, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[265] [0.5833333333333334, 1.0, 0.0, 0.15094339622641517, 0.4771689497716895, 0.0, 0.5, 0.5496183206106869, 1.0, 0.4838709677419355, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[266] [0.20833333333333337, 1.0, 0.0, 0.2264150943396227, 0.2123287671232877, 0.0, 0.5, 0.5267175572519084, 0.0, 0.1935483870967742, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[267] [0.4374999999999999, 1.0, 0.6666666666666666, 0.4339622641509434, 0.24429223744292233, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0967741935483871, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[268] [0.7291666666666666, 1.0, 1.0, 0.7169811320754716, 0.2305936073059361, 0.0, 0.0, 0.6412213740458014, 0.0, 0.0967741935483871, 0.5, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[269] [0.8333333333333334, 0.0, 1.0, 0.4339622641509434, 0.2579908675799087, 0.0, 0.5, 0.6106870229007634, 0.0, 0.2903225806451613, 1.0, 0.5, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[270] [0.3125, 1.0, 0.3333333333333333, 0.24528301886792458, 0.2146118721461187, 0.0, 0.5, 0.7557251908396946, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[271] [0.4999999999999999, 1.0, 0.6666666666666666, 0.339622641509434, 0.27397260273972607, 1.0, 0.0, 0.7786259541984731, 0.0, 0.0, 1.0, 0.75, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[272] [0.5833333333333334, 1.0, 0.0, 0.3584905660377359, 0.18493150684931509, 0.0, 0.5, 0.7404580152671755, 1.0, 0.0, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[273] [0.125, 1.0, 0.3333333333333333, 0.26415094339622647, 0.1506849315068493, 0.0, 0.5, 0.7862595419847328, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[274] [0.6458333333333334, 0.0, 0.6666666666666666, 0.24528301886792458, 0.1187214611872146, 1.0, 0.5, 0.19083969465648853, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[275] [0.3125, 1.0, 0.6666666666666666, 0.4339622641509434, 0.2488584474885845, 0.0, 0.0, 0.8320610687022899, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[276] [0.45833333333333337, 0.0, 0.6666666666666666, 0.4339622641509434, 0.4155251141552511, 0.0, 0.0, 0.5419847328244275, 0.0, 0.24193548387096775, 1.0, 0.25, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[277] [0.5833333333333334, 1.0, 0.6666666666666666, 0.3207547169811321, 0.23515981735159813, 0.0, 0.0, 0.6030534351145037, 0.0, 0.06451612903225806, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[278] [0.25, 1.0, 0.3333333333333333, 0.3867924528301888, 0.17579908675799089, 0.0, 0.5, 0.46564885496183195, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333] => 1 (expected 0) SALAH\n",
      "[279] [0.41666666666666663, 1.0, 0.3333333333333333, 0.339622641509434, 0.31963470319634696, 0.0, 0.5, 0.763358778625954, 0.0, 0.0967741935483871, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[280] [0.10416666666666663, 1.0, 1.0, 0.2264150943396227, 0.1278538812785388, 0.0, 0.0, 0.7862595419847328, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[281] [0.25, 1.0, 0.3333333333333333, 0.24528301886792458, 0.07077625570776253, 0.0, 0.5, 0.847328244274809, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[282] [0.5833333333333334, 1.0, 0.0, 0.4339622641509434, 0.1506849315068493, 0.0, 0.5, 0.5877862595419846, 0.0, 0.06451612903225806, 0.5, 0.0, 0.3333333333333333] => 1 (expected 1) BETUL\n",
      "[283] [0.8749999999999999, 0.0, 0.6666666666666666, 0.15094339622641517, 0.31735159817351594, 1.0, 0.0, 0.45038167938931295, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[284] [0.6874999999999999, 0.0, 0.0, 0.5283018867924528, 0.2694063926940639, 0.0, 0.5, 0.6335877862595419, 1.0, 0.2258064516129032, 0.5, 0.0, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[285] [0.125, 0.0, 0.0, 0.41509433962264153, 0.13013698630136988, 0.0, 0.5, 0.847328244274809, 0.0, 0.2258064516129032, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[286] [0.6874999999999999, 1.0, 0.3333333333333333, 0.3207547169811321, 0.1872146118721461, 1.0, 0.0, 0.5267175572519084, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[287] [0.6666666666666666, 1.0, 0.6666666666666666, 0.5283018867924528, 0.2671232876712329, 1.0, 0.5, 0.5038167938931296, 1.0, 0.16129032258064516, 0.5, 0.0, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[288] [0.6249999999999999, 1.0, 1.0, 0.6226415094339622, 0.33561643835616434, 0.0, 0.0, 0.4122137404580152, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[289] [0.27083333333333337, 0.0, 0.0, 0.07547169811320764, 0.31735159817351594, 0.0, 0.0, 0.3893129770992366, 0.0, 0.0967741935483871, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[290] [0.5208333333333334, 1.0, 0.0, 0.15094339622641517, 0.182648401826484, 0.0, 0.0, 0.28244274809160297, 1.0, 0.0, 0.5, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[291] [0.6874999999999999, 0.0, 0.6666666666666666, 0.339622641509434, 0.3127853881278539, 0.0, 0.5, 0.1984732824427481, 0.0, 0.1935483870967742, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[292] [0.6249999999999999, 1.0, 0.6666666666666666, 0.5283018867924528, 0.1963470319634703, 1.0, 0.5, 0.6564885496183205, 0.0, 0.25806451612903225, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[293] [0.5416666666666666, 1.0, 0.3333333333333333, 0.339622641509434, 0.31050228310502287, 0.0, 0.5, 0.6412213740458014, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[294] [0.3125, 1.0, 0.0, 0.16981132075471705, 0.37442922374429216, 0.0, 0.0, 0.6259541984732825, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[295] [0.16666666666666663, 1.0, 0.6666666666666666, 0.339622641509434, 0.28310502283105016, 0.0, 0.5, 0.8854961832061069, 0.0, 0.564516129032258, 0.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[296] [0.1875, 1.0, 0.6666666666666666, 0.41509433962264153, 0.11187214611872148, 0.0, 0.5, 0.7786259541984731, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[297] [0.47916666666666663, 1.0, 0.3333333333333333, 0.37735849056603776, 0.17123287671232873, 0.0, 0.5, 0.6641221374045801, 0.0, 0.12903225806451613, 1.0, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[298] [0.7916666666666666, 1.0, 0.0, 0.2924528301886792, 0.29223744292237447, 1.0, 0.5, 0.7022900763358778, 0.0, 0.03225806451612903, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[299] [0.29166666666666663, 1.0, 0.0, 0.24528301886792458, 0.11643835616438353, 0.0, 0.0, 0.3740458015267175, 1.0, 0.4032258064516129, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[300] [0.35416666666666663, 1.0, 0.0, 0.24528301886792458, 0.28082191780821913, 0.0, 0.0, 0.5572519083969466, 0.0, 0.12903225806451613, 1.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[301] [0.27083333333333337, 1.0, 0.6666666666666666, 0.339622641509434, 0.1232876712328767, 0.0, 0.5, 0.6030534351145037, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[302] [0.6458333333333334, 1.0, 0.6666666666666666, 0.4339622641509434, 0.13470319634703193, 0.0, 0.0, 0.6412213740458014, 0.0, 0.4838709677419355, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[303] [0.27083333333333337, 1.0, 1.0, 0.5094339622641509, 0.2694063926940639, 0.0, 0.0, 0.816793893129771, 0.0, 0.12903225806451613, 1.0, 0.5, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[304] [0.39583333333333337, 1.0, 0.0, 0.28301886792452835, 0.33789954337899536, 0.0, 0.0, 0.7251908396946564, 0.0, 0.08064516129032258, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[305] [0.6249999999999999, 1.0, 0.6666666666666666, 0.30188679245283023, 0.2100456621004566, 1.0, 0.5, 0.48091603053435106, 0.0, 0.3548387096774194, 0.5, 0.25, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[306] [0.25, 0.0, 0.3333333333333333, 0.30188679245283023, 0.41095890410958896, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[307] [0.6874999999999999, 0.0, 0.0, 0.28301886792452835, 0.18949771689497713, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[308] [0.6666666666666666, 1.0, 0.6666666666666666, 0.5283018867924528, 0.2671232876712329, 1.0, 0.5, 0.5038167938931296, 1.0, 0.16129032258064516, 0.5, 0.0, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[309] [0.8749999999999999, 0.0, 0.6666666666666666, 0.15094339622641517, 0.31735159817351594, 1.0, 0.0, 0.45038167938931295, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[310] [0.6041666666666666, 1.0, 0.0, 0.18867924528301894, 0.4383561643835617, 0.0, 1.0, 0.5267175572519084, 0.0, 0.7096774193548387, 0.0, 0.75, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[311] [0.33333333333333337, 1.0, 0.0, 0.19811320754716977, 0.3059360730593607, 0.0, 0.0, 0.8702290076335878, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[312] [0.35416666666666663, 1.0, 0.6666666666666666, 0.5283018867924528, 0.2397260273972603, 0.0, 0.5, 0.5801526717557252, 0.0, 0.5806451612903226, 0.5, 0.0, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[313] [0.6249999999999999, 0.0, 0.0, 0.7547169811320754, 0.28082191780821913, 0.0, 0.5, 0.5496183206106869, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[314] [0.6249999999999999, 1.0, 1.0, 0.7924528301886792, 0.32876712328767127, 0.0, 0.0, 0.564885496183206, 0.0, 0.6774193548387097, 0.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[315] [0.7083333333333334, 0.0, 0.0, 0.28301886792452835, 0.16210045662100453, 0.0, 0.5, 0.49618320610687017, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[316] [0.27083333333333337, 0.0, 0.0, 0.07547169811320764, 0.31735159817351594, 0.0, 0.0, 0.3893129770992366, 0.0, 0.0967741935483871, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[317] [0.125, 0.0, 0.0, 0.41509433962264153, 0.13013698630136988, 0.0, 0.5, 0.847328244274809, 0.0, 0.2258064516129032, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[318] [0.6249999999999999, 1.0, 0.0, 0.7169811320754716, 0.4566210045662101, 0.0, 0.0, 0.5267175572519084, 1.0, 0.5483870967741935, 0.0, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[319] [0.6458333333333334, 1.0, 0.0, 0.339622641509434, 0.28995433789954334, 0.0, 0.5, 0.5572519083969466, 1.0, 0.2258064516129032, 1.0, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[320] [0.5624999999999999, 1.0, 0.3333333333333333, 0.24528301886792458, 0.2602739726027397, 0.0, 0.5, 0.7480916030534351, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[321] [0.6041666666666666, 0.0, 0.0, 0.05660377358490576, 0.2785388127853881, 0.0, 0.0, 0.3893129770992366, 0.0, 0.16129032258064516, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[322] [0.8124999999999999, 1.0, 0.6666666666666666, 0.2264150943396227, 0.34474885844748854, 0.0, 0.5, 0.6106870229007634, 0.0, 0.16129032258064516, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[323] [0.5833333333333334, 1.0, 0.6666666666666666, 0.3207547169811321, 0.23515981735159813, 0.0, 0.0, 0.6030534351145037, 0.0, 0.06451612903225806, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[324] [0.6041666666666666, 1.0, 0.0, 0.49056603773584906, 0.2100456621004566, 0.0, 0.5, 0.2595419847328244, 0.0, 0.3225806451612903, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[325] [0.5208333333333334, 1.0, 0.0, 0.15094339622641517, 0.182648401826484, 0.0, 0.0, 0.28244274809160297, 1.0, 0.0, 0.5, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[326] [0.16666666666666663, 0.0, 0.6666666666666666, 0.24528301886792458, 0.2031963470319635, 0.0, 0.5, 0.7557251908396946, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[327] [0.6041666666666666, 1.0, 0.0, 0.2924528301886792, 0.3972602739726027, 0.0, 0.0, 0.763358778625954, 0.0, 0.0, 1.0, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[328] [0.8541666666666666, 1.0, 0.0, 0.339622641509434, 0.44748858447488576, 0.0, 0.0, 0.2900763358778625, 0.0, 0.3870967741935484, 0.5, 0.75, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[329] [0.6041666666666666, 1.0, 0.0, 0.2924528301886792, 0.3972602739726027, 0.0, 0.0, 0.763358778625954, 0.0, 0.0, 1.0, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[330] [0.4374999999999999, 0.0, 0.0, 0.15094339622641517, 0.29223744292237447, 0.0, 0.0, 0.6717557251908396, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[331] [0.39583333333333337, 1.0, 0.3333333333333333, 0.339622641509434, 0.27168949771689493, 0.0, 0.0, 0.8320610687022899, 0.0, 0.03225806451612903, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[332] [0.22916666666666663, 1.0, 1.0, 0.4339622641509434, 0.16666666666666669, 0.0, 0.5, 0.816793893129771, 1.0, 0.2258064516129032, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[333] [0.45833333333333337, 1.0, 0.6666666666666666, 0.15094339622641517, 0.11187214611872148, 0.0, 0.5, 0.3969465648854962, 0.0, 0.0967741935483871, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[334] [0.27083333333333337, 1.0, 0.6666666666666666, 0.339622641509434, 0.1232876712328767, 0.0, 0.5, 0.6030534351145037, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[335] [0.41666666666666663, 0.0, 0.3333333333333333, 0.37735849056603776, 0.3310502283105023, 0.0, 0.5, 0.6946564885496184, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[336] [0.7499999999999999, 1.0, 0.0, 0.15094339622641517, 0.2785388127853881, 0.0, 0.0, 0.6641221374045801, 0.0, 0.0967741935483871, 1.0, 0.5, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[337] [0.6666666666666666, 1.0, 0.0, 0.41509433962264153, 0.091324200913242, 0.0, 0.0, 0.4122137404580152, 1.0, 0.5806451612903226, 0.5, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[338] [0.7916666666666666, 1.0, 0.0, 0.2924528301886792, 0.29223744292237447, 1.0, 0.5, 0.7022900763358778, 0.0, 0.03225806451612903, 0.5, 0.5, 1.0] => 0 (expected 0) BETUL\n",
      "[339] [0.125, 1.0, 0.3333333333333333, 0.26415094339622647, 0.1506849315068493, 0.0, 0.5, 0.7862595419847328, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[340] [0.5624999999999999, 1.0, 0.3333333333333333, 0.24528301886792458, 0.2602739726027397, 0.0, 0.5, 0.7480916030534351, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[341] [0.33333333333333337, 1.0, 0.0, 0.19811320754716977, 0.3059360730593607, 0.0, 0.0, 0.8702290076335878, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[342] [0.6458333333333334, 1.0, 0.0, 0.339622641509434, 0.182648401826484, 0.0, 0.0, 0.46564885496183195, 1.0, 0.3870967741935484, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[343] [0.6041666666666666, 1.0, 0.0, 0.3207547169811321, 0.3036529680365297, 0.0, 0.0, 0.45038167938931295, 1.0, 0.4838709677419355, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[344] [0.41666666666666663, 1.0, 0.6666666666666666, 0.2264150943396227, 0.052511415525114125, 0.0, 0.0, 0.41984732824427473, 0.0, 0.12903225806451613, 1.0, 0.75, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[345] [0.47916666666666663, 1.0, 0.0, 0.16981132075471705, 0.23744292237442927, 0.0, 0.5, 0.6793893129770993, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[346] [0.6874999999999999, 1.0, 0.6666666666666666, 0.339622641509434, 0.2397260273972603, 0.0, 0.5, 0.5725190839694655, 0.0, 0.2903225806451613, 0.5, 0.75, 1.0] => 0 (expected 0) BETUL\n",
      "[347] [0.10416666666666663, 1.0, 1.0, 0.2264150943396227, 0.1278538812785388, 0.0, 0.0, 0.7862595419847328, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[348] [0.5833333333333334, 1.0, 0.0, 0.5471698113207547, 0.33789954337899536, 0.0, 0.5, 0.1297709923664122, 1.0, 0.1935483870967742, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[349] [0.5416666666666666, 0.0, 0.0, 0.3207547169811321, 0.18036529680365293, 0.0, 1.0, 0.45038167938931295, 1.0, 0.3225806451612903, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[350] [0.8541666666666666, 1.0, 0.6666666666666666, 0.6226415094339622, 0.32648401826484014, 0.0, 0.5, 0.3129770992366412, 1.0, 0.46774193548387094, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[351] [0.7708333333333334, 0.0, 0.6666666666666666, 0.49056603773584906, 0.34703196347031967, 0.0, 0.0, 0.6183206106870228, 0.0, 0.0, 0.5, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[352] [0.6041666666666666, 1.0, 0.6666666666666666, 0.10377358490566047, 0.2602739726027397, 0.0, 0.0, 0.6335877862595419, 1.0, 0.0967741935483871, 0.5, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[353] [0.6458333333333334, 0.0, 0.0, 0.5283018867924528, 0.30136986301369856, 0.0, 0.0, 0.6564885496183205, 0.0, 0.41935483870967744, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[354] [0.5833333333333334, 0.0, 0.0, 0.3207547169811321, 0.4041095890410959, 0.0, 0.0, 0.6717557251908396, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[355] [0.6458333333333334, 0.0, 0.0, 0.5283018867924528, 0.30136986301369856, 0.0, 0.0, 0.6564885496183205, 0.0, 0.41935483870967744, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[356] [0.7916666666666666, 1.0, 0.6666666666666666, 0.5471698113207547, 0.1963470319634703, 0.0, 0.0, 0.6030534351145037, 0.0, 0.12903225806451613, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[357] [0.5624999999999999, 1.0, 0.6666666666666666, 0.339622641509434, 0.2968036529680365, 1.0, 0.0, 0.5419847328244275, 1.0, 0.0967741935483871, 0.5, 0.25, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[358] [0.6458333333333334, 1.0, 0.0, 0.2924528301886792, 0.30136986301369856, 0.0, 0.0, 0.5343511450381678, 1.0, 0.4516129032258064, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[359] [0.5833333333333334, 1.0, 0.0, 0.5471698113207547, 0.33789954337899536, 0.0, 0.5, 0.1297709923664122, 1.0, 0.1935483870967742, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[360] [0.6874999999999999, 1.0, 0.0, 0.24528301886792458, 0.3219178082191781, 0.0, 0.5, 0.2137404580152671, 1.0, 0.2903225806451613, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[361] [0.5833333333333334, 1.0, 0.0, 0.15094339622641517, 0.17123287671232873, 0.0, 0.5, 0.41984732824427473, 1.0, 0.24193548387096775, 0.5, 0.0, 0.3333333333333333] => 1 (expected 1) BETUL\n",
      "[362] [0.0, 1.0, 0.3333333333333333, 0.339622641509434, 0.1780821917808219, 0.0, 0.0, 0.9999999999999999, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[363] [0.10416666666666663, 0.0, 0.3333333333333333, 0.2264150943396227, 0.1917808219178082, 0.0, 0.5, 0.9236641221374046, 0.0, 0.1129032258064516, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[364] [0.16666666666666663, 1.0, 0.6666666666666666, 0.339622641509434, 0.28310502283105016, 0.0, 0.5, 0.8854961832061069, 0.0, 0.564516129032258, 0.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[365] [0.3125, 1.0, 0.3333333333333333, 0.24528301886792458, 0.3127853881278539, 0.0, 0.5, 0.7786259541984731, 0.0, 0.0, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[366] [0.7916666666666666, 0.0, 0.0, 0.1132075471698114, 0.2214611872146119, 0.0, 0.5, 0.5419847328244275, 0.0, 0.04838709677419355, 1.0, 0.5, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[367] [0.7083333333333334, 0.0, 0.0, 0.5283018867924528, 0.6415525114155252, 0.0, 0.0, 0.6335877862595419, 0.0, 0.6451612903225806, 0.5, 0.75, 1.0] => 0 (expected 0) BETUL\n",
      "[368] [0.6041666666666666, 0.0, 0.6666666666666666, 0.24528301886792458, 0.4885844748858447, 0.0, 0.5, 0.7709923664122137, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[369] [0.20833333333333337, 0.0, 0.6666666666666666, 0.0, 0.16666666666666669, 0.0, 0.5, 0.8244274809160305, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[370] [0.7291666666666666, 1.0, 0.0, 0.3207547169811321, 0.3127853881278539, 0.0, 0.5, 0.2595419847328244, 1.0, 0.03225806451612903, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[371] [0.6666666666666666, 0.0, 0.0, 0.48113207547169823, 0.4132420091324201, 0.0, 0.0, 0.5725190839694655, 1.0, 0.16129032258064516, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[372] [0.7916666666666666, 1.0, 0.0, 0.6226415094339622, 0.36529680365296807, 0.0, 0.0, 0.28244274809160297, 1.0, 0.24193548387096775, 0.5, 0.75, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[373] [0.20833333333333337, 0.0, 0.6666666666666666, 0.41509433962264153, 0.2146118721461187, 0.0, 0.5, 0.6183206106870228, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[374] [0.6874999999999999, 0.0, 0.6666666666666666, 0.339622641509434, 0.3127853881278539, 0.0, 0.5, 0.1984732824427481, 0.0, 0.1935483870967742, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[375] [0.5208333333333334, 1.0, 0.6666666666666666, 0.24528301886792458, 0.30136986301369856, 0.0, 0.0, 0.5801526717557252, 0.0, 0.06451612903225806, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[376] [0.7499999999999999, 1.0, 0.0, 0.24528301886792458, 0.11643835616438353, 0.0, 0.5, 0.5267175572519084, 0.0, 0.06451612903225806, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[377] [0.6874999999999999, 0.0, 0.0, 0.6226415094339622, 0.0867579908675799, 0.0, 0.0, 0.564885496183206, 0.0, 1.0, 0.0, 0.75, 1.0] => 0 (expected 1) SALAH\n",
      "[378] [0.10416666666666663, 0.0, 0.3333333333333333, 0.2264150943396227, 0.1917808219178082, 0.0, 0.5, 0.9236641221374046, 0.0, 0.1129032258064516, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[379] [0.3125, 1.0, 0.3333333333333333, 0.339622641509434, 0.2123287671232877, 0.0, 0.0, 0.8931297709923663, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[380] [0.6874999999999999, 0.0, 0.0, 0.4339622641509434, 0.3242009132420091, 0.0, 0.0, 0.6793893129770993, 0.0, 0.5806451612903226, 0.0, 0.5, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[381] [0.4999999999999999, 0.0, 0.6666666666666666, 0.3207547169811321, 0.2054794520547945, 0.0, 0.0, 0.33587786259541985, 0.0, 0.0, 1.0, 0.0, 0.0] => 1 (expected 0) SALAH\n",
      "[382] [0.22916666666666663, 1.0, 0.0, 0.5471698113207547, 0.2214611872146119, 0.0, 0.5, 0.8396946564885496, 0.0, 0.0, 1.0, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[383] [0.27083333333333337, 0.0, 0.6666666666666666, 0.24528301886792458, 0.18949771689497713, 0.0, 0.5, 0.7786259541984731, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[384] [0.7291666666666666, 1.0, 0.0, 0.3207547169811321, 0.3127853881278539, 0.0, 0.5, 0.2595419847328244, 1.0, 0.03225806451612903, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[385] [0.5416666666666666, 0.0, 0.3333333333333333, 0.3584905660377359, 0.4931506849315069, 0.0, 0.5, 0.7251908396946564, 0.0, 0.1935483870967742, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[386] [0.47916666666666663, 1.0, 0.0, 0.3207547169811321, 0.2945205479452055, 0.0, 0.5, 0.6870229007633587, 1.0, 0.0, 1.0, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[387] [0.7083333333333334, 0.0, 0.0, 0.28301886792452835, 0.16210045662100453, 0.0, 0.5, 0.49618320610687017, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[388] [0.4374999999999999, 1.0, 0.6666666666666666, 0.4339622641509434, 0.24429223744292233, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0967741935483871, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[389] [0.6458333333333334, 0.0, 1.0, 0.5283018867924528, 0.2602739726027397, 0.0, 0.5, 0.763358778625954, 0.0, 0.14516129032258066, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[390] [0.29166666666666663, 1.0, 0.0, 0.24528301886792458, 0.11643835616438353, 0.0, 0.0, 0.3740458015267175, 1.0, 0.4032258064516129, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[391] [0.6666666666666666, 1.0, 0.0, 0.24528301886792458, 0.3059360730593607, 0.0, 0.5, 0.5267175572519084, 1.0, 0.5806451612903226, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[392] [0.4374999999999999, 1.0, 0.0, 0.5283018867924528, 0.2671232876712329, 0.0, 0.0, 0.43511450381679384, 0.0, 0.41935483870967744, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[393] [0.22916666666666663, 1.0, 0.0, 0.15094339622641517, 0.09360730593607308, 0.0, 0.0, 0.3282442748091603, 1.0, 0.3225806451612903, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[394] [0.35416666666666663, 0.0, 0.0, 0.41509433962264153, 0.2671232876712329, 0.0, 0.0, 0.6183206106870228, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[395] [0.45833333333333337, 1.0, 0.6666666666666666, 0.05660377358490576, 0.21917808219178087, 0.0, 0.5, 0.5496183206106869, 1.0, 0.1935483870967742, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[396] [0.375, 1.0, 0.6666666666666666, 0.339622641509434, 0.28995433789954334, 0.0, 0.5, 0.8244274809160305, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[397] [0.7916666666666666, 1.0, 0.6666666666666666, 0.5471698113207547, 0.1963470319634703, 0.0, 0.0, 0.6030534351145037, 0.0, 0.12903225806451613, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[398] [0.8124999999999999, 1.0, 0.6666666666666666, 0.8113207547169811, 0.33789954337899536, 1.0, 0.0, 0.6030534351145037, 1.0, 0.25806451612903225, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[399] [0.25, 0.0, 0.6666666666666666, 0.16981132075471705, 0.3242009132420091, 0.0, 0.0, 0.7709923664122137, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[400] [0.6874999999999999, 0.0, 0.0, 0.41509433962264153, 0.3835616438356165, 1.0, 0.5, 0.26717557251908397, 0.0, 0.30645161290322576, 0.5, 0.75, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[401] [0.7291666666666666, 0.0, 0.0, 0.8113207547169811, 0.45433789954337894, 0.0, 0.5, 0.6335877862595419, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[402] [0.6249999999999999, 1.0, 1.0, 0.7924528301886792, 0.32876712328767127, 0.0, 0.0, 0.564885496183206, 0.0, 0.6774193548387097, 0.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[403] [0.7708333333333334, 1.0, 0.3333333333333333, 0.6226415094339622, 0.27397260273972607, 0.0, 0.5, 0.3740458015267175, 1.0, 0.0, 0.5, 0.75, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[404] [0.25, 1.0, 0.3333333333333333, 0.3867924528301888, 0.17579908675799089, 0.0, 0.5, 0.46564885496183195, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333] => 1 (expected 0) SALAH\n",
      "[405] [0.7083333333333334, 0.0, 0.3333333333333333, 0.4339622641509434, 0.15753424657534248, 0.0, 0.5, 0.8244274809160305, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[406] [0.41666666666666663, 0.0, 0.3333333333333333, 0.37735849056603776, 0.3310502283105023, 0.0, 0.5, 0.6946564885496184, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[407] [0.7291666666666666, 0.0, 0.0, 0.8113207547169811, 0.45433789954337894, 0.0, 0.5, 0.6335877862595419, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[408] [0.25, 0.0, 0.3333333333333333, 0.339622641509434, 0.1780821917808219, 0.0, 0.0, 0.7709923664122137, 0.0, 0.2258064516129032, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[409] [0.47916666666666663, 1.0, 0.0, 0.16981132075471705, 0.23744292237442927, 0.0, 0.5, 0.6793893129770993, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[410] [0.5416666666666666, 1.0, 0.0, 0.4339622641509434, 0.20776255707762553, 0.0, 0.5, 0.30534351145038163, 1.0, 0.9032258064516128, 0.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[411] [0.7708333333333334, 0.0, 0.6666666666666666, 0.49056603773584906, 0.34703196347031967, 0.0, 0.0, 0.6183206106870228, 0.0, 0.0, 0.5, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[412] [0.6874999999999999, 1.0, 0.6666666666666666, 0.339622641509434, 0.2397260273972603, 0.0, 0.5, 0.5725190839694655, 0.0, 0.2903225806451613, 0.5, 0.75, 1.0] => 0 (expected 0) BETUL\n",
      "[413] [0.8333333333333334, 1.0, 1.0, 0.6226415094339622, 0.24657534246575336, 1.0, 0.0, 0.4580152671755725, 0.0, 0.016129032258064516, 0.5, 0.25, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[414] [0.6249999999999999, 1.0, 1.0, 0.7169811320754716, 0.3698630136986301, 0.0, 0.0, 0.6717557251908396, 0.0, 0.03225806451612903, 0.5, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[415] [0.5833333333333334, 1.0, 0.0, 0.5283018867924528, 0.3424657534246575, 0.0, 0.0, 0.3129770992366412, 1.0, 0.0967741935483871, 0.5, 0.25, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[416] [0.7083333333333334, 0.0, 0.0, 0.28301886792452835, 0.16210045662100453, 0.0, 0.5, 0.49618320610687017, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[417] [0.39583333333333337, 1.0, 0.3333333333333333, 0.15094339622641517, 0.23515981735159813, 0.0, 0.5, 0.7404580152671755, 0.0, 0.16129032258064516, 0.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[418] [0.41666666666666663, 1.0, 0.6666666666666666, 0.24528301886792458, 0.1415525114155251, 0.0, 0.5, 0.5190839694656487, 0.0, 0.3225806451612903, 0.5, 0.75, 1.0] => 0 (expected 1) SALAH\n",
      "[419] [0.6041666666666666, 1.0, 0.6666666666666666, 0.4339622641509434, 0.1940639269406393, 1.0, 0.0, 0.7175572519083969, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[420] [0.5208333333333334, 1.0, 0.6666666666666666, 0.24528301886792458, 0.30136986301369856, 0.0, 0.0, 0.5801526717557252, 0.0, 0.06451612903225806, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[421] [0.4374999999999999, 1.0, 0.0, 0.4716981132075472, 0.1689497716894977, 0.0, 0.0, 0.41984732824427473, 1.0, 0.14516129032258066, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[422] [0.6249999999999999, 1.0, 0.0, 0.3867924528301888, 0.24657534246575336, 0.0, 0.5, 0.6870229007633587, 0.0, 0.08064516129032258, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[423] [0.4374999999999999, 1.0, 0.0, 0.4716981132075472, 0.1689497716894977, 0.0, 0.0, 0.41984732824427473, 1.0, 0.14516129032258066, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[424] [0.45833333333333337, 1.0, 0.6666666666666666, 0.05660377358490576, 0.21917808219178087, 0.0, 0.5, 0.5496183206106869, 1.0, 0.1935483870967742, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[425] [0.33333333333333337, 1.0, 0.3333333333333333, 0.3207547169811321, 0.4155251141552511, 0.0, 0.0, 0.7557251908396946, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[426] [0.3125, 1.0, 0.6666666666666666, 0.4339622641509434, 0.2488584474885845, 0.0, 0.0, 0.8320610687022899, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[427] [0.20833333333333337, 0.0, 0.6666666666666666, 0.0, 0.16666666666666669, 0.0, 0.5, 0.8244274809160305, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[428] [0.45833333333333337, 0.0, 0.0, 0.339622641509434, 0.40867579908675794, 0.0, 0.5, 0.5419847328244275, 1.0, 0.1935483870967742, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[429] [0.6666666666666666, 1.0, 0.0, 0.4339622641509434, 0.18493150684931509, 0.0, 0.0, 0.5114503816793893, 1.0, 0.30645161290322576, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[430] [0.3125, 1.0, 0.3333333333333333, 0.24528301886792458, 0.2146118721461187, 0.0, 0.5, 0.7557251908396946, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[431] [0.27083333333333337, 1.0, 0.3333333333333333, 0.24528301886792458, 0.3858447488584475, 0.0, 0.5, 0.6946564885496184, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[432] [0.4999999999999999, 1.0, 0.0, 0.4339622641509434, 0.17579908675799089, 1.0, 0.0, 0.6412213740458014, 1.0, 0.5, 0.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[433] [0.45833333333333337, 1.0, 0.6666666666666666, 0.15094339622641517, 0.11187214611872148, 0.0, 0.5, 0.3969465648854962, 0.0, 0.0967741935483871, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[434] [0.8333333333333334, 0.0, 1.0, 0.4339622641509434, 0.2579908675799087, 0.0, 0.5, 0.6106870229007634, 0.0, 0.2903225806451613, 1.0, 0.5, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[435] [0.6458333333333334, 1.0, 0.0, 0.4339622641509434, 0.38127853881278534, 0.0, 0.0, 0.7557251908396946, 0.0, 0.1935483870967742, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[436] [0.6874999999999999, 0.0, 0.0, 0.4339622641509434, 0.6118721461187213, 0.0, 0.0, 0.6564885496183205, 0.0, 0.1935483870967742, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[437] [0.5833333333333334, 1.0, 0.0, 0.669811320754717, 0.37214611872146114, 1.0, 0.0, 0.4045801526717556, 0.0, 0.16129032258064516, 0.5, 0.75, 1.0] => 0 (expected 0) BETUL\n",
      "[438] [0.7499999999999999, 0.0, 0.6666666666666666, 0.5754716981132076, 0.32648401826484014, 0.0, 0.5, 0.5877862595419846, 0.0, 0.12903225806451613, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[439] [0.25, 1.0, 0.3333333333333333, 0.3867924528301888, 0.17579908675799089, 0.0, 0.5, 0.46564885496183195, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333] => 1 (expected 0) SALAH\n",
      "[440] [0.8124999999999999, 1.0, 0.6666666666666666, 0.2264150943396227, 0.34474885844748854, 0.0, 0.5, 0.6106870229007634, 0.0, 0.16129032258064516, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[441] [0.5208333333333334, 0.0, 0.6666666666666666, 0.3867924528301888, 0.4063926940639269, 1.0, 0.5, 0.7557251908396946, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[442] [0.45833333333333337, 1.0, 0.6666666666666666, 0.05660377358490576, 0.21917808219178087, 0.0, 0.5, 0.5496183206106869, 1.0, 0.1935483870967742, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[443] [0.41666666666666663, 0.0, 0.0, 0.339622641509434, 0.32648401826484014, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[444] [0.6874999999999999, 1.0, 0.3333333333333333, 0.24528301886792458, 0.35388127853881274, 0.0, 0.0, 0.2442748091603053, 0.0, 0.2258064516129032, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[445] [0.6041666666666666, 1.0, 0.6666666666666666, 0.10377358490566047, 0.2602739726027397, 0.0, 0.0, 0.6335877862595419, 1.0, 0.0967741935483871, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[446] [0.6041666666666666, 1.0, 0.6666666666666666, 0.4339622641509434, 0.1940639269406393, 1.0, 0.0, 0.7175572519083969, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[447] [0.6041666666666666, 1.0, 0.0, 0.05660377358490576, 0.24657534246575336, 0.0, 0.5, 0.648854961832061, 0.0, 0.016129032258064516, 1.0, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[448] [0.5416666666666666, 0.0, 0.0, 0.8113207547169811, 0.4589041095890411, 0.0, 1.0, 0.35114503816793885, 1.0, 0.5483870967741935, 0.5, 0.0, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[449] [0.6041666666666666, 0.0, 0.0, 0.339622641509434, 0.16210045662100453, 0.0, 0.5, 0.4580152671755725, 0.0, 0.0967741935483871, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[450] [0.35416666666666663, 1.0, 0.3333333333333333, 0.0660377358490567, 0.16210045662100453, 1.0, 0.5, 0.648854961832061, 0.0, 0.0, 1.0, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[451] [0.27083333333333337, 1.0, 0.0, 0.39622641509433965, 0.4315068493150685, 0.0, 0.5, 0.4122137404580152, 1.0, 0.2903225806451613, 0.5, 0.0, 0.3333333333333333] => 0 (expected 1) SALAH\n",
      "[452] [0.35416666666666663, 0.0, 0.3333333333333333, 0.10377358490566047, 0.1780821917808219, 0.0, 0.5, 0.7709923664122137, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[453] [0.41666666666666663, 0.0, 0.3333333333333333, 0.37735849056603776, 0.3310502283105023, 0.0, 0.5, 0.6946564885496184, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[454] [0.7083333333333334, 0.0, 0.0, 0.5283018867924528, 0.6415525114155252, 0.0, 0.0, 0.6335877862595419, 0.0, 0.6451612903225806, 0.5, 0.75, 1.0] => 0 (expected 0) BETUL\n",
      "[455] [0.6666666666666666, 1.0, 1.0, 0.37735849056603776, 0.24657534246575336, 0.0, 0.5, 0.564885496183206, 0.0, 0.41935483870967744, 0.5, 0.5, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[456] [0.7083333333333334, 1.0, 0.0, 0.339622641509434, 0.29223744292237447, 0.0, 0.0, 0.5801526717557252, 0.0, 0.2258064516129032, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[457] [0.47916666666666663, 1.0, 1.0, 0.5471698113207547, 0.39269406392694056, 1.0, 0.5, 0.816793893129771, 0.0, 0.1935483870967742, 0.5, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[458] [0.6458333333333334, 1.0, 0.0, 0.21698113207547165, 0.23744292237442927, 1.0, 0.5, 0.6793893129770993, 1.0, 0.2258064516129032, 1.0, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[459] [0.6249999999999999, 1.0, 0.0, 0.4339622641509434, 0.11643835616438353, 0.0, 0.5, 0.6946564885496184, 1.0, 0.0, 1.0, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[460] [0.33333333333333337, 1.0, 0.0, 0.09433962264150952, 0.1872146118721461, 0.0, 0.0, 0.5877862595419846, 1.0, 0.4838709677419355, 0.5, 0.0, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[461] [0.5208333333333334, 1.0, 0.3333333333333333, 0.9245283018867926, 0.3584474885844749, 0.0, 0.0, 0.9465648854961831, 0.0, 0.0, 1.0, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[462] [0.25, 1.0, 0.3333333333333333, 0.24528301886792458, 0.07077625570776253, 0.0, 0.5, 0.847328244274809, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[463] [0.4999999999999999, 1.0, 0.0, 0.4528301886792453, 0.22831050228310495, 0.0, 0.0, 0.30534351145038163, 1.0, 0.0, 1.0, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[464] [0.29166666666666663, 1.0, 0.0, 0.3584905660377359, 0.2762557077625571, 1.0, 0.0, 0.5496183206106869, 1.0, 0.016129032258064516, 0.5, 1.0, 1.0] => 0 (expected 1) SALAH\n",
      "[465] [0.5833333333333334, 1.0, 0.3333333333333333, 0.28301886792452835, 0.30821917808219174, 0.0, 0.5, 0.5343511450381678, 0.0, 0.04838709677419355, 1.0, 0.0, 1.0] => 1 (expected 1) BETUL\n",
      "[466] [0.6249999999999999, 1.0, 0.0, 0.41509433962264153, 0.3310502283105023, 0.0, 0.0, 0.847328244274809, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[467] [0.6041666666666666, 1.0, 0.6666666666666666, 0.16981132075471705, 0.23744292237442927, 0.0, 0.0, 0.7175572519083969, 0.0, 0.4032258064516129, 0.5, 0.25, 1.0] => 0 (expected 0) BETUL\n",
      "[468] [0.0, 1.0, 0.3333333333333333, 0.339622641509434, 0.1780821917808219, 0.0, 0.0, 0.9999999999999999, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[469] [0.25, 0.0, 0.3333333333333333, 0.30188679245283023, 0.41095890410958896, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[470] [0.6874999999999999, 0.0, 0.0, 0.4339622641509434, 0.3242009132420091, 0.0, 0.0, 0.6793893129770993, 0.0, 0.5806451612903226, 0.0, 0.5, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[471] [0.7083333333333334, 0.0, 0.6666666666666666, 0.3867924528301888, 0.2876712328767123, 0.0, 0.0, 0.7709923664122137, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[472] [0.1875, 1.0, 1.0, 0.24528301886792458, 0.2397260273972603, 0.0, 0.5, 0.847328244274809, 1.0, 0.6129032258064515, 0.5, 0.0, 1.0] => 0 (expected 1) SALAH\n",
      "[473] [0.5624999999999999, 0.0, 0.3333333333333333, 0.4339622641509434, 0.3835616438356165, 0.0, 0.0, 0.6259541984732825, 0.0, 0.20967741935483872, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[474] [0.6041666666666666, 0.0, 0.6666666666666666, 0.24528301886792458, 0.4885844748858447, 0.0, 0.5, 0.7709923664122137, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[475] [0.6041666666666666, 1.0, 0.0, 0.3207547169811321, 0.3036529680365297, 0.0, 0.0, 0.45038167938931295, 1.0, 0.4838709677419355, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[476] [0.7291666666666666, 0.0, 0.0, 0.339622641509434, 0.4041095890410959, 0.0, 0.5, 0.3893129770992366, 0.0, 0.3225806451612903, 0.5, 0.5, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[477] [0.6874999999999999, 0.0, 0.0, 0.28301886792452835, 0.18949771689497713, 0.0, 0.5, 0.7022900763358778, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[478] [0.7916666666666666, 0.0, 0.0, 0.1132075471698114, 0.2214611872146119, 0.0, 0.5, 0.5419847328244275, 0.0, 0.04838709677419355, 1.0, 0.5, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[479] [0.47916666666666663, 1.0, 0.0, 0.1320754716981133, 0.24429223744292233, 1.0, 0.5, 0.5801526717557252, 0.0, 0.016129032258064516, 1.0, 0.75, 1.0] => 1 (expected 0) SALAH\n",
      "[480] [0.25, 1.0, 0.6666666666666666, 0.16981132075471705, 0.28310502283105016, 0.0, 0.5, 0.8244274809160305, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[481] [0.10416666666666663, 1.0, 1.0, 0.2264150943396227, 0.1278538812785388, 0.0, 0.0, 0.7862595419847328, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[482] [0.6041666666666666, 1.0, 0.6666666666666666, 0.10377358490566047, 0.2602739726027397, 0.0, 0.0, 0.6335877862595419, 1.0, 0.0967741935483871, 0.5, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[483] [0.41666666666666663, 1.0, 0.6666666666666666, 0.2264150943396227, 0.052511415525114125, 0.0, 0.0, 0.41984732824427473, 0.0, 0.12903225806451613, 1.0, 0.75, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[484] [0.27083333333333337, 1.0, 0.6666666666666666, 0.24528301886792458, 0.2602739726027397, 1.0, 0.5, 0.9389312977099237, 0.0, 0.12903225806451613, 0.0, 0.0, 1.0] => 1 (expected 0) SALAH\n",
      "[485] [0.5624999999999999, 1.0, 0.3333333333333333, 0.24528301886792458, 0.2602739726027397, 0.0, 0.5, 0.7480916030534351, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[486] [0.3125, 1.0, 0.0, 0.24528301886792458, 0.09817351598173513, 0.0, 0.5, 0.5572519083969466, 1.0, 0.4516129032258064, 0.0, 0.0, 0.3333333333333333] => 0 (expected 0) BETUL\n",
      "[487] [0.7499999999999999, 0.0, 0.0, 0.5283018867924528, 0.22602739726027393, 0.0, 0.0, 0.3282442748091603, 0.0, 0.16129032258064516, 0.5, 0.75, 1.0] => 0 (expected 0) BETUL\n",
      "[488] [0.45833333333333337, 1.0, 0.6666666666666666, 0.2924528301886792, 0.27168949771689493, 1.0, 0.0, 0.7251908396946564, 0.0, 0.3870967741935484, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[489] [0.3125, 1.0, 0.6666666666666666, 0.339622641509434, 0.24429223744292233, 0.0, 0.5, 0.8244274809160305, 1.0, 0.06451612903225806, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[490] [0.8749999999999999, 0.0, 0.0, 0.16981132075471705, 0.052511415525114125, 0.0, 0.5, 0.4122137404580152, 0.0, 0.25806451612903225, 0.5, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[491] [0.45833333333333337, 0.0, 0.6666666666666666, 0.339622641509434, 0.2968036529680365, 0.0, 0.0, 0.5954198473282443, 0.0, 0.08064516129032258, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n",
      "[492] [0.8124999999999999, 1.0, 0.6666666666666666, 0.8113207547169811, 0.33789954337899536, 1.0, 0.0, 0.6030534351145037, 1.0, 0.25806451612903225, 0.5, 0.0, 1.0] => 0 (expected 0) BETUL\n",
      "[493] [0.33333333333333337, 0.0, 0.0, 0.41509433962264153, 0.2511415525114155, 0.0, 0.0, 0.6183206106870228, 1.0, 0.03225806451612903, 0.5, 0.0, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[494] [0.5208333333333334, 1.0, 0.0, 0.24528301886792458, 0.1415525114155251, 0.0, 0.5, 0.32061068702290074, 0.0, 0.2258064516129032, 0.5, 0.25, 1.0] => 0 (expected 1) SALAH\n",
      "[495] [0.6458333333333334, 1.0, 0.0, 0.339622641509434, 0.182648401826484, 0.0, 0.0, 0.46564885496183195, 1.0, 0.3870967741935484, 0.5, 0.5, 1.0] => 0 (expected 1) SALAH\n",
      "[496] [0.5208333333333334, 1.0, 0.0, 0.26415094339622647, 0.36529680365296807, 0.0, 0.0, 0.3435114503816793, 1.0, 0.5161290322580645, 0.5, 0.5, 0.6666666666666666] => 0 (expected 0) BETUL\n",
      "[497] [0.7499999999999999, 0.0, 0.6666666666666666, 0.4339622641509434, 0.6643835616438356, 1.0, 0.0, 0.6564885496183205, 0.0, 0.12903225806451613, 1.0, 0.25, 0.6666666666666666] => 1 (expected 0) SALAH\n",
      "[498] [0.29166666666666663, 1.0, 0.6666666666666666, 0.339622641509434, 0.4315068493150685, 0.0, 0.5, 0.6946564885496184, 0.0, 0.30645161290322576, 1.0, 0.25, 0.6666666666666666] => 0 (expected 1) SALAH\n",
      "[499] [0.6874999999999999, 1.0, 0.3333333333333333, 0.3207547169811321, 0.1872146118721461, 1.0, 0.0, 0.5267175572519084, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666] => 1 (expected 1) BETUL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = (model.predict(X_train) > 0.5).astype(int)\n",
    "for i in range(500):\n",
    "    if predictions[i] == y[i]:\n",
    "        print('[%d] %s => %d (expected %d) BETUL' % (i,X_train[i].tolist(), predictions[i], y[i]))\n",
    "    else:\n",
    "        print('[%d] %s => %d (expected %d) SALAH' % (i,X_train[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd8a729e9011353bea70d827c4a883ab4de60514c57adf329a667d269e868491"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
